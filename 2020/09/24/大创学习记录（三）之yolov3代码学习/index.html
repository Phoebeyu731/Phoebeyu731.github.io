<!DOCTYPE html>
<html lang="en">
    <!-- title -->




<!-- keywords -->




<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="John Doe">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="John Doe">
    
    <meta name="keywords" content="hexo,hexo-theme,hexo-blog">
    
    <meta name="description" content="">
    <meta name="description" content="PyTorch-YOLOv3项目训练与代码学习 借助从零开始的PyTorch项目理解YOLOv3目标检测的实现  PyTorch对于PyTorch就不用多说了，目前最灵活、最容易掌握的深度学习库，它有诸多优点，举下面三个例子：  易于使用的API-它就像Python一样简单。 Python的支持—如上所述，PyTorch可以顺利地与Python数据科学栈集成。 动态计算图—取代了具有特定功能的预定">
<meta property="og:type" content="article">
<meta property="og:title" content="大创学习记录（三）之yolov3代码学习">
<meta property="og:url" content="http://yoursite.com/2020/09/24/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8Byolov3%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="PyTorch-YOLOv3项目训练与代码学习 借助从零开始的PyTorch项目理解YOLOv3目标检测的实现  PyTorch对于PyTorch就不用多说了，目前最灵活、最容易掌握的深度学习库，它有诸多优点，举下面三个例子：  易于使用的API-它就像Python一样简单。 Python的支持—如上所述，PyTorch可以顺利地与Python数据科学栈集成。 动态计算图—取代了具有特定功能的预定">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200920111240441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200920123016892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200920164849390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200920165141558.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200920165515522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200921175928444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922165908202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922171331565.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922171300997.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922172118534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922172805375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922174400667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922174423704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922174534924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922174656406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020092217515347.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922175739780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922175934750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922180246468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922180614936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922180935263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922181448190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922210457663.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922213820150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922214525167.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922214635950.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200922214644929.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923203340676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020092321093911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923211239660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923211703697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923215913232.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923223018564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020092322310363.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923231534284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923233652727.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923234645255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923235052608.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200923235420229.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924000346228.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924151223279.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924151320586.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924151331970.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924151341353.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924151348479.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924151623243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924152545360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924154348208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924154617876.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924154701884.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924154709505.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924155836773.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924160004857.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924160259886.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924160407747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924160418500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924160453237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924161757438.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924161814266.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924161920323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924163036860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924163311998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020092416341434.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200924163603334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
<meta property="article:published_time" content="2020-09-24T09:51:55.000Z">
<meta property="article:modified_time" content="2020-09-24T09:52:41.941Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="PyTorch-YOLOv3">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200920111240441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    
    <link rel="alternate" href="/atom.xml" title="Phoebe&#39;s blog" type="application/atom+xml">
    
    <title>大创学习记录（三）之yolov3代码学习 · Phoebe&#39;s blog</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= "/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= "/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/assets/favicon.ico" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
<meta name="generator" content="Hexo 4.2.0"></head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Phoebe&#39;s blog</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">大创学习记录（三）之yolov3代码学习</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Phoebe's blog</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            大创学习记录（三）之yolov3代码学习
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "PyTorch-YOLOv3">PyTorch-YOLOv3</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">17.1k</span>Reading time: <span class="post-count reading-time">80 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2020/09/24</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="PyTorch-YOLOv3项目训练与代码学习"><a href="#PyTorch-YOLOv3项目训练与代码学习" class="headerlink" title="PyTorch-YOLOv3项目训练与代码学习"></a>PyTorch-YOLOv3项目训练与代码学习</h1><blockquote>
<p>借助从零开始的PyTorch项目理解YOLOv3目标检测的实现</p>
</blockquote>
<h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><p>对于PyTorch就不用多说了，目前最灵活、最容易掌握的深度学习库，它有诸多优点，举下面三个例子：</p>
<ul>
<li><strong>易于使用的API</strong>-它就像Python一样简单。</li>
<li><strong>Python的支持</strong>—如上所述，PyTorch可以顺利地与Python数据科学栈集成。</li>
<li><strong>动态计算图</strong>—取代了具有特定功能的预定义图形，PyTorch为我们提供了一个框架，以便可以在运行时构建计算图，甚至在运行时更改它们。在不知道创建神经网络需要多少内存的情况下这非常有价值。<br>还有比如多gpu支持，自定义数据加载器和简化的预处理器等优点，若想要了解更多细节，可参考<a href="https://www.jianshu.com/p/d66319506dd7" target="_blank" rel="noopener">PyTorch入门</a></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200920111240441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p>上次大创启动会的时候简单给大家分享了YOLO算法的原理，这里放上自己参考的文章<a href="https://www.cnblogs.com/ywheunji/p/10761239.html" target="_blank" rel="noopener">YOLOv1到YOLOv3的演变过程</a><br>以及一些详细讲述了工作原理、训练过程和与其他检测器的性能规避的原始论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="noopener">YOLO V1: You Only Look Once: Unified, Real-Time Object Detection</a></li>
<li><a href="https://arxiv.org/pdf/1612.08242.pdf" target="_blank" rel="noopener">YOLO V2: YOLO9000: Better, Faster, Stronger</a></li>
<li><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">YOLO V3: An Incremental Improvement</a></li>
<li><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">Convolutional Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Bounding Box Regression (Appendix C)</a></li>
<li><a href="https://www.youtube.com/watch?v=DNEm4fJ-rto" target="_blank" rel="noopener">IoU</a></li>
<li><a href="https://www.youtube.com/watch?v=A46HZGR5fMw" target="_blank" rel="noopener">Non maximum suppresion</a></li>
<li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">PyTorch Official Tutorial</a></li>
</ul>
<h2 id="PyTorch-YOLOv3"><a href="#PyTorch-YOLOv3" class="headerlink" title="PyTorch-YOLOv3"></a>PyTorch-YOLOv3</h2><h3 id="所需环境"><a href="#所需环境" class="headerlink" title="所需环境"></a>所需环境</h3><p>==在这里先记录一个创建环境的问题==，（要是创建环境的时候遇到问题可以拿去用一用）打开anaconda的时候想要创建一个yolo的环境，就用了最简单的创建环境的命令行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n yolo python=3.7</span><br></pre></td></tr></table></figure>
<p>但是创建了一晚上，需要的那些包都没下载下来，并且报错：</p>
<blockquote>
<p>“Multiple Errors Encountered”</p>
</blockquote>
<p>`解决办法：更换下载源（更换了以后，下载速度快到飞起，我还一直以为是网络问题）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure>
<p>回到正题，需要的环境有：</p>
<p>==PyTorch==环境搭建（具体搭建过程可以参考我的一篇博客<a href="https://phoebeyu731.github.io/2020/05/06/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/" target="_blank" rel="noopener">大创学习记录（二）</a>）<br>==Python== 3.5<br>==OpenCV==（在搭建好的PyTorch环境下输入命令<code>pip install opencv-python</code>即可）</p>
<h3 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h3><p>权重文件下载<a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener">yolov3_weights</a><br>或者使用的是Linux系统，可以在终端输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://pjreddie.com/media/files/yolov3.weights</span><br></pre></td></tr></table></figure>
<h2 id="Run-the-detector"><a href="#Run-the-detector" class="headerlink" title="Run the detector"></a>Run the detector</h2><ol>
<li>下载yolo源代码，进入到代码存放的目录（<a href="https://github.com/ayooshkathuria/pytorch-yolo-v3" target="_blank" rel="noopener">github地址</a>）</li>
<li>下载权重文件，并且放到源代码下载的目录下<br><img src="https://img-blog.csdnimg.cn/20200920123016892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
<li>运行<code>python detect.py --images imgs --det det</code></li>
</ol>
<p> –images标志定义从中加载图像的目录或单个图像文件，而–det是将图像保存到的目录<br><img src="https://img-blog.csdnimg.cn/20200920164849390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对应的结果：<br><img src="https://img-blog.csdnimg.cn/20200920165141558.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>还可以通过更多flag改变精度和速度，输入<code>python detect.py -h</code>可以查看<br><img src="https://img-blog.csdnimg.cn/20200920165515522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>==需要注意：==<br>可以通过–reso标志更改输入图像的分辨率。 默认值为416。无论您选择什么值，都请记住它应该是32的倍数并且大于32（比如<code>python detect.py --images imgs --det det --reso 320</code>）。<br> 4. <strong>On Video</strong><br> 运行<code>python video_demo.py --video video.avi</code><br> 视频文件应为.avi格式，因为openCV仅接受avi作为输入格式。<br> 5. <strong>On a Camera</strong><br> 运行<code>python cam_demo.py</code><br> 这里会打开电脑的摄像头，进行识别（因为懒得给自己打码了，就不把照片po上来了，有兴趣的可以自己去运行一下）。</p>
<p>==改变训练权重：== 一些权重文件的下载地址：<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">yolo website</a><br>==改变物体检测规模：== YOLO v3进行跨不同级别的检测，每种检测都代表检测不同大小的对象，可以通过–scales标志来尝试这些比例，比如输入<code>python detect.py --scales 1,3</code></p>
<h2 id="YOLOv3-Keras（训练自己的权重来预测）"><a href="#YOLOv3-Keras（训练自己的权重来预测）" class="headerlink" title="YOLOv3-Keras（训练自己的权重来预测）"></a>YOLOv3-Keras（训练自己的权重来预测）</h2><p>本文是基于PyTorch的环境下训练的，另一个基于Keras的也是十分重要的，参考文章<a href="https://blog.csdn.net/Patrick_Lxc/article/details/80615433" target="_blank" rel="noopener">Keras/Tensorflow+python+yolo3训练自己的数据集</a><br>以及<a href="https://jennyvanessa.github.io/2020/09/21/%E5%88%A9%E7%94%A8Keras%E5%AE%9E%E7%8E%B0Yolov3/" target="_blank" rel="noopener">jennyvanessa的blog之利用Keras实现Yolov3</a></p>
<h2 id="代码详细分析"><a href="#代码详细分析" class="headerlink" title="代码详细分析"></a>代码详细分析</h2><p>接下来仔细看一下YOLOv3代码的细节，只有在代码中才能完全理解YOLOv3的思想。但是前面那个代码我跑的那个代码只有官方提供的测试的部分，并不包含训练部分，所以又去找了一个完整的代码，附上地址<a href="https://github.com/eriklindernoren/PyTorch-YOLOv3" target="_blank" rel="noopener">PyTorch-YOLOv3github地址</a>，关于这个项目详细的使用以及测试过程在相应的github地址的readme的文档中已经列出，我也已经完全按照上面的过程跑过一遍代码了，没有问题，接下来分析它的代码。</p>
<h3 id="detect-py"><a href="#detect-py" class="headerlink" title="detect.py"></a>detect.py</h3><h4 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.datasets <span class="keyword">import</span> *</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="keyword">from</span> matplotlib.ticker <span class="keyword">import</span> NullLocator</span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">（1）import argparse    首先导入模块</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   （2）parser = argparse.ArgumentParser（）    创建一个解析对象</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    （3）parser.add_argument()    向该对象中添加你要关注的命令行参数和选项</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser.add_argument(<span class="string">"--image_folder"</span>, type=str, default=<span class="string">"data/samples"</span>, help=<span class="string">"path to dataset"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--model_def"</span>, type=str, default=<span class="string">"config/yolov3.cfg"</span>, help=<span class="string">"path to model definition file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--weights_path"</span>, type=str, default=<span class="string">"weights/yolov3.weights"</span>, help=<span class="string">"path to weights file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--class_path"</span>, type=str, default=<span class="string">"data/coco.names"</span>, help=<span class="string">"path to class label file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--conf_thres"</span>, type=float, default=<span class="number">0.8</span>, help=<span class="string">"object confidence threshold"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--nms_thres"</span>, type=float, default=<span class="number">0.4</span>, help=<span class="string">"iou thresshold for non-maximum suppression"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--batch_size"</span>, type=int, default=<span class="number">1</span>, help=<span class="string">"size of the batches"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--n_cpu"</span>, type=int, default=<span class="number">0</span>, help=<span class="string">"number of cpu threads to use during batch generation"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--img_size"</span>, type=int, default=<span class="number">416</span>, help=<span class="string">"size of each image dimension"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--checkpoint_model"</span>, type=str, help=<span class="string">"path to checkpoint model"</span>)</span><br><span class="line">     <span class="string">"""</span></span><br><span class="line"><span class="string">    （4）parser.parse_args()    进行解析</span></span><br><span class="line"><span class="string">     """</span></span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line">    print(opt)</span><br><span class="line">    <span class="comment">#选择是否使用GPU设备</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">     <span class="comment">#创建多级目录</span></span><br><span class="line">    os.makedirs(<span class="string">"output"</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Set up model  调用darknet模型</span></span><br><span class="line">    model = Darknet(opt.model_def, img_size=opt.img_size).to(device)</span><br></pre></td></tr></table></figure>

<p>最后这句话，model = Darknet(opt.model_def, img_size=opt.img_size).to(device)，这条语句加载了==darknet==模型，即==YOLOv3==模型，所以接下来我们再看Darknet模型，这个模型在==model.py==中定义。</p>
<h5 id="YOLOv3（darknet模型）"><a href="#YOLOv3（darknet模型）" class="headerlink" title="YOLOv3（darknet模型）"></a>YOLOv3（darknet模型）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Darknet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""YOLOv3 object detection model"""</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config_path, img_size=<span class="number">416</span>)</span>:</span></span><br><span class="line">        super(Darknet, self).__init__()</span><br><span class="line">        <span class="comment">#解析cfg文件</span></span><br><span class="line">        self.module_defs = parse_model_config(config_path)</span><br><span class="line">        <span class="comment">#print("module_defs   : ",self.module_defs)</span></span><br><span class="line">        self.hyperparams, self.module_list = create_modules(self.module_defs)</span><br><span class="line">        <span class="comment">#print("module_list   : ",self.module_list)</span></span><br><span class="line">        <span class="comment"># hasattr() 函数用于判断对象是否包含对应的属性。</span></span><br><span class="line">        <span class="comment"># yolo层有 metrics 属性</span></span><br><span class="line">        self.yolo_layers = [layer[<span class="number">0</span>] <span class="keyword">for</span> layer <span class="keyword">in</span> self.module_list <span class="keyword">if</span> hasattr(layer[<span class="number">0</span>], <span class="string">"metrics"</span>)]</span><br><span class="line">        <span class="comment">#print("self.yolo_layers:\n",self.yolo_layers)</span></span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.seen = <span class="number">0</span></span><br><span class="line">        self.header_info = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, self.seen, <span class="number">0</span>], dtype=np.int32)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, targets=None)</span>:</span></span><br><span class="line">        img_dim = x.shape[<span class="number">2</span>]</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        layer_outputs, yolo_outputs = [], []</span><br><span class="line">        print(<span class="string">"x.shape: "</span>,x.shape)</span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">            <span class="comment">#print("module_defs   : ",module_def)</span></span><br><span class="line">            <span class="comment">#print("module   : ",module)</span></span><br><span class="line">            <span class="comment">#print("i: ",i," x.shape: ",x.shape)</span></span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] <span class="keyword">in</span> [<span class="string">"convolutional"</span>, <span class="string">"upsample"</span>, <span class="string">"maxpool"</span>]:</span><br><span class="line">                x = module(x)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"route"</span>:</span><br><span class="line">                print(<span class="string">"i: "</span>,i,<span class="string">" x.shape: "</span>,x.shape)</span><br><span class="line">                <span class="keyword">for</span> layer_i <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>):</span><br><span class="line">                    print(<span class="string">"layer_i:\n"</span>,layer_i)</span><br><span class="line">                x = torch.cat([layer_outputs[int(layer_i)] <span class="keyword">for</span> layer_i <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>)], <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">                layer_i = int(module_def[<span class="string">"from"</span>])</span><br><span class="line">                x = layer_outputs[<span class="number">-1</span>] + layer_outputs[layer_i]</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">                x, layer_loss = module[<span class="number">0</span>](x, targets, img_dim)</span><br><span class="line">                loss += layer_loss</span><br><span class="line">                yolo_outputs.append(x)</span><br><span class="line">            layer_outputs.append(x)</span><br><span class="line">        yolo_outputs = to_cpu(torch.cat(yolo_outputs, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> yolo_outputs <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> (loss, yolo_outputs)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_darknet_weights</span><span class="params">(self, weights_path)</span>:</span></span><br><span class="line">        <span class="string">"""Parses and loads the weights stored in 'weights_path'"""</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Open the weights file</span></span><br><span class="line">        <span class="keyword">with</span> open(weights_path, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            header = np.fromfile(f, dtype=np.int32, count=<span class="number">5</span>)  <span class="comment"># First five are header values</span></span><br><span class="line">            self.header_info = header  <span class="comment"># Needed to write header when saving weights</span></span><br><span class="line">            self.seen = header[<span class="number">3</span>]  <span class="comment"># number of images seen during training</span></span><br><span class="line">            weights = np.fromfile(f, dtype=np.float32)  <span class="comment"># The rest are weights</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("------------------------------------")</span></span><br><span class="line"><span class="string">            print("header:\n",header)</span></span><br><span class="line"><span class="string">            print("weights:\n",weights)</span></span><br><span class="line"><span class="string">            print("weights.shape:\n",weights.shape)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">        <span class="comment"># Establish cutoff for loading backbone weights</span></span><br><span class="line">        cutoff = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"darknet53.conv.74"</span> <span class="keyword">in</span> weights_path:</span><br><span class="line">            cutoff = <span class="number">75</span></span><br><span class="line"> </span><br><span class="line">        ptr = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">            <span class="comment">#print("i:\n",i)</span></span><br><span class="line">            <span class="comment">#print("module_def:\n",module_def)</span></span><br><span class="line">            <span class="comment">#print("module:\n",module)</span></span><br><span class="line">            <span class="keyword">if</span> i == cutoff:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] == <span class="string">"convolutional"</span>:</span><br><span class="line">                conv_layer = module[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> module_def[<span class="string">"batch_normalize"</span>]:</span><br><span class="line">                    <span class="comment"># Load BN bias, weights, running mean and running variance</span></span><br><span class="line">                    bn_layer = module[<span class="number">1</span>]</span><br><span class="line">                    num_b = bn_layer.bias.numel()  <span class="comment"># Number of biases</span></span><br><span class="line">                    <span class="comment">#print("bn_layer:\n",bn_layer)</span></span><br><span class="line">                    <span class="comment">#print("num_b:\n",num_b)</span></span><br><span class="line">                    <span class="comment"># Bias</span></span><br><span class="line">                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)</span><br><span class="line">                    bn_layer.bias.data.copy_(bn_b)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                    <span class="comment"># Weight</span></span><br><span class="line">                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)</span><br><span class="line">                    bn_layer.weight.data.copy_(bn_w)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                    <span class="comment"># Running Mean</span></span><br><span class="line">                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)</span><br><span class="line">                    bn_layer.running_mean.data.copy_(bn_rm)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                    <span class="comment"># Running Var</span></span><br><span class="line">                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)</span><br><span class="line">                    bn_layer.running_var.data.copy_(bn_rv)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># Load conv. bias</span></span><br><span class="line">                    num_b = conv_layer.bias.numel()</span><br><span class="line">                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)</span><br><span class="line">                    conv_layer.bias.data.copy_(conv_b)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                <span class="comment"># Load conv. weights</span></span><br><span class="line">                num_w = conv_layer.weight.numel()</span><br><span class="line">                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)</span><br><span class="line">                conv_layer.weight.data.copy_(conv_w)</span><br><span class="line">                ptr += num_w</span><br><span class="line">                <span class="comment">#print("conv_w:\n",conv_w)</span></span><br><span class="line">                <span class="comment">#print("num_w:\n",num_w)</span></span><br><span class="line">                <span class="comment">#print("ptr:\n",ptr)</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_darknet_weights</span><span class="params">(self, path, cutoff=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            @:param path    - path of the new weights file</span></span><br><span class="line"><span class="string">            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -&gt; all are saved)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        fp = open(path, <span class="string">"wb"</span>)</span><br><span class="line">        self.header_info[<span class="number">3</span>] = self.seen</span><br><span class="line">        self.header_info.tofile(fp)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Iterate through layers</span></span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):</span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] == <span class="string">"convolutional"</span>:</span><br><span class="line">                conv_layer = module[<span class="number">0</span>]</span><br><span class="line">                <span class="comment"># If batch norm, load bn first</span></span><br><span class="line">                <span class="keyword">if</span> module_def[<span class="string">"batch_normalize"</span>]:</span><br><span class="line">                    bn_layer = module[<span class="number">1</span>]</span><br><span class="line">                    bn_layer.bias.data.cpu().numpy().tofile(fp)</span><br><span class="line">                    bn_layer.weight.data.cpu().numpy().tofile(fp)</span><br><span class="line">                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)</span><br><span class="line">                    bn_layer.running_var.data.cpu().numpy().tofile(fp)</span><br><span class="line">                <span class="comment"># Load conv bias</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    conv_layer.bias.data.cpu().numpy().tofile(fp)</span><br><span class="line">                <span class="comment"># Load conv weights</span></span><br><span class="line">                conv_layer.weight.data.cpu().numpy().tofile(fp)</span><br><span class="line"> </span><br><span class="line">        fp.close()</span><br></pre></td></tr></table></figure>
<p>首先看<code>__init__()</code>函数，大致流程是从.cfg中解析文件，然后根据文件内容生成相关的网络结构。<br>解析后会生成一个列表，存储网络结构的各种属性，通过遍历这个列表便可以得到网络结构，解析后的列表如下图所示（部分）：<br><img src="https://img-blog.csdnimg.cn/20200921175928444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><code>self.hyperparams, self.module_list = create_modules(self.module_defs)</code>，这条语句会根据生成的列表构建网络结构，<code>create_modules（）</code>函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_modules</span><span class="params">(module_defs)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Constructs module list of layer blocks from module configuration in module_defs</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。</span></span><br><span class="line">    hyperparams = module_defs.pop(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#初始值对应于输入数据通道，"channels"，用来存储我们需要持续追踪被应用卷积层的卷积核数量（上一层的卷积核数量（或特征图深度））,并且我们不仅需要追踪前一层的卷积核数量，还需要追踪之前每个层。随着不断地迭代，我们将每个模块的输出卷积核数量添加到 output_filters 列表上。</span></span><br><span class="line">    output_filters = [int(hyperparams[<span class="string">"channels"</span>])]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># module_list用于存储每个block,每个block对应cfg文件中一个块，类似[convolutional]里面就对应一个卷积块</span></span><br><span class="line">    module_list = nn.ModuleList()</span><br><span class="line">    <span class="comment">#这里，我们迭代module_defs</span></span><br><span class="line">    <span class="keyword">for</span> module_i, module_def <span class="keyword">in</span> enumerate(module_defs):</span><br><span class="line">    <span class="comment"># 这里每个block用nn.sequential()创建为了一个module,一个module有多个层</span></span><br><span class="line">            modules = nn.Sequential()</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> module_def[<span class="string">"type"</span>] == <span class="string">"convolutional"</span>:</span><br><span class="line">        <span class="comment">#设置filter尺寸、数量，添加batch normalize层（在.cfg文件中batch_normalize=1），以及pad层</span></span><br><span class="line">            bn = int(module_def[<span class="string">"batch_normalize"</span>])</span><br><span class="line">            filters = int(module_def[<span class="string">"filters"</span>])</span><br><span class="line">            kernel_size = int(module_def[<span class="string">"size"</span>])</span><br><span class="line">            pad = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># 开始创建并添加相应层</span></span><br><span class="line">            <span class="comment"># Add the convolutional layer</span></span><br><span class="line">            <span class="comment"># nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True)</span></span><br><span class="line">            modules.add_module(</span><br><span class="line">                <span class="string">f"conv_<span class="subst">&#123;module_i&#125;</span>"</span>,</span><br><span class="line">                nn.Conv2d(</span><br><span class="line">                    in_channels=output_filters[<span class="number">-1</span>],</span><br><span class="line">                    out_channels=filters,</span><br><span class="line">                    kernel_size=kernel_size,</span><br><span class="line">                    stride=int(module_def[<span class="string">"stride"</span>]),</span><br><span class="line">                    padding=pad,</span><br><span class="line">                    bias=<span class="keyword">not</span> bn,</span><br><span class="line">                ),</span><br><span class="line">            )</span><br><span class="line">            <span class="comment">#Add the Batch Norm Layer</span></span><br><span class="line">            <span class="keyword">if</span> bn:</span><br><span class="line">                modules.add_module(<span class="string">f"batch_norm_<span class="subst">&#123;module_i&#125;</span>"</span>, nn.BatchNorm2d(filters, momentum=<span class="number">0.9</span>, eps=<span class="number">1e-5</span>))</span><br><span class="line">            <span class="comment">#检查激活函数 </span></span><br><span class="line">            <span class="comment">#It is either Linear or a Leaky ReLU for YOLO</span></span><br><span class="line">            <span class="comment"># 给定参数负轴系数0.1</span></span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"activation"</span>] == <span class="string">"leaky"</span>:</span><br><span class="line">                modules.add_module(<span class="string">f"leaky_<span class="subst">&#123;module_i&#125;</span>"</span>, nn.LeakyReLU(<span class="number">0.1</span>))</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"maxpool"</span>:</span><br><span class="line">            kernel_size = int(module_def[<span class="string">"size"</span>])</span><br><span class="line">            stride = int(module_def[<span class="string">"stride"</span>])</span><br><span class="line">            <span class="keyword">if</span> kernel_size == <span class="number">2</span> <span class="keyword">and</span> stride == <span class="number">1</span>:</span><br><span class="line">                <span class="comment">#保证输出是偶数</span></span><br><span class="line">                modules.add_module(<span class="string">f"_debug_padding_<span class="subst">&#123;module_i&#125;</span>"</span>, nn.ZeroPad2d((<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - <span class="number">1</span>) // <span class="number">2</span>))</span><br><span class="line">            modules.add_module(<span class="string">f"maxpool_<span class="subst">&#123;module_i&#125;</span>"</span>, maxpool)</span><br><span class="line"> </span><br><span class="line">            <span class="string">'''</span></span><br><span class="line"><span class="string">            upsampling layer</span></span><br><span class="line"><span class="string">            没有使用 Bilinear2dUpsampling</span></span><br><span class="line"><span class="string">            实际使用的为最近邻插值</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"upsample"</span>:</span><br><span class="line">            upsample = Upsample(scale_factor=int(module_def[<span class="string">"stride"</span>]), mode=<span class="string">"nearest"</span>)</span><br><span class="line">            <span class="comment">#这个stride在cfg中就是2，所以下面的scale_factor写2或者stride是等价的</span></span><br><span class="line">            modules.add_module(<span class="string">f"upsample_<span class="subst">&#123;module_i&#125;</span>"</span>, upsample)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># route layer -&gt; Empty layer</span></span><br><span class="line">        <span class="comment"># route层的作用：当layer取值为正时，输出这个正数对应的层的特征，如果layer取值为负数，输出route层向后退layer层对应层的特征</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"route"</span>:</span><br><span class="line">            layers = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            filters = sum([output_filters[<span class="number">1</span>:][i] <span class="keyword">for</span> i <span class="keyword">in</span> layers])</span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("------------------------------------")</span></span><br><span class="line"><span class="string">            print("layers:  \n",layers)</span></span><br><span class="line"><span class="string">            print("output_filters:\n",output_filters)</span></span><br><span class="line"><span class="string">            print("output_filters[1:][i] :\n",[output_filters[1:][i] for i in layers])</span></span><br><span class="line"><span class="string">            print("output_filters[1:]:\n",output_filters[1:])</span></span><br><span class="line"><span class="string">            print("output_filters[1:][1]:\n",output_filters[1:][1])</span></span><br><span class="line"><span class="string">            print("output_filters[1:][3]:\n",output_filters[1:][3])</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            modules.add_module(<span class="string">f"route_<span class="subst">&#123;module_i&#125;</span>"</span>, EmptyLayer())</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#shortcut corresponds to skip connection</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">            filters = output_filters[<span class="number">1</span>:][int(module_def[<span class="string">"from"</span>])]</span><br><span class="line">            <span class="comment">#使用空的层，因为它还要执行一个非常简单的操作（加）。没必要更新 filters 变量,因为它只是将前一层的特征图添加到后面的层上而已。</span></span><br><span class="line">            modules.add_module(<span class="string">f"shortcut_<span class="subst">&#123;module_i&#125;</span>"</span>, EmptyLayer())</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#Yolo is the detection layer</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">            anchor_idxs = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"mask"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            <span class="comment"># Extract anchors</span></span><br><span class="line">            <span class="comment">#print("----------------------------------")</span></span><br><span class="line">            <span class="comment">#print("anchor_idxs\n:",anchor_idxs)</span></span><br><span class="line">            anchors = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"anchors"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            <span class="comment">#print("1. anchors \n:",anchors)</span></span><br><span class="line">            anchors = [(anchors[i], anchors[i + <span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(anchors), <span class="number">2</span>)]</span><br><span class="line">            <span class="comment">#print("2. anchors \n:",anchors)</span></span><br><span class="line">            anchors = [anchors[i] <span class="keyword">for</span> i <span class="keyword">in</span> anchor_idxs]</span><br><span class="line">            <span class="comment">#print("3. anchors \n:",anchors)</span></span><br><span class="line">            num_classes = int(module_def[<span class="string">"classes"</span>])</span><br><span class="line">            img_size = int(hyperparams[<span class="string">"height"</span>])</span><br><span class="line">            <span class="comment"># Define detection layer</span></span><br><span class="line">            <span class="comment"># 锚点,检测,位置回归,分类，这个类会在后面分析</span></span><br><span class="line">            yolo_layer = YOLOLayer(anchors, num_classes, img_size)</span><br><span class="line">            modules.add_module(<span class="string">f"yolo_<span class="subst">&#123;module_i&#125;</span>"</span>, yolo_layer)</span><br><span class="line">        <span class="comment"># Register module list and number of output filters</span></span><br><span class="line">        module_list.append(modules)</span><br><span class="line">        output_filters.append(filters)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> hyperparams, module_list</span><br></pre></td></tr></table></figure>
<p>create_module()传入配置文件中网络结构的定义的属性，根据列表会生成相应的网络结构，我们使用的配置文件定义了6中不同的type，convolutional、maxpool、upsample、route、shortcut、yolo层。</p>
<p>==convolutional层==构建方法很常规：设置filter尺寸、数量，添加batch normalize层（在.cfg文件中batch_normalize=1），以及pad层，使用leaky激活函数。</p>
<p>==maxpool层==，不过在YOLOv3中没有使用最大池化来进行下采样，是使用的3*3的卷积核，步长=2的卷积操作进行下采样，一共5次，下采样2^5=32倍数。<br><img src="https://img-blog.csdnimg.cn/20200922165908202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>==upsample层==，上采样层。<br>==route层==，这层十分重要。这层的作用相当于把前面的特征图进行相融合。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[route]</span><br><span class="line">layers = -4      <span class="comment"># 只有一个值，一个路径</span></span><br><span class="line"> </span><br><span class="line">[route]</span><br><span class="line">layers = -1, 61  <span class="comment"># 两个值，两个路径，两个特征图进行特征融合</span></span><br></pre></td></tr></table></figure>
<p>==shortcut层==，直连层，借鉴于ResNet网络。关于ResNet网络更多细节可以查看<a href="https://cloud.tencent.com/developer/article/1148375" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1148375</a>和<a href="https://blog.csdn.net/u014665013/article/details/81985082" target="_blank" rel="noopener">https://blog.csdn.net/u014665013/article/details/81985082</a><br>YOLOv3完整的结构有100+层，所以采用直连的方式来优化网络结构，能使网络更好的训练、更快的收敛。值得注意的是，YOLOv3的shortcut层是把网络的值进行叠加，没有改变特征图的大小，所以仔细会发现在shortcut层的前后，输入输出大小没变。<br> <img src="https://img-blog.csdnimg.cn/20200922171331565.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200922171300997.png#pic_center" alt="在这里插入图片描述"><br>==yolo层==（重点！）<br>仔细看上图的五次采样，会发现有三个Scale，分别是Scale1（下采样8倍）,Scale2（下采样16倍），Scale3（下采样2^5=32倍），此时网络默认的尺寸是416<em>416，对应的feature map为52</em>52，26<em>26，13</em>13。这里借用一幅图：<br><a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a><br><img src="https://img-blog.csdnimg.cn/20200922172118534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里是YOLOv3的多尺度检测的思想的体现，使用3种尺度，是为了加强对小目标的检测，这个应该是借鉴SSD的思想。比较大的特征图来检测相对较小的目标，而小的特征图负责检测大目标。<br>在有多尺度的概念下，使用k-means得到9个先验框的尺寸（416<em>416的尺寸下）。<br>*</em>解析yolo层代码**（加入代码，将每一层的参数打印出来观察）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">            anchor_idxs = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"mask"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            <span class="comment"># Extract anchors</span></span><br><span class="line">            print(<span class="string">"----------------------------------"</span>)</span><br><span class="line">            print(<span class="string">"anchor_idxs\n:"</span>,anchor_idxs)</span><br><span class="line">            anchors = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"anchors"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            print(<span class="string">"1. anchors \n:"</span>,anchors)</span><br><span class="line">            anchors = [(anchors[i], anchors[i + <span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(anchors), <span class="number">2</span>)]</span><br><span class="line">            print(<span class="string">"2. anchors \n:"</span>,anchors)</span><br><span class="line">            anchors = [anchors[i] <span class="keyword">for</span> i <span class="keyword">in</span> anchor_idxs]</span><br><span class="line">            print(<span class="string">"3. anchors \n:"</span>,anchors)</span><br><span class="line">            num_classes = int(module_def[<span class="string">"classes"</span>])</span><br><span class="line">            img_size = int(hyperparams[<span class="string">"height"</span>])</span><br><span class="line">            <span class="comment"># Define detection layer</span></span><br><span class="line">            yolo_layer = YOLOLayer(anchors, num_classes, img_size)</span><br><span class="line">            modules.add_module(<span class="string">f"yolo_<span class="subst">&#123;module_i&#125;</span>"</span>, yolo_layer)</span><br></pre></td></tr></table></figure>
<p>可以看到输出：<br><img src="https://img-blog.csdnimg.cn/20200922172805375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>可以看到yolo层搭建了三次，第一个yolo层是下采样2^5=32倍，特征图尺寸是13*13（默认输入416 * 416，下同）。这层选择mask的ID是6，7，8，对应的anchor box尺寸是（116， 90）、（156， 198）、（373， 326）。这对应了上面所说的，小的特征图检测大目标，所以使用的anchor box最大。</p>
<p>至此，Darknet(YOLOv3)模型基本加载完毕，接下来就是，加载权重.weights文件，进行预测。</p>
<h4 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h4><h5 id="获取检测框"><a href="#获取检测框" class="headerlink" title="获取检测框"></a>获取检测框</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查找weights_path路径下的.weights的文件</span></span><br><span class="line">    <span class="keyword">if</span> opt.weights_path.endswith(<span class="string">".weights"</span>):</span><br><span class="line">        <span class="comment"># Load darknet weights</span></span><br><span class="line">        model.load_darknet_weights(opt.weights_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Load checkpoint weights</span></span><br><span class="line">        model.load_state_dict(torch.load(opt.weights_path))</span><br><span class="line">    <span class="comment"># model.eval()，让model变成测试模式，这主要是对dropout和batch normalization的操作在训练和测试的时候是不一样的</span></span><br><span class="line">    model.eval()  <span class="comment"># Set in evaluation mode</span></span><br><span class="line"> </span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        ImageFolder(opt.image_folder, img_size=opt.img_size),</span><br><span class="line">        batch_size=opt.batch_size,</span><br><span class="line">        shuffle=<span class="literal">False</span>,</span><br><span class="line">        num_workers=opt.n_cpu,</span><br><span class="line">    )</span><br><span class="line"> </span><br><span class="line">    classes = load_classes(opt.class_path)  <span class="comment"># Extracts class labels from file</span></span><br><span class="line"> </span><br><span class="line">    Tensor = torch.cuda.FloatTensor <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line"> </span><br><span class="line">    imgs = []  <span class="comment"># Stores image paths</span></span><br><span class="line">    img_detections = []  <span class="comment"># Stores detections for each image index</span></span><br><span class="line"> </span><br><span class="line">    print(<span class="string">"\nPerforming object detection:"</span>)</span><br><span class="line">    <span class="comment">#返回当前时间的时间戳</span></span><br><span class="line">    prev_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> batch_i, (img_paths, input_imgs) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        <span class="comment"># Configure input</span></span><br><span class="line">        input_imgs = Variable(input_imgs.type(Tensor))</span><br><span class="line">        <span class="comment">#print("img_paths:\n",img_paths)</span></span><br><span class="line">        <span class="comment"># Get detections</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment">#52*52+26*26+13*13）*3=10647</span></span><br><span class="line">            <span class="comment"># 5 + 80 =85</span></span><br><span class="line">            <span class="comment"># detections : 10647*85</span></span><br><span class="line">            detections = model(input_imgs)            </span><br><span class="line">            <span class="comment">#非极大值抑制</span></span><br><span class="line">            detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)</span><br><span class="line">            <span class="comment">#print("detections:\n",detections)</span></span><br><span class="line">        <span class="comment"># Log progress</span></span><br><span class="line">        current_time = time.time()</span><br><span class="line">        <span class="comment">#timedelta代表两个datetime之间的时间差</span></span><br><span class="line">        inference_time = datetime.timedelta(seconds=current_time - prev_time)</span><br><span class="line">        prev_time = current_time</span><br><span class="line">        print(<span class="string">"\t+ Batch %d, Inference Time: %s"</span> % (batch_i, inference_time))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Save image and detections</span></span><br><span class="line">        <span class="comment">#extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。</span></span><br><span class="line">        imgs.extend(img_paths)</span><br><span class="line">        img_detections.extend(detections)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Bounding-box colors</span></span><br><span class="line">    cmap = plt.get_cmap(<span class="string">"tab20b"</span>)</span><br><span class="line">    colors = [cmap(i) <span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">20</span>)]</span><br><span class="line"> </span><br><span class="line">    print(<span class="string">"\nSaving images:"</span>)</span><br><span class="line">    <span class="comment"># Iterate through images and save plot of detections</span></span><br><span class="line">    <span class="keyword">for</span> img_i, (path, detections) <span class="keyword">in</span> enumerate(zip(imgs, img_detections)):</span><br><span class="line"> </span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"(%d) Image: '%s'"</span> % (img_i, path))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Create plot</span></span><br><span class="line">        img = np.array(Image.open(path))</span><br><span class="line">        plt.figure()</span><br><span class="line">        fig, ax = plt.subplots(<span class="number">1</span>)</span><br><span class="line">        ax.imshow(img)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Draw bounding boxes and labels of detections</span></span><br><span class="line">        <span class="keyword">if</span> detections <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Rescale boxes to original image</span></span><br><span class="line">            detections  = rescale_boxes(detections, opt.img_size, img.shape[:<span class="number">2</span>])</span><br><span class="line">            unique_labels = detections[:, <span class="number">-1</span>].cpu().unique()</span><br><span class="line">            n_cls_preds = len(unique_labels)</span><br><span class="line">            bbox_colors = random.sample(colors, n_cls_preds)</span><br><span class="line">            <span class="keyword">for</span> x1, y1, x2, y2, conf, cls_conf, cls_pred <span class="keyword">in</span> detections:</span><br><span class="line"> </span><br><span class="line">                print(<span class="string">"\t+ Label: %s, Conf: %.5f"</span> % (classes[int(cls_pred)], cls_conf.item()))</span><br><span class="line"> </span><br><span class="line">                box_w = x2 - x1</span><br><span class="line">                box_h = y2 - y1</span><br><span class="line"> </span><br><span class="line">                color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[<span class="number">0</span>])]</span><br><span class="line">                <span class="comment"># Create a Rectangle patch</span></span><br><span class="line">                bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=<span class="number">2</span>, edgecolor=color, facecolor=<span class="string">"none"</span>)</span><br><span class="line">                <span class="comment"># Add the bbox to the plot</span></span><br><span class="line">                ax.add_patch(bbox)</span><br><span class="line">                <span class="comment"># Add label</span></span><br><span class="line">                plt.text(</span><br><span class="line">                    x1,</span><br><span class="line">                    y1,</span><br><span class="line">                    s=classes[int(cls_pred)],</span><br><span class="line">                    color=<span class="string">"white"</span>,</span><br><span class="line">                    verticalalignment=<span class="string">"top"</span>,</span><br><span class="line">                    bbox=&#123;<span class="string">"color"</span>: color, <span class="string">"pad"</span>: <span class="number">0</span>&#125;,</span><br><span class="line">                )</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Save generated image with detections</span></span><br><span class="line">        plt.axis(<span class="string">"off"</span>)</span><br><span class="line">        plt.gca().xaxis.set_major_locator(NullLocator())</span><br><span class="line">        plt.gca().yaxis.set_major_locator(NullLocator())</span><br><span class="line">        filename = path.split(<span class="string">"/"</span>)[<span class="number">-1</span>].split(<span class="string">"."</span>)[<span class="number">0</span>]</span><br><span class="line">        plt.savefig(<span class="string">f"output/<span class="subst">&#123;filename&#125;</span>.jpg"</span>, bbox_inches=<span class="string">"tight"</span>, pad_inches=<span class="number">0.0</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.close()</span><br></pre></td></tr></table></figure>
<p><code>model.load_darknet_weights(opt.weights_path)</code>,通过这个语句加载yolov3.weights。加载完.weights文件之后，便开始加载测试图片数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(</span><br><span class="line">        ImageFolder(opt.image_folder, img_size=opt.img_size),</span><br><span class="line">        batch_size=opt.batch_size,</span><br><span class="line">        shuffle=<span class="literal">False</span>,</span><br><span class="line">        num_workers=opt.n_cpu,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>ImageFolder是遍历文件夹下的测试图片，完整定义如下。ImageFolder中的<strong>getitem</strong>()函数会把图像归一化处理成img_size(默认416)大小的图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageFolder</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, folder_path, img_size=<span class="number">416</span>)</span>:</span></span><br><span class="line">        <span class="comment">#sorted(iterable[, cmp[, key[, reverse]]])</span></span><br><span class="line">        <span class="comment">#sorted() 函数对所有可迭代的对象进行排序操作</span></span><br><span class="line">        <span class="comment">##获取指定目录下的所有文件</span></span><br><span class="line">        self.files = sorted(glob.glob(<span class="string">"%s/*.*"</span> % folder_path))</span><br><span class="line">        self.img_size = img_size</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img_path = self.files[index % len(self.files)]</span><br><span class="line">        <span class="comment"># Extract image as PyTorch tensor</span></span><br><span class="line">        img = transforms.ToTensor()(Image.open(img_path))</span><br><span class="line">        <span class="comment"># Pad to square resolution 变成方形</span></span><br><span class="line">        img, _ = pad_to_square(img, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Resize</span></span><br><span class="line">        img = resize(img, self.img_size)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> img_path, img</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.files)</span><br></pre></td></tr></table></figure>
<p>回到==detect.py==中，<code>detections = model(input_imgs)</code>，把图像放进模型中，得到检测结果。这里是通过Darknet的forward()函数得到检测结果。其完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, targets=None)</span>:</span></span><br><span class="line">        img_dim = x.shape[<span class="number">2</span>]</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        layer_outputs, yolo_outputs = [], []</span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] <span class="keyword">in</span> [<span class="string">"convolutional"</span>, <span class="string">"upsample"</span>, <span class="string">"maxpool"</span>]:</span><br><span class="line">                x = module(x)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"route"</span>:</span><br><span class="line">                x = torch.cat([layer_outputs[int(layer_i)] <span class="keyword">for</span> layer_i <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>)], <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">                layer_i = int(module_def[<span class="string">"from"</span>])</span><br><span class="line">                x = layer_outputs[<span class="number">-1</span>] + layer_outputs[layer_i]</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">                x, layer_loss = module[<span class="number">0</span>](x, targets, img_dim)</span><br><span class="line">                loss += layer_loss</span><br><span class="line">                yolo_outputs.append(x)</span><br><span class="line">            layer_outputs.append(x)</span><br><span class="line">        yolo_outputs = to_cpu(torch.cat(yolo_outputs, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> yolo_outputs <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> (loss, yolo_outputs)</span><br></pre></td></tr></table></figure>
<p>通过遍历==self.module_defs==,与==self.module_list==，来完成网络的前向传播。<br>如果是”<strong>convolutional</strong>“, “<strong>upsample</strong>“, “<strong>maxpool</strong>“层，则直接使用前向传播即可。<br>如果是<strong>route</strong>层，则使用torch.cat()完成特征图的融合（拼接）。<br>比如，我前面用来测试的一张图：</p>
<p><img src="https://img-blog.csdnimg.cn/20200922174400667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200922174423704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这张图的尺寸为3 * 768 * 576，我们看看放进模型进行测试的时候，其shape是如何变化的。图像会根据cfg归一化成416 * 416.<br><img src="https://img-blog.csdnimg.cn/20200922174534924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>接下来查看一下route层对应的ID以及shape：<br><img src="https://img-blog.csdnimg.cn/20200922174656406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>该模型的每一层的输出通过<strong>layer_outputs.append(x)</strong>，保存在<strong>layer_outputs</strong>列表中，本次结构完全符合本文前面所论述的部分。如果layer只有一个值，那么该<strong>route</strong>层的输出就是该层。如果layer有两个值，则route层输出是对应两个层的特征图的融合。</p>
<p>如果是<strong>shortcut</strong>层，则特别清晰，直接对应两层相叠加即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">               layer_i = int(module_def[<span class="string">"from"</span>])</span><br><span class="line">               x = layer_outputs[<span class="number">-1</span>] + layer_outputs[layer_i]</span><br></pre></td></tr></table></figure>
<p>如果是yolo层，yolo层有三个，分别对应的特征图大小为13<em>13，26</em>26，52*52。每一个特征图的每一个cell会预测3个bounding boxes。每一个bounding box会预测预测三类值：</p>
<ol>
<li>每个框的位置（4个值，中心坐标tx和ty，，框的高度bh和宽度bw），</li>
<li>一个objectness prediction ，一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量</li>
<li>N个类别，COCO有80类，VOC有20类。</li>
</ol>
<p>所以不难理解，在这里是COCO数据集，在13<em>13的特征图中，一共有*</em>13 * 13 * 3=507**个bounding boxes，每一个bounding box预测（4+1+80=85）个值，用张量的形式表示为[1, 507, 85]，那个1表示的是batch size。同理，其余张量的shape不难理解。</p>
<p><img src="https://img-blog.csdnimg.cn/2020092217515347.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>那么如何得到这个张量呢</strong>，主要要了解yolo层的==forward()== 和 ==compute_grid_offstes==，其完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YOLOLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Detection layer"""</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, anchors, num_classes, img_dim=<span class="number">416</span>)</span>:</span></span><br><span class="line">        super(YOLOLayer, self).__init__()</span><br><span class="line">        self.anchors = anchors</span><br><span class="line">        self.num_anchors = len(anchors)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.ignore_thres = <span class="number">0.5</span></span><br><span class="line">        self.mse_loss = nn.MSELoss()</span><br><span class="line">        self.bce_loss = nn.BCELoss()</span><br><span class="line">        self.obj_scale = <span class="number">1</span></span><br><span class="line">        self.noobj_scale = <span class="number">100</span></span><br><span class="line">        self.metrics = &#123;&#125;</span><br><span class="line">        self.img_dim = img_dim</span><br><span class="line">        self.grid_size = <span class="number">0</span>  <span class="comment"># grid size</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_grid_offsets</span><span class="params">(self, grid_size, cuda=True)</span>:</span></span><br><span class="line">        self.grid_size = grid_size</span><br><span class="line">        g = self.grid_size</span><br><span class="line">        FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">        self.stride = self.img_dim / self.grid_size</span><br><span class="line">        <span class="comment"># Calculate offsets for each grid</span></span><br><span class="line">        <span class="comment">#repeat 相当于一个broadcasting的机制repeat(*sizes)</span></span><br><span class="line">        <span class="comment">#沿着指定的维度重复tensor。不同与expand()，本函数复制的是tensor中的数据。</span></span><br><span class="line">        self.grid_x = torch.arange(g).repeat(g, <span class="number">1</span>).view([<span class="number">1</span>, <span class="number">1</span>, g, g]).type(FloatTensor)</span><br><span class="line">        self.grid_y = torch.arange(g).repeat(g, <span class="number">1</span>).t().view([<span class="number">1</span>, <span class="number">1</span>, g, g]).type(FloatTensor)</span><br><span class="line">        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) <span class="keyword">for</span> a_w, a_h <span class="keyword">in</span> self.anchors])</span><br><span class="line">        self.anchor_w = self.scaled_anchors[:, <span class="number">0</span>:<span class="number">1</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.anchor_h = self.scaled_anchors[:, <span class="number">1</span>:<span class="number">2</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, targets=None, img_dim=None)</span>:</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Tensors for cuda support</span></span><br><span class="line">        FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">        LongTensor = torch.cuda.LongTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.LongTensor</span><br><span class="line">        ByteTensor = torch.cuda.ByteTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.ByteTensor</span><br><span class="line"> </span><br><span class="line">        self.img_dim = img_dim</span><br><span class="line">        num_samples = x.size(<span class="number">0</span>)</span><br><span class="line">        grid_size = x.size(<span class="number">2</span>)</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        所以在输入为416*416时，每个cell的三个anchor box为(116 ,90);</span></span><br><span class="line"><span class="string">        (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为</span></span><br><span class="line"><span class="string">        (30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，</span></span><br><span class="line"><span class="string">        因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416*416时，</span></span><br><span class="line"><span class="string">        实际总共有（52*52+26*26+13*13）*3=10647个proposal box。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        prediction = (</span><br><span class="line">            x.view(num_samples, self.num_anchors, self.num_classes + <span class="number">5</span>, grid_size, grid_size)</span><br><span class="line">            .permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">        )</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("----------------------------------")</span></span><br><span class="line"><span class="string">        print("num_samples:\n",num_samples)</span></span><br><span class="line"><span class="string">        print("self.num_anchors:\n",self.num_anchors)</span></span><br><span class="line"><span class="string">        print("self.grid_size:\n",self.grid_size)</span></span><br><span class="line"><span class="string">        print("grid_size:\n",grid_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#print("x:\n",x)</span></span><br><span class="line">        <span class="comment">#print("prediction:\n",prediction)</span></span><br><span class="line">        <span class="comment"># Get outputs</span></span><br><span class="line">        <span class="comment">#print("prediction\n:",prediction)</span></span><br><span class="line">        <span class="comment">#print("prediction.shape:\n",prediction.shape)</span></span><br><span class="line">        x = torch.sigmoid(prediction[..., <span class="number">0</span>])  <span class="comment"># Center x</span></span><br><span class="line">        </span><br><span class="line">        y = torch.sigmoid(prediction[..., <span class="number">1</span>])  <span class="comment"># Center y</span></span><br><span class="line">        w = prediction[..., <span class="number">2</span>]  <span class="comment"># Width</span></span><br><span class="line">        h = prediction[..., <span class="number">3</span>]  <span class="comment"># Height</span></span><br><span class="line">        pred_conf = torch.sigmoid(prediction[..., <span class="number">4</span>])  <span class="comment"># Conf</span></span><br><span class="line">        pred_cls = torch.sigmoid(prediction[..., <span class="number">5</span>:])  <span class="comment"># Cls pred.</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("anchors \n:",self.anchors)</span></span><br><span class="line"><span class="string">        print("x.shape\n:",x.shape)</span></span><br><span class="line"><span class="string">        print("y.shape\n:",y.shape)</span></span><br><span class="line"><span class="string">        print("w.shape\n:",w.shape)</span></span><br><span class="line"><span class="string">        print("h.shape\n:",h.shape)</span></span><br><span class="line"><span class="string">        print("pred_conf.shape\n:",pred_conf.shape)</span></span><br><span class="line"><span class="string">        print("pred_cls.shape\n:",pred_cls.shape)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># If grid size does not match current we compute new offsets</span></span><br><span class="line">        <span class="keyword">if</span> grid_size != self.grid_size:</span><br><span class="line">            print(<span class="string">"··················different··················"</span>)</span><br><span class="line">            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Add offset and scale with anchors</span></span><br><span class="line">        pred_boxes = FloatTensor(prediction[..., :<span class="number">4</span>].shape)</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("prediction[..., :4].shape:\n",prediction[..., :4].shape)</span></span><br><span class="line"><span class="string">        print("self.grid_x:\n",self.grid_x)</span></span><br><span class="line"><span class="string">        print("self.grid_y:\n",self.grid_y)</span></span><br><span class="line"><span class="string">        print("self.anchor_w:\n",self.anchor_w)</span></span><br><span class="line"><span class="string">        print("self.anchor_h:\n",self.anchor_h)</span></span><br><span class="line"><span class="string">        print("self.anchors:\n",self.anchors)</span></span><br><span class="line"><span class="string">        print("self.stride:\n",self.stride)  </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pred_boxes[..., <span class="number">0</span>] = x.data + self.grid_x</span><br><span class="line">        pred_boxes[..., <span class="number">1</span>] = y.data + self.grid_y</span><br><span class="line">        pred_boxes[..., <span class="number">2</span>] = torch.exp(w.data) * self.anchor_w</span><br><span class="line">        pred_boxes[..., <span class="number">3</span>] = torch.exp(h.data) * self.anchor_h</span><br><span class="line">        <span class="comment">#torch.cat 按最后一维拼接</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("pred_boxes.view(num_samples, -1, 4).shape:\n",pred_boxes.view(num_samples, -1, 4).shape)</span></span><br><span class="line"><span class="string">        print("pred_conf.view(num_samples, -1, 1).shape:\n",pred_conf.view(num_samples, -1, 1).shape)</span></span><br><span class="line"><span class="string">        print("pred_cls.view(num_samples, -1, self.num_classes).shape:\n",pred_cls.view(num_samples, -1, self.num_classes).shape)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        output = torch.cat(</span><br><span class="line">            (</span><br><span class="line">                pred_boxes.view(num_samples, <span class="number">-1</span>, <span class="number">4</span>) * self.stride,</span><br><span class="line">                pred_conf.view(num_samples, <span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">                pred_cls.view(num_samples, <span class="number">-1</span>, self.num_classes),</span><br><span class="line">            ),</span><br><span class="line">            <span class="number">-1</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#print("output.shape:\n",output.shape)</span></span><br><span class="line">        <span class="comment">#print("targets:\n",targets)</span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output, <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(</span><br><span class="line">                pred_boxes=pred_boxes,</span><br><span class="line">                pred_cls=pred_cls,</span><br><span class="line">                target=targets,</span><br><span class="line">                anchors=self.scaled_anchors,</span><br><span class="line">                ignore_thres=self.ignore_thres,</span><br><span class="line">            )</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Loss : Mask outputs to ignore non-existing objects (except with conf. loss)</span></span><br><span class="line">            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])</span><br><span class="line">            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])</span><br><span class="line">            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])</span><br><span class="line">            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])</span><br><span class="line">            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])</span><br><span class="line">            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj</span><br><span class="line">            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])</span><br><span class="line">            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Metrics</span></span><br><span class="line">            cls_acc = <span class="number">100</span> * class_mask[obj_mask].mean()</span><br><span class="line">            conf_obj = pred_conf[obj_mask].mean()</span><br><span class="line">            conf_noobj = pred_conf[noobj_mask].mean()</span><br><span class="line">            conf50 = (pred_conf &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou50 = (iou_scores &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou75 = (iou_scores &gt; <span class="number">0.75</span>).float()</span><br><span class="line">            detected_mask = conf50 * class_mask * tconf</span><br><span class="line">            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">            self.metrics = &#123;</span><br><span class="line">                <span class="string">"loss"</span>: to_cpu(total_loss).item(),</span><br><span class="line">                <span class="string">"x"</span>: to_cpu(loss_x).item(),</span><br><span class="line">                <span class="string">"y"</span>: to_cpu(loss_y).item(),</span><br><span class="line">                <span class="string">"w"</span>: to_cpu(loss_w).item(),</span><br><span class="line">                <span class="string">"h"</span>: to_cpu(loss_h).item(),</span><br><span class="line">                <span class="string">"conf"</span>: to_cpu(loss_conf).item(),</span><br><span class="line">                <span class="string">"cls"</span>: to_cpu(loss_cls).item(),</span><br><span class="line">                <span class="string">"cls_acc"</span>: to_cpu(cls_acc).item(),</span><br><span class="line">                <span class="string">"recall50"</span>: to_cpu(recall50).item(),</span><br><span class="line">                <span class="string">"recall75"</span>: to_cpu(recall75).item(),</span><br><span class="line">                <span class="string">"precision"</span>: to_cpu(precision).item(),</span><br><span class="line">                <span class="string">"conf_obj"</span>: to_cpu(conf_obj).item(),</span><br><span class="line">                <span class="string">"conf_noobj"</span>: to_cpu(conf_noobj).item(),</span><br><span class="line">                <span class="string">"grid_size"</span>: grid_size,</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> output, total_loss</span><br></pre></td></tr></table></figure>
<p><strong>num_samples</strong>是每一批有多少张图片，<strong>grid_size</strong>是特征图的大小。<br><img src="https://img-blog.csdnimg.cn/20200922175739780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>使用<strong>torch.view</strong>,改变输入<strong>yolo</strong>层的张量结构（shape），以<strong>prediction</strong>命名的张量进行预测处理。<br><img src="https://img-blog.csdnimg.cn/20200922175934750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>接下来是便是对边框进行预测，具体细节可以参考：<a href="https://blog.csdn.net/qq_34199326/article/details/84109828" target="_blank" rel="noopener">https://blog.csdn.net/qq_34199326/article/details/84109828</a>。x，y坐标都是使用了sigmoid函数进行处理，置信度和类别概率使用同样的方法处理。</p>
<p>论文中的边界框预测：<br><img src="https://img-blog.csdnimg.cn/20200922180246468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p>Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of ﬁlter application using a sigmoid function. This ﬁgure blatantly self-plagiarized from.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.sigmoid(prediction[..., <span class="number">0</span>])  <span class="comment"># Center x</span></span><br><span class="line">        y = torch.sigmoid(prediction[..., <span class="number">1</span>])  <span class="comment"># Center y</span></span><br><span class="line">        w = prediction[..., <span class="number">2</span>]  <span class="comment"># Width</span></span><br><span class="line">        h = prediction[..., <span class="number">3</span>]  <span class="comment"># Height</span></span><br><span class="line">        pred_conf = torch.sigmoid(prediction[..., <span class="number">4</span>])  <span class="comment"># Conf</span></span><br><span class="line">        pred_cls = torch.sigmoid(prediction[..., <span class="number">5</span>:])  <span class="comment"># Cls pred.</span></span><br></pre></td></tr></table></figure>
<p>在3个尺度下，分别进行预测坐标、置信度、类别概率。<br><img src="https://img-blog.csdnimg.cn/20200922180614936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>从图中我们发现<strong>grid_size</strong>和<strong>self.grid_size</strong>是不相等的，所以需要进行计算偏移，即<strong>compute_grid_offsets</strong>。完整代码在==YOLOLayer==中。</p>
<p>以gird=13为例。此时特征图是13 * 13，但原图shape尺寸是416 * 416，所以要把416 * 416评价切成13 * 13个方格，需要得到间隔（步距<strong>self.stride</strong>=416/13=32）。相应的并把anchor的尺寸进行缩放，即<strong>116/32=3.6250，90/32=2.8125</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/20200922180935263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>前面已经说过每一个小方格（cell），都会预测3个边界框，同样以gird=13为列。第一个小方格（cell），会预测3个边界框，每个边界框都有坐标+置信度+类别概率。所以以下代码中的x.shape=[1, 3, 13, 13],并且与y,w,h的shape一致。<br>同时由于在最后进行拼接，得到输出output 。其<strong>507=13 * 13 * 3</strong>，<strong>2028=26 * 26 * 3</strong>，<strong>8112=52 * 52 * 3</strong>不难理解。<br><img src="https://img-blog.csdnimg.cn/20200922181448190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h5><p>代码涉及部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># detections : 10647*85</span></span><br><span class="line">            detections = model(input_imgs)            </span><br><span class="line">            <span class="comment">#非极大值抑制</span></span><br><span class="line">            detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)</span><br></pre></td></tr></table></figure>
<p>在获取检测框之后，需要使用非极大值抑制来筛选框。即 <code>detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)</code></p>
<p>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">non_max_suppression</span><span class="params">(prediction, conf_thres=<span class="number">0.5</span>, nms_thres=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes detections with lower object confidence score than 'conf_thres' and performs</span></span><br><span class="line"><span class="string">    Non-Maximum Suppression to further filter detections.</span></span><br><span class="line"><span class="string">    Returns detections with shape:</span></span><br><span class="line"><span class="string">        (x1, y1, x2, y2, object_conf, class_score, class_pred)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># From (center x, center y, width, height) to (x1, y1, x2, y2)</span></span><br><span class="line">    prediction[..., :<span class="number">4</span>] = xywh2xyxy(prediction[..., :<span class="number">4</span>])</span><br><span class="line">    output = [<span class="literal">None</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(prediction))]</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> image_i, image_pred <span class="keyword">in</span> enumerate(prediction):</span><br><span class="line">        <span class="comment"># Filter out confidence scores below threshold</span></span><br><span class="line">        print(<span class="string">"------------------------------"</span>)</span><br><span class="line">        <span class="comment">#print("image_i:\n",image_i)</span></span><br><span class="line">        print(<span class="string">"image_pred.shape:\n"</span>,image_pred.shape)</span><br><span class="line">        image_pred = image_pred[image_pred[:, <span class="number">4</span>] &gt;= conf_thres]<span class="comment">#保留大于置信度的边界框</span></span><br><span class="line">        print(<span class="string">"image_pred.size(0)"</span>,image_pred.size(<span class="number">0</span>))</span><br><span class="line">        <span class="comment"># If none are remaining =&gt; process next image</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_pred.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># Object confidence times class confidence</span></span><br><span class="line">        <span class="comment"># .max(1) 返回每行tensor的最大值  .max(1)[0]具体的最大值 .max(1)[1] 最大值对应的索引</span></span><br><span class="line">        score = image_pred[:, <span class="number">4</span>] * image_pred[:, <span class="number">5</span>:].max(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("image_pred[:, 5:]:\n",image_pred[:, 5:])</span></span><br><span class="line"><span class="string">        print("image_pred[:, 5:].max(1):\n",image_pred[:, 5:].max(1))</span></span><br><span class="line"><span class="string">        print("image_pred[:, 5:].max(1)[0]:\n",image_pred[:, 5:].max(1)[0])</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Sort by it</span></span><br><span class="line">        <span class="comment"># 完成从大到小排序 </span></span><br><span class="line">        image_pred = image_pred[(-score).argsort()]</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("score:\n",score)</span></span><br><span class="line"><span class="string">        print("(-score).argsort():\n",(-score).argsort())</span></span><br><span class="line"><span class="string">        print("image_pred:\n",image_pred)\</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。</span></span><br><span class="line">        <span class="comment">#否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，</span></span><br><span class="line">        <span class="comment">#最终输出张量会比输入张量少一个维度。</span></span><br><span class="line">        class_confs, class_preds = image_pred[:, <span class="number">5</span>:].max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#print("image_pred[:, 5:].max(1, keepdim=True):\n",image_pred[:, 5:].max(1, keepdim=True))        </span></span><br><span class="line">        <span class="comment">#print("image_pred[:, 5:].max(1, keepdim=False):\n",image_pred[:, 5:].max(1, keepdim=False))        </span></span><br><span class="line">        detections = torch.cat((image_pred[:, :<span class="number">5</span>], class_confs.float(), class_preds.float()), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Perform non-maximum suppression</span></span><br><span class="line">        <span class="comment">#print("detections.size():\n",detections.size())</span></span><br><span class="line">        <span class="comment">#print("detections.size(0):\n",detections.size(0))    </span></span><br><span class="line">        <span class="comment">#print("image_pred[:, :5]:\n",image_pred[:, :5]) </span></span><br><span class="line">        keep_boxes = []</span><br><span class="line">        <span class="keyword">while</span> detections.size(<span class="number">0</span>):</span><br><span class="line">            <span class="comment">#torch.unsqueeze()这个函数主要是对数据维度进行扩充</span></span><br><span class="line">            large_overlap = bbox_iou(detections[<span class="number">0</span>, :<span class="number">4</span>].unsqueeze(<span class="number">0</span>), detections[:, :<span class="number">4</span>]) &gt; nms_thres</span><br><span class="line">            label_match = detections[<span class="number">0</span>, <span class="number">-1</span>] == detections[:, <span class="number">-1</span>]</span><br><span class="line">            <span class="comment"># Indices of boxes with lower confidence scores, large IOUs and matching labels</span></span><br><span class="line">            invalid = large_overlap &amp; label_match</span><br><span class="line">            weights = detections[invalid, <span class="number">4</span>:<span class="number">5</span>]<span class="comment">#置信度</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("1.detections:\n",detections)</span></span><br><span class="line"><span class="string">            print("large_overlap:\n",large_overlap)</span></span><br><span class="line"><span class="string">            print("detections[0, -1]:\n",detections[0, -1])</span></span><br><span class="line"><span class="string">            print("detections[:, -1]:\n",detections[:, -1])</span></span><br><span class="line"><span class="string">            print("label_match:\n",label_match)</span></span><br><span class="line"><span class="string">            print("invalid:\n",invalid)</span></span><br><span class="line"><span class="string">            print("weights:\n",weights)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment"># Merge overlapping bboxes by order of confidence</span></span><br><span class="line">            detections[<span class="number">0</span>, :<span class="number">4</span>] = (weights * detections[invalid, :<span class="number">4</span>]).sum(<span class="number">0</span>) / weights.sum()</span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("detections[invalid, :4]:\n",detections[invalid, :4])</span></span><br><span class="line"><span class="string">            print("weights * detections[invalid, :4]:\n",weights * detections[invalid, :4])</span></span><br><span class="line"><span class="string">            print("detections[invalid, :4].sum(0):\n",detections[invalid, :4].sum(0))</span></span><br><span class="line"><span class="string">            print("weights * detections[invalid, :4].sum(0):\n",weights * detections[invalid, :4].sum(0))</span></span><br><span class="line"><span class="string">            print("2.detections:\n",detections)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            keep_boxes += [detections[<span class="number">0</span>]]</span><br><span class="line">            detections = detections[~invalid]</span><br><span class="line">            <span class="comment">#print("3.detections:\n",detections)</span></span><br><span class="line">        <span class="keyword">if</span> keep_boxes:</span><br><span class="line">            output[image_i] = torch.stack(keep_boxes)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>非极大值抑制算法可参考：<br><a href="https://www.cnblogs.com/makefile/p/nms.html" target="_blank" rel="noopener">https://www.cnblogs.com/makefile/p/nms.html</a><br><a href="https://www.jianshu.com/p/d452b5615850" target="_blank" rel="noopener">https://www.jianshu.com/p/d452b5615850</a><br>在经过非极大值抑制处理之后，在这里唯一有一点不同的是，这里采取了边界框“融合”的策略：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge overlapping bboxes by order of confidence            </span></span><br><span class="line">detections[<span class="number">0</span>, :<span class="number">4</span>] = (weights * detections[invalid, :<span class="number">4</span>]).sum(<span class="number">0</span>) / weights.sum()</span><br></pre></td></tr></table></figure>
<p>最终可以得到我们的检验结果。</p>
<h3 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h3><h4 id="训练前准备工作"><a href="#训练前准备工作" class="headerlink" title="训练前准备工作"></a>训练前准备工作</h4><h5 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.logger <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.datasets <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.parse_config <span class="keyword">import</span> *</span><br><span class="line"><span class="comment">#from test import evaluate</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> terminaltables <span class="keyword">import</span> AsciiTable</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">"--epochs"</span>, type=int, default=<span class="number">100</span>, help=<span class="string">"number of epochs"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--batch_size"</span>, type=int, default=<span class="number">8</span>, help=<span class="string">"size of each image batch"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--gradient_accumulations"</span>, type=int, default=<span class="number">2</span>, help=<span class="string">"number of gradient accums before step"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--model_def"</span>, type=str, default=<span class="string">"config/yolov3_myself.cfg"</span>, help=<span class="string">"path to model definition file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--data_config"</span>, type=str, default=<span class="string">"config/voc_myself.data"</span>, help=<span class="string">"path to data config file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--pretrained_weights"</span>, type=str, default=<span class="string">"weights/darknet53.conv.74"</span>, help=<span class="string">"if specified starts from checkpoint model"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--n_cpu"</span>, type=int, default=<span class="number">0</span>, help=<span class="string">"number of cpu threads to use during batch generation"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--img_size"</span>, type=int, default=<span class="number">416</span>, help=<span class="string">"size of each image dimension"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--checkpoint_interval"</span>, type=int, default=<span class="number">1</span>, help=<span class="string">"interval between saving model weights"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--evaluation_interval"</span>, type=int, default=<span class="number">1</span>, help=<span class="string">"interval evaluations on validation set"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--compute_map"</span>, default=<span class="literal">False</span>, help=<span class="string">"if True computes mAP every tenth batch"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--multiscale_training"</span>, default=<span class="literal">True</span>, help=<span class="string">"allow for multi-scale training"</span>)</span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line">    print(opt)</span><br><span class="line"> </span><br><span class="line">    logger = Logger(<span class="string">"logs"</span>)</span><br><span class="line"> </span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"> </span><br><span class="line">    os.makedirs(<span class="string">"output"</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    os.makedirs(<span class="string">"checkpoints"</span>, exist_ok=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h5 id="加载网络"><a href="#加载网络" class="headerlink" title="加载网络"></a>加载网络</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get data configuration</span></span><br><span class="line"><span class="comment">#从.cfg文件中解析出路径，包括训练路径、验证路径、训练类别。同时加载Darknet（YOLOv3）模型到model中</span></span><br><span class="line">    data_config = parse_data_config(opt.data_config)</span><br><span class="line">    train_path = data_config[<span class="string">"train"</span>]</span><br><span class="line">    valid_path = data_config[<span class="string">"valid"</span>]</span><br><span class="line">    class_names = load_classes(data_config[<span class="string">"names"</span>])</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Initiate model</span></span><br><span class="line">    <span class="comment">#model.apply(weights_init_normal)**，自定义初始化方式。</span></span><br><span class="line">    model = Darknet(opt.model_def).to(device)</span><br><span class="line">    model.apply(weights_init_normal)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># If specified we start from checkpoint</span></span><br><span class="line">    <span class="keyword">if</span> opt.pretrained_weights:</span><br><span class="line">        <span class="keyword">if</span> opt.pretrained_weights.endswith(<span class="string">".pth"</span>):</span><br><span class="line">            <span class="comment">#通常训练的时候，会加载预训练模型model.load_state_dict(torch.load(opt.pretrained_weights))。</span></span><br><span class="line">            model.load_state_dict(torch.load(opt.pretrained_weights))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model.load_darknet_weights(opt.pretrained_weights)</span><br></pre></td></tr></table></figure>
<p>从.cfg文件中解析出路径，包括训练路径、验证路径、训练类别。同时加载Darknet（YOLOv3）模型到model中。<code>model.apply(weights_init_normal)</code>，自定义初始化方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init_normal</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">"Conv"</span>) != <span class="number">-1</span>:</span><br><span class="line">        torch.nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">"BatchNorm2d"</span>) != <span class="number">-1</span>:</span><br><span class="line">        torch.nn.init.normal_(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<h5 id="放进DataLoader"><a href="#放进DataLoader" class="headerlink" title="放进DataLoader"></a>放进DataLoader</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DataLoader的collate_fn参数，实现自定义的batch输出</span></span><br><span class="line">    <span class="comment">#- shuffle：设置为True的时候，每个世代都会打乱数据集 </span></span><br><span class="line">    <span class="comment">#- collate_fn：如何取样本的，我们可以定义自己的函数来准确地实现想要的功能 </span></span><br><span class="line">    <span class="comment">#- drop_last：告诉如何处理数据集长度除于batch_size余下的数据。True就抛弃，否则保留</span></span><br><span class="line">    dataloader = torch.utils.data.DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=opt.batch_size,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        num_workers=opt.n_cpu,</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        collate_fn=dataset.collate_fn,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#使用优化器</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"> </span><br><span class="line">    metrics = [</span><br><span class="line">        <span class="string">"grid_size"</span>,</span><br><span class="line">        <span class="string">"loss"</span>,</span><br><span class="line">        <span class="string">"x"</span>,</span><br><span class="line">        <span class="string">"y"</span>,</span><br><span class="line">        <span class="string">"w"</span>,</span><br><span class="line">        <span class="string">"h"</span>,</span><br><span class="line">        <span class="string">"conf"</span>,</span><br><span class="line">        <span class="string">"cls"</span>,</span><br><span class="line">        <span class="string">"cls_acc"</span>,</span><br><span class="line">        <span class="string">"recall50"</span>,</span><br><span class="line">        <span class="string">"recall75"</span>,</span><br><span class="line">        <span class="string">"precision"</span>,</span><br><span class="line">        <span class="string">"conf_obj"</span>,</span><br><span class="line">        <span class="string">"conf_noobj"</span>,</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>
<h4 id="训练并计算loss"><a href="#训练并计算loss" class="headerlink" title="训练并计算loss"></a>训练并计算loss</h4><h5 id="开始迭代"><a href="#开始迭代" class="headerlink" title="开始迭代"></a>开始迭代</h5><p>加载所有的图片，迭代的完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        print(<span class="string">"len(dataloader):\n"</span>,len(dataloader))</span><br><span class="line">        <span class="keyword">for</span> batch_i, (_, imgs, targets) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">            batches_done = len(dataloader) * epoch + batch_i</span><br><span class="line">            print(<span class="string">"batch_i:\n"</span>,batch_i)</span><br><span class="line">            print(<span class="string">"imgs.shape:\n"</span>,imgs.shape)</span><br><span class="line">            print(<span class="string">"batches_done:\n"</span>,batches_done)</span><br><span class="line">            imgs = Variable(imgs.to(device))</span><br><span class="line">            targets = Variable(targets.to(device), requires_grad=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">            loss, outputs = model(imgs, targets)</span><br><span class="line">            loss.backward()</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> batches_done % opt.gradient_accumulations:</span><br><span class="line">                <span class="comment"># Accumulates gradient before each step</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad()</span><br></pre></td></tr></table></figure>

<h5 id="从batch中获取图片，从label中获取标签"><a href="#从batch中获取图片，从label中获取标签" class="headerlink" title="从batch中获取图片，从label中获取标签"></a>从batch中获取图片，从label中获取标签</h5><p><code>for batch_i, (_, imgs, targets) in enumerate(dataloader):</code>，这里主要要参考ListDataset中的<strong>getitem</strong>和DataLoader中的<strong>collate_fn</strong>设置。<br>ListDataset中的<strong>getitem</strong>（部分）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.exists(label_path):</span><br><span class="line">            </span><br><span class="line">            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(<span class="number">-1</span>, <span class="number">5</span>))</span><br><span class="line">            <span class="comment"># Extract coordinates for unpadded + unscaled image</span></span><br><span class="line">            x1 = w_factor * (boxes[:, <span class="number">1</span>] - boxes[:, <span class="number">3</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#xmin</span></span><br><span class="line">            y1 = h_factor * (boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">4</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#ymin</span></span><br><span class="line">            x2 = w_factor * (boxes[:, <span class="number">1</span>] + boxes[:, <span class="number">3</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#xmax</span></span><br><span class="line">            y2 = h_factor * (boxes[:, <span class="number">2</span>] + boxes[:, <span class="number">4</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#ymax</span></span><br><span class="line">            <span class="comment"># Adjust for added padding</span></span><br><span class="line">            <span class="comment"># 标注的边界框根据pad进行偏移</span></span><br><span class="line">            x1 += pad[<span class="number">0</span>]<span class="comment">#左</span></span><br><span class="line">            y1 += pad[<span class="number">2</span>]<span class="comment">#上</span></span><br><span class="line">            x2 += pad[<span class="number">1</span>]<span class="comment">#右</span></span><br><span class="line">            y2 += pad[<span class="number">3</span>]<span class="comment">#下</span></span><br><span class="line">            <span class="comment"># Returns (x, y, w, h) 坐标进行微调(放缩)</span></span><br><span class="line">            boxes[:, <span class="number">1</span>] = ((x1 + x2) / <span class="number">2</span>) / padded_w</span><br><span class="line">            boxes[:, <span class="number">2</span>] = ((y1 + y2) / <span class="number">2</span>) / padded_h</span><br><span class="line">            boxes[:, <span class="number">3</span>] *= w_factor / padded_w</span><br><span class="line">            boxes[:, <span class="number">4</span>] *= h_factor / padded_h</span><br><span class="line"> </span><br><span class="line">            targets = torch.zeros((len(boxes), <span class="number">6</span>))</span><br><span class="line">            targets[:, <span class="number">1</span>:] = boxes</span><br><span class="line">            print(<span class="string">"len(boxes)："</span>,len(boxes))</span><br><span class="line">            print(<span class="string">"boxes:\n"</span>,boxes)</span><br><span class="line">            print(<span class="string">"targets:\n"</span>,targets)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200922210457663.png#pic_center" alt="在这里插入图片描述"></p>
<p>这里是标注的.txt文件中解析坐标，生成VOC数据集标注txt的脚本是voc_label.py。完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, getcwd</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join</span><br><span class="line"> </span><br><span class="line">sets=[(<span class="string">''</span>, <span class="string">'train'</span>), (<span class="string">''</span>, <span class="string">'val'</span>), (<span class="string">''</span>, <span class="string">'test'</span>)]</span><br><span class="line"> </span><br><span class="line">classes = [<span class="string">"nodule"</span>]</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(size, box)</span>:</span></span><br><span class="line">    dw = <span class="number">1.</span>/(size[<span class="number">0</span>])</span><br><span class="line">    dh = <span class="number">1.</span>/(size[<span class="number">1</span>])</span><br><span class="line">    x = (box[<span class="number">0</span>] + box[<span class="number">1</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    y = (box[<span class="number">2</span>] + box[<span class="number">3</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    w = box[<span class="number">1</span>] - box[<span class="number">0</span>]</span><br><span class="line">    h = box[<span class="number">3</span>] - box[<span class="number">2</span>]</span><br><span class="line">    x = x*dw</span><br><span class="line">    w = w*dw</span><br><span class="line">    y = y*dh</span><br><span class="line">    h = h*dh</span><br><span class="line">    <span class="keyword">return</span> (x,y,w,h)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_annotation</span><span class="params">(year, image_id)</span>:</span></span><br><span class="line">    in_file = open(<span class="string">'VOCdevkit/VOC%s/Annotations/%s.xml'</span>%(year, image_id))</span><br><span class="line">    out_file = open(<span class="string">'VOCdevkit/VOC%s/labels/%s.txt'</span>%(year, image_id), <span class="string">'w'</span>)</span><br><span class="line">    tree=ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    size = root.find(<span class="string">'size'</span>)</span><br><span class="line">    w = int(size.find(<span class="string">'width'</span>).text)</span><br><span class="line">    h = int(size.find(<span class="string">'height'</span>).text)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> root.iter(<span class="string">'object'</span>):</span><br><span class="line">        <span class="comment">#difficult = obj.find('difficult').text</span></span><br><span class="line">        difficult = <span class="number">0</span></span><br><span class="line">        cls = obj.find(<span class="string">'name'</span>).text</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> classes <span class="keyword">or</span> int(difficult)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        cls_id = classes.index(cls)</span><br><span class="line">        xmlbox = obj.find(<span class="string">'bndbox'</span>)</span><br><span class="line">        b = (float(xmlbox.find(<span class="string">'xmin'</span>).text), float(xmlbox.find(<span class="string">'xmax'</span>).text), float(xmlbox.find(<span class="string">'ymin'</span>).text), float(xmlbox.find(<span class="string">'ymax'</span>).text))</span><br><span class="line">        bb = convert((w,h), b)</span><br><span class="line">        out_file.write(str(cls_id) + <span class="string">" "</span> + <span class="string">" "</span>.join([str(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">'\n'</span>)</span><br><span class="line"> </span><br><span class="line">wd = getcwd()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> year, image_set <span class="keyword">in</span> sets:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'VOCdevkit/VOC%s/labels/'</span>%(year)):</span><br><span class="line">        os.makedirs(<span class="string">'VOCdevkit/VOC%s/labels/'</span>%(year))</span><br><span class="line">    image_ids = open(<span class="string">'VOCdevkit/VOC%s/ImageSets/Main/%s.txt'</span>%(year, image_set)).read().strip().split()</span><br><span class="line">    list_file = open(<span class="string">'%s_%s.txt'</span>%(year, image_set), <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">for</span> image_id <span class="keyword">in</span> image_ids:</span><br><span class="line">        list_file.write(<span class="string">'%s/VOCdevkit/VOC%s/JPEGImages/%s.png\n'</span>%(wd, year, image_id))</span><br><span class="line">        convert_annotation(year, image_id)</span><br><span class="line">    list_file.close()</span><br><span class="line"> </span><br><span class="line">os.system(<span class="string">"cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt &gt; train.txt"</span>)</span><br><span class="line">os.system(<span class="string">"cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt &gt; train.all.txt"</span>)</span><br></pre></td></tr></table></figure>
<p>注意其中的==convert== 函数，以及语句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = (float(xmlbox.find(<span class="string">'xmin'</span>).text), float(xmlbox.find(<span class="string">'xmax'</span>).text), float(xmlbox.find(<span class="string">'ymin'</span>).text), float(xmlbox.find(<span class="string">'ymax'</span>).text))</span><br><span class="line">        bb = convert((w,h), b)</span><br><span class="line">        out_file.write(str(cls_id) + <span class="string">" "</span> + <span class="string">" "</span>.join([str(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<p>这个脚本把<strong>xmax</strong>，<strong>xmin</strong>，<strong>ymax</strong>，<strong>ymin</strong>，转换成编辑框坐标中心，并同<strong>width</strong>和<strong>height</strong>进行归一化到0~1之间。那么需要在训练的过程中解析这些边界框坐标及大小，放进名为tatgets的张量中进行训练，这个坐标如何转换计算的，可以参考下图。<br><img src="https://img-blog.csdnimg.cn/20200922213820150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>（注：<strong>getitem</strong>函数中的w_factor和h_factor是获取的图像的宽高。注意，最后放进<strong>targets</strong>的值是，<strong>groud truth</strong>的中心点坐标，以及w和h（均是在padw和padh放缩之后的值）。这里targets在下面的坐标预测的时候有用。<br>==collate_fn==函数主要是调整imgs的尺寸大小，因为YOLOv3在训练的过程中采用多尺度训练，不断的改变图像的分辨率大小，使得YOLOv3可以很好的适用于各种分辨率大小的图像检测。<strong>collate_fn</strong>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        paths, imgs, targets = list(zip(*batch))</span><br><span class="line">        <span class="comment"># Remove empty placeholder targets</span></span><br><span class="line">        targets = [boxes <span class="keyword">for</span> boxes <span class="keyword">in</span> targets <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>]</span><br><span class="line">        <span class="comment"># Add sample index to targets</span></span><br><span class="line">        <span class="keyword">for</span> i, boxes <span class="keyword">in</span> enumerate(targets):</span><br><span class="line">            boxes[:, <span class="number">0</span>] = i</span><br><span class="line">        targets = torch.cat(targets, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Selects new image size every tenth batch</span></span><br><span class="line">        <span class="keyword">if</span> self.multiscale <span class="keyword">and</span> self.batch_count % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 图像进行放缩 调整分辨率大小</span></span><br><span class="line">            self.img_size = random.choice(range(self.min_size, self.max_size + <span class="number">1</span>, <span class="number">32</span>))</span><br><span class="line">        <span class="comment"># Resize images to input shape</span></span><br><span class="line">        imgs = torch.stack([resize(img, self.img_size) <span class="keyword">for</span> img <span class="keyword">in</span> imgs])</span><br><span class="line">        self.batch_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> paths, imgs, targets</span><br></pre></td></tr></table></figure>
<p>需要注意的是<strong>targets</strong>的变化方式，在ListDataset类的<strong>getitem</strong>函数中，<strong>targets</strong>的第一位是0，那这个第一位是有什么用呢？<strong>targets</strong>最后输出的是一个<strong>列表</strong>，列表的每一个元素都是一张image对应的n个<strong>target</strong>（这个是张量），target[:,0]表示的是对应image的ID。在训练的时候<strong>collate_fn</strong>函数都会把所有<strong>target</strong>融合在一起成为一个张量（<code>targets = torch.cat(targets, 0)</code>），只有这个张量的第一位（<strong>target[:,0]</strong>）才可以判断这个target属于哪一张图片（即能够匹配图像ID）。<br><img src="https://img-blog.csdnimg.cn/20200922214525167.png#pic_center" alt="在这里插入图片描述"><br><strong>collate_fn</strong>函数的使用也是为什么你图像尺寸是512x512的，但是进行训练的时候却是384x384（以像素点32的进行放缩加减）。<br><img src="https://img-blog.csdnimg.cn/20200922214635950.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200922214644929.png#pic_center" alt="在这里插入图片描述"></p>
<h5 id="计算Loss"><a href="#计算Loss" class="headerlink" title="计算Loss"></a>计算Loss</h5><p><code>loss, outputs = model(imgs, targets)，</code>这里进行计算loss。其实这个loss的计算是在yolo层计算的，其实不难理解，yolo层是负责目标检测的层，需要输出目标的类别、坐标、大小，所以会在这一层进行loss计算。</p>
<p>yolo层的具体实现是在==YOLOLayer==中，可查看其forward函数得知loss计算过程，代码（YOLOLayer部分）如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output, <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(</span><br><span class="line">                pred_boxes=pred_boxes,</span><br><span class="line">                pred_cls=pred_cls,</span><br><span class="line">                target=targets,</span><br><span class="line">                anchors=self.scaled_anchors,</span><br><span class="line">                ignore_thres=self.ignore_thres,</span><br><span class="line">            )</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Loss : Mask outputs to ignore non-existing objects (except with conf. loss)</span></span><br><span class="line">            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])</span><br><span class="line">            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])</span><br><span class="line">            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])</span><br><span class="line">            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])</span><br><span class="line">            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])</span><br><span class="line">            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj</span><br><span class="line">            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])</span><br><span class="line">            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Metrics</span></span><br><span class="line">            cls_acc = <span class="number">100</span> * class_mask[obj_mask].mean()</span><br><span class="line">            conf_obj = pred_conf[obj_mask].mean()</span><br><span class="line">            conf_noobj = pred_conf[noobj_mask].mean()</span><br><span class="line">            conf50 = (pred_conf &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou50 = (iou_scores &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou75 = (iou_scores &gt; <span class="number">0.75</span>).float()</span><br><span class="line">            detected_mask = conf50 * class_mask * tconf</span><br><span class="line">            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">            self.metrics = &#123;</span><br><span class="line">                <span class="string">"loss"</span>: to_cpu(total_loss).item(),</span><br><span class="line">                <span class="string">"x"</span>: to_cpu(loss_x).item(),</span><br><span class="line">                <span class="string">"y"</span>: to_cpu(loss_y).item(),</span><br><span class="line">                <span class="string">"w"</span>: to_cpu(loss_w).item(),</span><br><span class="line">                <span class="string">"h"</span>: to_cpu(loss_h).item(),</span><br><span class="line">                <span class="string">"conf"</span>: to_cpu(loss_conf).item(),</span><br><span class="line">                <span class="string">"cls"</span>: to_cpu(loss_cls).item(),</span><br><span class="line">                <span class="string">"cls_acc"</span>: to_cpu(cls_acc).item(),</span><br><span class="line">                <span class="string">"recall50"</span>: to_cpu(recall50).item(),</span><br><span class="line">                <span class="string">"recall75"</span>: to_cpu(recall75).item(),</span><br><span class="line">                <span class="string">"precision"</span>: to_cpu(precision).item(),</span><br><span class="line">                <span class="string">"conf_obj"</span>: to_cpu(conf_obj).item(),</span><br><span class="line">                <span class="string">"conf_noobj"</span>: to_cpu(conf_noobj).item(),</span><br><span class="line">                <span class="string">"grid_size"</span>: grid_size,</span><br><span class="line">            &#125;</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">return</span> output, total_loss</span><br></pre></td></tr></table></figure>
<p>可以看到，batch设置的是8，看到图片的尺寸被放缩成了【352， 352】，分别进行8、16、32倍下采样，即对应的shape是【44，44】【22， 22】【11， 11】<br><img src="https://img-blog.csdnimg.cn/20200923203340676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>同时使用==build_targets==函数得到<strong>iou_scores</strong>, <strong>class_mask</strong>, <strong>obj_mask</strong>, <strong>noobj_mask</strong>, <strong>tx</strong>, <strong>ty</strong>, <strong>tw</strong>, <strong>th</strong>, <strong>tcls</strong>, <strong>tconf</strong>。<br><strong>obj_mask</strong>表示有物体落在特征图中某一个cell的索引，所以在初始化的时候置<strong>0</strong>，如果有物体落在那个cell中，那个对应的位置会置<strong>1</strong>。所以会有代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">obj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">........</span><br><span class="line">obj_mask[b, best_n, gj, gi] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>同理，表示没有物体落在特征图中某一个cell的索引,所以在初始化的时候置<strong>1</strong>，如果没有有物体落在那个cell中，那个对应的位置会置<strong>0</strong>。同时，如果预测的IOU值过大，（大于阈值ignore_thres）时，那么可以认为这个cell是有物体的，要置0。所以会有代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">1</span>)</span><br><span class="line">.......   </span><br><span class="line">noobj_mask[b, best_n, gj, gi] = <span class="number">0</span>    </span><br><span class="line"><span class="comment"># Set noobj mask to zero where iou exceeds ignore threshold    </span></span><br><span class="line"><span class="keyword">for</span> i, anchor_ious <span class="keyword">in</span> enumerate(ious.t()):        </span><br><span class="line">    noobj_mask[b[i], anchor_ious &gt; ignore_thres, gj[i], gi[i]] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>查看==build_targets==代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_targets</span><span class="params">(pred_boxes, pred_cls, target, anchors, ignore_thres)</span>:</span></span><br><span class="line"> </span><br><span class="line">    ByteTensor = torch.cuda.ByteTensor <span class="keyword">if</span> pred_boxes.is_cuda <span class="keyword">else</span> torch.ByteTensor</span><br><span class="line">    FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> pred_boxes.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line"> </span><br><span class="line">    nB = pred_boxes.size(<span class="number">0</span>)</span><br><span class="line">    nA = pred_boxes.size(<span class="number">1</span>)</span><br><span class="line">    nC = pred_cls.size(<span class="number">-1</span>)</span><br><span class="line">    nG = pred_boxes.size(<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Output tensors</span></span><br><span class="line">    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">1</span>)</span><br><span class="line">    class_mask = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    tx = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    ty = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    tw = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    th = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Convert to position relative to box</span></span><br><span class="line">    target_boxes = target[:, <span class="number">2</span>:<span class="number">6</span>] * nG</span><br><span class="line">    gxy = target_boxes[:, :<span class="number">2</span>]</span><br><span class="line">    gwh = target_boxes[:, <span class="number">2</span>:]</span><br><span class="line">    <span class="comment"># Get anchors with best iou</span></span><br><span class="line">    ious = torch.stack([bbox_wh_iou(anchor, gwh) <span class="keyword">for</span> anchor <span class="keyword">in</span> anchors])</span><br><span class="line">    best_ious, best_n = ious.max(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Separate target values</span></span><br><span class="line">    b, target_labels = target[:, :<span class="number">2</span>].long().t()</span><br><span class="line">    gx, gy = gxy.t()</span><br><span class="line">    gw, gh = gwh.t()</span><br><span class="line">    gi, gj = gxy.long().t()</span><br><span class="line">    <span class="comment"># Set masks</span></span><br><span class="line">    obj_mask[b, best_n, gj, gi] = <span class="number">1</span></span><br><span class="line">    noobj_mask[b, best_n, gj, gi] = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Set noobj mask to zero where iou exceeds ignore threshold</span></span><br><span class="line">    <span class="keyword">for</span> i, anchor_ious <span class="keyword">in</span> enumerate(ious.t()):</span><br><span class="line">        noobj_mask[b[i], anchor_ious &gt; ignore_thres, gj[i], gi[i]] = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Coordinates</span></span><br><span class="line">    tx[b, best_n, gj, gi] = gx - gx.floor()</span><br><span class="line">    ty[b, best_n, gj, gi] = gy - gy.floor()</span><br><span class="line">    <span class="comment"># Width and height</span></span><br><span class="line">    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, <span class="number">0</span>] + <span class="number">1e-16</span>)</span><br><span class="line">    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, <span class="number">1</span>] + <span class="number">1e-16</span>)</span><br><span class="line">    <span class="comment"># One-hot encoding of label</span></span><br><span class="line">    tcls[b, best_n, gj, gi, target_labels] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># Compute label correctness and iou at best anchor</span></span><br><span class="line">    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(<span class="number">-1</span>) == target_labels).float()</span><br><span class="line">    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">    tconf = obj_mask.float()</span><br><span class="line">    <span class="keyword">return</span> iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf</span><br></pre></td></tr></table></figure>
<p>根据下图，不难理解：<br><strong>nB</strong>：Batch是多大。<br><strong>nA</strong>：多少个Anchor 。<br><strong>nC</strong>：训练多少个class，在这里我之训练一个类，所以是1。<br><strong>nG</strong>：grid大小，每一行分（列）成多少个cell。<img src="https://img-blog.csdnimg.cn/2020092321093911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">)<img src="https://img-blog.csdnimg.cn/20200923211239660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>同时提取<strong>targets</strong>中的坐标信息，分别给<strong>gxy</strong>和<strong>gwh</strong>张量，乘以<strong>nG</strong>是因为坐标信息是归一化到0~1之间，需要进行放大。<br><img src="https://img-blog.csdnimg.cn/20200923211703697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>下一步便是用anchor进行计算iou值 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get anchors with best iou</span></span><br><span class="line">    ious = torch.stack([bbox_wh_iou(anchor, gwh) <span class="keyword">for</span> anchor <span class="keyword">in</span> anchors])</span><br><span class="line">    best_ious, best_n = ious.max(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>实现的函数为 ==bbox_wh_iou==，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_wh_iou</span><span class="params">(wh1, wh2)</span>:</span></span><br><span class="line">    wh2 = wh2.t()</span><br><span class="line">    w1, h1 = wh1[<span class="number">0</span>], wh1[<span class="number">1</span>]</span><br><span class="line">    w2, h2 = wh2[<span class="number">0</span>], wh2[<span class="number">1</span>]</span><br><span class="line">    inter_area = torch.min(w1, w2) * torch.min(h1, h2)</span><br><span class="line">    union_area = (w1 * h1 + <span class="number">1e-16</span>) + w2 * h2 - inter_area</span><br><span class="line">    <span class="keyword">return</span> inter_area / union_area</span><br></pre></td></tr></table></figure>
<p>计算结果如下。仍然把<strong>batch</strong>设为8。<strong>ious.shape</strong>为【3， 8】这是因为有三个<strong>anchor</strong>，每一个anchor都会和标记的label进行计算<strong>iou</strong>值，即看哪一个<strong>anchor</strong>和<strong>ground truth</strong>（真实的、标注的边界框）最接近。<strong>注意：【3，8】的8不是batch是8，而是有8个target，恰好每一张图都有一个target，所以是8，但往往一张图可能存在多个taget</strong>。<br><img src="https://img-blog.csdnimg.cn/20200923215913232.png#pic_center" alt="在这里插入图片描述"><br>gxy.t()是为了把shape从n x 2 变成 2 x n。 <code>gi, gj = gxy.long().t()</code>，是通过.long的方式去除小数点，保留整数。如此便可以设置masks。<strong>b</strong>是指第几个<strong>target</strong>。<strong>gi</strong>, <strong>gj</strong> 便是特征图中对应的左上角的坐标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set masks</span></span><br><span class="line">   obj_mask[b, best_n, gj, gi] = <span class="number">1</span></span><br><span class="line">   noobj_mask[b, best_n, gj, gi] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h6 id="坐标预测"><a href="#坐标预测" class="headerlink" title="坐标预测"></a>坐标预测</h6><p>接下来是坐标预测，我们先来看YOLOv3坐标预测图。<br><img src="https://img-blog.csdnimg.cn/20200923223018564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其中，Cx,Cy是feature map中grid cell的左上角坐标，在yolov3中每个grid cell在feature map中的宽和高均为1。如下图1的情形时，这个bbox边界框的中心属于第二行第二列的grid cell，它的左上角坐标为(1,1)，故Cx=1,Cy=1.公式中的Pw、Ph是预设的anchor box映射到feature map中的宽和高(<strong>anchor box原本设定是相对于416*416坐标系下的坐标，在yolov3.cfg文件中写明了，代码中是把cfg中读取的坐标除以stride如32映射到feature map坐标系中</strong>)。</p>
<p>最终得到的边框坐标值是bx,by,bw,bh，即边界框bbox相对于feature map的位置和大小，是我们需要的预测输出坐标。<strong>但我们网络实际上的学习目标是tx,ty,tw,th这４个offsets</strong>，其中tx,ty是预测的坐标偏移值，tw,th是尺度缩放，有了这４个offsets，自然可以根据之前的公式去求得真正需要的bx,by,bw,bh４个坐标。</p>
<p><strong>那么我们的网络为何不直接学习bx,by,bw,bh呢</strong>？因为YOLO 的输出是一个卷积特征图，包含沿特征图深度的边界框属性。边界框属性由彼此堆叠的单元格预测得出。因此，如果你需要在 (5,6) 处访问该单元格的第二个边框bbox，那么你需要通过 map[5,6, (5+C): 2<em>(5+C)] 将其编入索引。这种格式对于输出处理过程（例如通过目标置信度进行阈值处理、添加对中心的网格偏移、应用锚点等）很不方便，因此我们求偏移量即可。那么这样就只需要求偏移量，也就可以用上面的公式求出bx,by,bw,bh，反正是等价的。另外，*</em>通过学习偏移量，就可以通过网络原始给定的anchor box坐标经过线性回归微调（平移加尺度缩放）去逐渐靠近groundtruth**。为何微调可看做线性回归往下看。</p>
<p>这里需要注意的是，虽然输入尺寸是416 * 416,但原图是按照纵横比例缩放至416 * 416的， <strong>取 min(w/img_w, h/img_h)这个比例来缩放，保证长的边缩放为需要的输入尺寸416，而短边按比例缩放不会扭曲</strong>，img_w,img_h是原图尺寸768,576, 缩放后的尺寸为new_w, new_h=416,312，需要的输入尺寸是w,h=416 * 416.如下图所示：<br><img src="https://img-blog.csdnimg.cn/2020092322310363.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200923231534284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>剩下的灰色区域用(128,128,128)填充即可构造为416 * 416。不管训练还是测试时都需要这样操作原图。pytorch代码中比较好理解这一点。下面这个函数实现了对原图的变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterbox_image</span><span class="params">(img, inp_dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    lteerbox_image()将图片按照纵横比进行缩放，将空白部分用(128,128,128)填充,调整图像尺寸</span></span><br><span class="line"><span class="string">    具体而言,此时某个边正好可以等于目标长度,另一边小于等于目标长度</span></span><br><span class="line"><span class="string">    将缩放后的数据拷贝到画布中心,返回完成缩放</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    img_w, img_h = img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]</span><br><span class="line">    w, h = inp_dim<span class="comment">#inp_dim是需要resize的尺寸（如416*416）</span></span><br><span class="line">    <span class="comment"># 取min(w/img_w, h/img_h)这个比例来缩放，缩放后的尺寸为new_w, new_h,即保证较长的边缩放后正好等于目标长度(需要的尺寸)，另一边的尺寸缩放后还没有填充满.</span></span><br><span class="line">    new_w = int(img_w * min(w/img_w, h/img_h))</span><br><span class="line">    new_h = int(img_h * min(w/img_w, h/img_h))</span><br><span class="line">    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC) <span class="comment">#将图片按照纵横比不变来缩放为new_w x new_h，768 x 576的图片缩放成416x312.,用了双三次插值</span></span><br><span class="line">    <span class="comment"># 创建一个画布, 将resized_image数据拷贝到画布中心。</span></span><br><span class="line">    canvas = np.full((inp_dim[<span class="number">1</span>], inp_dim[<span class="number">0</span>], <span class="number">3</span>), <span class="number">128</span>)<span class="comment">#生成一个我们最终需要的图片尺寸hxwx3的array,这里生成416x416x3的array,每个元素值为128</span></span><br><span class="line">    <span class="comment"># 将wxhx3的array中对应new_wxnew_hx3的部分(这两个部分的中心应该对齐)赋值为刚刚由原图缩放得到的数组,得到最终缩放后图片</span></span><br><span class="line">    canvas[(h-new_h)//<span class="number">2</span>:(h-new_h)//<span class="number">2</span> + new_h,(w-new_w)//<span class="number">2</span>:(w-new_w)//<span class="number">2</span> + new_w,  :] = resized_image</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> canvas</span><br></pre></td></tr></table></figure>
<p>   而且我们注意yolov3需要的训练数据的label是根据原图尺寸归一化了的，这样做是因为怕<strong>大的边框的影响比小的边框影响大，因此做了归一化的操作，这样大的和小的边框都会被同等看待了，而且训练也容易收敛(类比于refinedbox)</strong>。既然label是根据原图的尺寸归一化了的，自己制作数据集时也需要归一化才行，如何转为yolov3需要的label网上有一大堆教程，也放一篇链接<a href="https://blog.csdn.net/qq_34199326/article/details/83819140" target="_blank" rel="noopener">https://blog.csdn.net/qq_34199326/article/details/83819140</a>。</p>
<p>  这里解释一下anchor box，YOLO3为每种FPN预测特征图（13 * 13,26 * 26,52 * 52）设定3种anchor box，总共聚类出9种尺寸的anchor box。在COCO数据集这9个anchor box是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。分配上，在最小的13 * 13特征图上由于其感受野最大故应用最大的anchor box (116x90)，(156x198)，(373x326)，（这几个坐标是针对416 * 416下的，当然要除以32把尺度缩放到13*13下），适合检测较大的目标。中等的26 * 26特征图上由于其具有中等感受野故应用中等的anchor box (30x61)，(62x45)，(59x119)，适合检测中等大小的目标。较大的52 * 52特征图上由于其具有较小的感受野故应用最小的anchor box(10x13)，(16x30)，(33x23)，适合检测较小的目标。同Faster-Rcnn一样，特征图的每个像素（即每个grid）都会有对应的三个anchor box，如13 * 13特征图的每个grid都有三个anchor box (116x90)，(156x198)，(373x326)（这几个坐标需除以32缩放尺寸）。</p>
<p><strong>那么4个坐标tx,ty,tw,th是怎么求出来的呢</strong>？对于训练样本，在大多数文章里需要用到ground truth的真实框来求这4个坐标：<br><img src="https://img-blog.csdnimg.cn/20200923233652727.png#pic_center" alt="在这里插入图片描述"><br>上面这个公式是<strong>faster-rcnn</strong>系列文章用到的公式，Px,Py在faster-rcnn系列文章是预设的anchor box在feature map上的中心点坐标。 Pw、Ph是预设的anchor box的在feature map上的宽和高。至于Gx、Gy、Gw、Gh自然就是ground truth在这个feature map的4个坐标了(其实上面已经描述了这个过程，要根据原图坐标系先根据原图纵横比不变映射为416 * 416坐标下的一个子区域如416 * 312，取 min(w/img_w, h/img_h)这个比例来缩放成416 * 312，再填充为416 * 416，坐标变换上只需要让ground truth在416 * 312下的y1,y2（即左上角和右下角纵坐标）加上图2灰色部分的一半<br>y1=y1+(416-416/768 * 576)/2=y1+(416-312)/2，<br>y2同样的操作，把x1,x2,y1,y2的坐标系的换算从针对实际红框的坐标系(416 * 312)变为416 * 416下了，这样保证bbox不会扭曲，然后除以stride得到相对于feature map的坐标)。</p>
<p><strong>用x,y坐标减去anchor box的x,y坐标得到偏移量好理解，为何要除以feature map上anchor box的宽和高呢</strong>？我认为可能是为了把绝对尺度变为相对尺度，毕竟作为偏移量，不能太大了对吧。而且不同尺度的anchor box如果都用Gx-Px来衡量显然不对，有的anchor box大有的却很小，都用Gx-Px会导致不同尺度的anchor box权重相同，而大的anchor box肯定更能容忍大点的偏移量，小的anchor box对小偏移都很敏感，故除以宽和高可以权衡不同尺度下的预测坐标偏移量。</p>
<p>但是在yolov3中与faster-rcnn系列文章用到的公式在前两行是不同的，yolov3里Px和Py就换为了feature map上的grid cell左上角坐标Cx,Cy了，即在yolov3里是Gx,Gy减去grid cell左上角坐标Cx,Cy。x,y坐标并没有针对anchon box求偏移量，所以并不需要除以Pw,Ph。</p>
<p>也就是说是tx = Gx - Cx ，ty = Gy - Cy<br>这样就可以直接求bbox中心距离grid cell左上角的坐标的偏移量。</p>
<p>tw和th的公式yolov3和faster-rcnn系列是一样的，是物体所在边框的长宽和anchor box长宽之间的比率，不管Faster-RCNN还是YOLO，都不是直接回归bounding box的长宽而是尺度缩放到对数空间，是怕训练会带来不稳定的梯度。因为如果不做变换，直接预测相对形变tw，那么要求tw&gt;0，因为你的框的宽高不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。</p>
<p>这里就有个重要的疑问了，<strong>一个尺度的feature map有三个anchors，那么对于某个ground truth框，究竟是哪个anchor负责匹配它呢</strong>？前面已经说过，和YOLOv1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的3个anchor box负责预测它，具体是哪个anchor box预测它，需要在训练中确定，即由那个与ground truth的IOU最大的anchor box预测它，而剩余的2个anchor box不与该ground truth匹配。YOLOv3需要假定每个cell至多含有一个grounth truth，而在实际上基本不会出现多于1个的情况。与ground truth匹配的anchor box计算坐标误差、置信度误差（此时target为1）以及分类误差，而其它的anchor box只计算置信度误差（此时target为0）。</p>
<p>有了平移（tx,ty）和尺度缩放（tw,th）才能让anchor box经过微调与grand truth重合。如图3，红色框为anchor box，绿色框为Ground Truth，平移+尺度缩放可实线红色框先平移到虚线红色框，然后再缩放到绿色框。边框回归最简单的想法就是通过平移加尺度缩放进行微调。</p>
<p><img src="https://img-blog.csdnimg.cn/20200923234645255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="加粗样式"><br><strong>边框回归为何只能微调</strong>？当输入的 Proposal 与 Ground Truth 相差较小时，即IOU很大时(RCNN 设置的是 IoU&gt;0.6)， 可以认为这种变换是一种线性变换， 那么我们就可以用线性回归（线性回归就是给定输入的特征向量 X, 学习一组参数 W, 使得经过线性回归后的值跟真实值 Y(Ground Truth)非常接近. 即Y≈WX ）来建模对窗口进行微调， 否则会导致训练的回归模型不work（当 Proposal跟 GT 离得较远，就是复杂的非线性问题了，此时用线性回归建模显然就不合理了）<br><img src="https://img-blog.csdnimg.cn/20200923235052608.png#pic_center" alt="在这里插入图片描述"><br>那么训练时用的groundtruth的4个坐标去做差值和比值得到tx,ty,tw,th，测试时就用预测的bbox就好了，公式修改就简单了，把Gx和Gy改为预测的x,y，Gw、Gh改为预测的w,h即可。</p>
<p>所以从前面的分析我们可以看出网络可以不断学习<strong>tx,ty,tw,th偏移量和尺度缩放</strong>，<strong>预测时</strong>使用这4个offsets求得bx,by,bw,bh即可，那么问题是：<br><img src="https://img-blog.csdnimg.cn/20200923235420229.png#pic_center" alt="在这里插入图片描述"><br><strong>这个公式tx,ty为何要sigmoid一下呢</strong>？前面讲到了在yolov3中没有让Gx - Cx后除以Pw得到tx，而是直接Gx - Cx得到tx，这样会有问题是导致tx比较大且很可能&gt;1.(因为没有除以Pw归一化尺度)。用sigmoid将tx,ty压缩到[0,1]区间內，可以有效的确保目标中心处于执行预测的网格单元中，防止偏移过多。举个例子，我们刚刚都知道了网络不会预测边界框中心的确切坐标而是预测与预测目标的grid cell左上角相关的偏移tx,ty。如13*13的feature map中，某个目标的中心点预测为(0.4,0.7)，它的cx,cy即中心落入的grid cell坐标是(6,6)，则该物体的在feature map中的中心实际坐标显然是(6.4,6.7).这种情况没毛病，但若tx,ty大于1，比如(1.2,0.7)则该物体在feature map的的中心实际坐标是(7.2,6.7)，注意这时候该物体中心在这个物体所属grid cell外面了，但(6,6)这个grid cell却检测出我们这个单元格内含有目标的中心（yolo是采取物体中心归哪个grid cell整个物体就归哪个grid celll了），这样就矛盾了，因为左上角为(6,6)的grid cell负责预测这个物体，这个物体中心必须出现在这个grid cell中而不能出现在它旁边网格中，一旦tx,ty算出来大于1就会引起矛盾，因而必须归一化。</p>
<p> <strong>看最后两行公式，tw为何要作为指数呢</strong>，这就好理解了，因为tw,th是log尺度缩放到对数空间了，当然要指数回来，而且这样可以保证大于0。 至于左边乘以Pw或者Ph是因为tw=log(Gw/Pw)当然应该乘回来得到真正的宽高。</p>
<p>记feature map大小为Ｗ，Ｈ（如13*13），可将bbox相对于整张图片的位置和大小计算出来（使4个值均处于[0,1]区间内）约束了bbox的位置预测值到[0,1]会使得模型更容易稳定训练（如果不是[0,1]区间，yolo的每个bbox的维度都是85，前5个属性是(Cx,Cy,w,h,confidence)，后80个是类别概率，如果坐标不归一化，和这些概率值一起训练肯定无法收敛）</p>
<p>只需要把之前计算的bx,bw都除以W,把by,bh都除以H。即<br><img src="https://img-blog.csdnimg.cn/20200924000346228.png#pic_center" alt="在这里插入图片描述"><br>所以回到我们的代码，gx表示x坐标的具体值，gx.floor（）则是向下取整，两者相减即可得到偏移值。<strong>所以其实总结一下在训练的时候非常巧妙，没有直接训练bw和bh，而是训练tw，th</strong>。这里注意代码是怎么写的：在==build_targets==函数中，<strong>gw</strong>和<strong>gh</strong>是标准的真实值（target）在该特征图的宽w和高h。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert to position relative to box</span></span><br><span class="line">    target_boxes = target[:, <span class="number">2</span>:<span class="number">6</span>] * nG</span><br><span class="line">    gxy = target_boxes[:, :<span class="number">2</span>]</span><br><span class="line">    gwh = target_boxes[:, <span class="number">2</span>:]</span><br></pre></td></tr></table></figure>
<p><strong>gw</strong>和<strong>gh</strong>则是通过尺度缩放成<strong>tw</strong>和<strong>th</strong>。注意下面代码中的参数：<strong>anchors</strong>[best_n][:, 0]和<strong>anchors</strong>[best_n][:, 1]，其实分别只指输入到该特征图大小的<strong>anchors</strong>的w和h。因为这个函数的输入<strong>anchors</strong>的值是<strong>self.scaled_anchors</strong> 。具体代码：</p>
<p><code>self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])</code></p>
<p>所以tw和th是该特征图大小下的标注的真实值（<strong>target</strong>）w和h与使用该特征图大小下进行检测的<strong>anchor</strong>的w和h的自然对数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Coordinates</span></span><br><span class="line">   tx[b, best_n, gj, gi] = gx - gx.floor()</span><br><span class="line">   ty[b, best_n, gj, gi] = gy - gy.floor()</span><br><span class="line">   <span class="comment"># Width and height</span></span><br><span class="line">   tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, <span class="number">0</span>] + <span class="number">1e-16</span>)</span><br><span class="line">   th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, <span class="number">1</span>] + <span class="number">1e-16</span>)</span><br></pre></td></tr></table></figure>
<p>接下来计算w和h的loss方式。计算方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask]</span><br></pre></td></tr></table></figure>
<p>tw和th我们知道怎么得到了，那么看下w和h是如何得到的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_boxes[..., <span class="number">2</span>] = torch.exp(w.data) * self.anchor_w</span><br><span class="line">        pred_boxes[..., <span class="number">3</span>] = torch.exp(h.data) * self.anchor_h</span><br></pre></td></tr></table></figure>
<p>这里的self.anchor_w和self.anchor_h就是self.scaled_anchors 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.anchor_w = self.scaled_anchors[:, <span class="number">0</span>:<span class="number">1</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.anchor_h = self.scaled_anchors[:, <span class="number">1</span>:<span class="number">2</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200924151223279.png#pic_center" alt="在这里插入图片描述"><br>其中可以把<img src="https://img-blog.csdnimg.cn/20200924151320586.png#pic_center" alt="在这里插入图片描述"><br>和<img src="https://img-blog.csdnimg.cn/20200924151331970.png#pic_center" alt="在这里插入图片描述"><br>当作真实值，<img src="https://img-blog.csdnimg.cn/20200924151341353.png#pic_center" alt="在这里插入图片描述"><br>和<img src="https://img-blog.csdnimg.cn/20200924151348479.png#pic_center" alt="在这里插入图片描述"><br>当作预测值，但是yolov3在训练的过程中从代码中我们也可以看到，不是直接做边界框回归，而是w和tw，h和th进行回归，做loss值。我们通过得到tw和th值就可以得到bw和bh。这是因为：<br><img src="https://img-blog.csdnimg.cn/20200924151623243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>th同理。</p>
<p>继续往下看==build_targets==的代码：下面这句代码，意思是第b张图片，使用第best_n个anchors来预测 哪一类（target_labels）物体。查看b和target_labels的值来方便理解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># One-hot encoding of label</span></span><br><span class="line">    tcls[b, best_n, gj, gi, target_labels] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200924152545360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>接下来计算<strong>class_mask,iou_scores,</strong>并返回。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute label correctness and iou at best anchor</span></span><br><span class="line">    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(<span class="number">-1</span>) == target_labels).float()</span><br><span class="line">    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=<span class="literal">False</span>)</span><br><span class="line">)</span><br><span class="line">    tconf = obj_mask.float()</span><br><span class="line">    <span class="keyword">return</span> iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf</span><br></pre></td></tr></table></figure>

<p><strong>class_mask</strong>的计算：b表示的targets对应image的ID，这个上面有解释，这里的b的长度是20，说明有20个target。每一个target都对应一个target_labels,即类别标签，表示这个target是什么类别，这里使用的是3类，所以target_labels的取值范围是0~2。<strong>pred_cls</strong>的shape也说明了这一点。<strong>.argmax(-1)</strong>返回最后一维度最大值的索引。注意，<strong>pred_cls</strong>[b, best_n, gj, gi].shape是【20， 3】和初期的<strong>pred_cls.shape</strong>是【8， 3， 12， 12， 3】是不一样的。<strong>pred_cls</strong>[b, best_n, gj, gi]的值如下图所示，可以抽象一点理解，[b, best_n, gj, gi]是索引号，<strong>pred_cls</strong>[b, best_n, gj, gi]便是这些索引号对应的张量堆叠而成的。如果<strong>pred_cls</strong>[b, best_n, gj, gi].argmax(-1) 等于target_labels的话，就会把这里相应位置的class_mask置1，表示这个特征地图的第<strong>gj</strong>行、第<strong>gi</strong>的cell预测的类别是正确的。</p>
<p><strong>iou</strong>值的计算：使用<strong>iou_scores</strong>函数。这里计算iou值是需要既考虑w，h还有坐标x，y。</p>
<p>原因：</p>
<ol>
<li>计算w和h的loss是anchor和target形状大小的匹配程度，得到一个和真实形状（target）最接近的anchor去进行预测（检测），然后由于IOU值很高，就可以通过平移放缩的方式进行微调，边界框回归。</li>
<li>还需要计算IOU值的得分，所以还必须要考虑预测框和真实框的坐标。</li>
</ol>
<p>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_iou</span><span class="params">(box1, box2, x1y1x2y2=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns the IoU of two bounding boxes</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> x1y1x2y2:</span><br><span class="line">        <span class="comment"># Transform from center and width to exact coordinates</span></span><br><span class="line">        b1_x1, b1_x2 = box1[:, <span class="number">0</span>] - box1[:, <span class="number">2</span>] / <span class="number">2</span>, box1[:, <span class="number">0</span>] + box1[:, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        b1_y1, b1_y2 = box1[:, <span class="number">1</span>] - box1[:, <span class="number">3</span>] / <span class="number">2</span>, box1[:, <span class="number">1</span>] + box1[:, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">        b2_x1, b2_x2 = box2[:, <span class="number">0</span>] - box2[:, <span class="number">2</span>] / <span class="number">2</span>, box2[:, <span class="number">0</span>] + box2[:, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        b2_y1, b2_y2 = box2[:, <span class="number">1</span>] - box2[:, <span class="number">3</span>] / <span class="number">2</span>, box2[:, <span class="number">1</span>] + box2[:, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the coordinates of bounding boxes</span></span><br><span class="line">        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, <span class="number">0</span>], box1[:, <span class="number">1</span>], box1[:, <span class="number">2</span>], box1[:, <span class="number">3</span>]</span><br><span class="line">        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, <span class="number">0</span>], box2[:, <span class="number">1</span>], box2[:, <span class="number">2</span>], box2[:, <span class="number">3</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># get the corrdinates of the intersection rectangle</span></span><br><span class="line">    inter_rect_x1 = torch.max(b1_x1, b2_x1)</span><br><span class="line">    inter_rect_y1 = torch.max(b1_y1, b2_y1)</span><br><span class="line">    inter_rect_x2 = torch.min(b1_x2, b2_x2)</span><br><span class="line">    inter_rect_y2 = torch.min(b1_y2, b2_y2)</span><br><span class="line">    <span class="comment"># Intersection area</span></span><br><span class="line">    <span class="comment"># torch.clamp torch.clamp(input, min, max, out=None) → Tensor</span></span><br><span class="line">    <span class="comment"># 将输入input张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量。</span></span><br><span class="line">    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + <span class="number">1</span>, min=<span class="number">0</span>) * torch.clamp(</span><br><span class="line">        inter_rect_y2 - inter_rect_y1 + <span class="number">1</span>, min=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Union Area</span></span><br><span class="line">    b1_area = (b1_x2 - b1_x1 + <span class="number">1</span>) * (b1_y2 - b1_y1 + <span class="number">1</span>)</span><br><span class="line">    b2_area = (b2_x2 - b2_x1 + <span class="number">1</span>) * (b2_y2 - b2_y1 + <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    iou = inter_area / (b1_area + b2_area - inter_area + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure>

<p>==build_targets==函数分析完了，回到==YOLOLayer==层代码中，接下来就是loss值计算，我们都知道loss需要分为三部分计算：</p>
<ol>
<li><strong>第一部分</strong>边界框损失，包含x,y,w,h。</li>
<li><strong>第二部分</strong>是置信度损失。</li>
<li><strong>第三部分</strong>是类别损失，代码如下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])</span><br><span class="line">            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])</span><br><span class="line">            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])</span><br><span class="line">            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])</span><br><span class="line">            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])</span><br><span class="line">            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj</span><br><span class="line">            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])</span><br><span class="line">            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls</span><br></pre></td></tr></table></figure>
<p>根据以上代码，我们写出<strong>YOLOv3的损失函数</strong><br><img src="https://img-blog.csdnimg.cn/20200924154348208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上式中<strong>batch</strong>是指批数据量的大小，<strong>anchor</strong>是指预测使用的框，每一层yolo中的<strong>anchor</strong>数为3，<strong>grid</strong>是特征图的尺寸。<img src="https://img-blog.csdnimg.cn/20200924154617876.png#pic_center" alt="在这里插入图片描述"><br>表示batch中的第<strong>i</strong>个数据，第<strong>j</strong>个anchor，在特征图中的第<strong>k</strong>个cell有预测的物体。<img src="https://img-blog.csdnimg.cn/20200924154701884.png#pic_center" alt="在这里插入图片描述"><br>和<img src="https://img-blog.csdnimg.cn/20200924154709505.png#pic_center" alt="在这里插入图片描述"><br>是惩罚项因子，在代码中是<strong>self.obj_scale</strong>和<strong>self.nobj_scale</strong>。</p>
<p>最后还有一小部分就是计算各种指标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Metrics</span></span><br><span class="line">           cls_acc = <span class="number">100</span> * class_mask[obj_mask].mean()</span><br><span class="line">           conf_obj = pred_conf[obj_mask].mean()</span><br><span class="line">           conf_noobj = pred_conf[noobj_mask].mean()</span><br><span class="line">           conf50 = (pred_conf &gt; <span class="number">0.5</span>).float()</span><br><span class="line">           iou50 = (iou_scores &gt; <span class="number">0.5</span>).float()</span><br><span class="line">           iou75 = (iou_scores &gt; <span class="number">0.75</span>).float()</span><br><span class="line">           detected_mask = conf50 * class_mask * tconf</span><br><span class="line">           precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + <span class="number">1e-16</span>)</span><br><span class="line">           recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line">           recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br></pre></td></tr></table></figure>
<p>再计算出loss值之和，并进行反向传播，梯度优化。</p>
<h4 id="查看训练指标并评估"><a href="#查看训练指标并评估" class="headerlink" title="查看训练指标并评估"></a>查看训练指标并评估</h4><p>这段完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        <span class="comment">#print("len(dataloader):\n",len(dataloader))</span></span><br><span class="line">        <span class="keyword">for</span> batch_i, (_, imgs, targets) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">            batches_done = len(dataloader) * epoch + batch_i</span><br><span class="line">            imgs = Variable(imgs.to(device))</span><br><span class="line">            targets = Variable(targets.to(device), requires_grad=<span class="literal">False</span>)</span><br><span class="line">            print(<span class="string">"targets.shape:\n"</span>,targets.shape)</span><br><span class="line">            loss, outputs = model(imgs, targets)                      </span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">if</span> batches_done % opt.gradient_accumulations:</span><br><span class="line">                <span class="comment"># Accumulates gradient before each step</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">            <span class="comment">#   Log progress</span></span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line"> </span><br><span class="line">            log_str = <span class="string">"\n---- [Epoch %d/%d, Batch %d/%d] ----\n"</span> % (epoch, opt.epochs, batch_i, len(dataloader))</span><br><span class="line"> </span><br><span class="line">            metric_table = [[<span class="string">"Metrics"</span>, *[<span class="string">f"YOLO Layer <span class="subst">&#123;i&#125;</span>"</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model.yolo_layers))]]]</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Log metrics at each YOLO layer</span></span><br><span class="line">            <span class="keyword">for</span> i, metric <span class="keyword">in</span> enumerate(metrics):</span><br><span class="line">                formats = &#123;m: <span class="string">"%.6f"</span> <span class="keyword">for</span> m <span class="keyword">in</span> metrics&#125;</span><br><span class="line">                formats[<span class="string">"grid_size"</span>] = <span class="string">"%2d"</span></span><br><span class="line">                formats[<span class="string">"cls_acc"</span>] = <span class="string">"%.2f%%"</span></span><br><span class="line">                row_metrics = [formats[metric] % yolo.metrics.get(metric, <span class="number">0</span>) <span class="keyword">for</span> yolo <span class="keyword">in</span> model.yolo_layers]</span><br><span class="line">                metric_table += [[metric, *row_metrics]]</span><br><span class="line"> </span><br><span class="line">                <span class="comment"># Tensorboard logging</span></span><br><span class="line">                tensorboard_log = []</span><br><span class="line">                <span class="keyword">for</span> j, yolo <span class="keyword">in</span> enumerate(model.yolo_layers):</span><br><span class="line">                    <span class="keyword">for</span> name, metric <span class="keyword">in</span> yolo.metrics.items():</span><br><span class="line">                        <span class="keyword">if</span> name != <span class="string">"grid_size"</span>:</span><br><span class="line">                            tensorboard_log += [(<span class="string">f"<span class="subst">&#123;name&#125;</span>_<span class="subst">&#123;j+<span class="number">1</span>&#125;</span>"</span>, metric)]</span><br><span class="line">                tensorboard_log += [(<span class="string">"loss"</span>, loss.item())]</span><br><span class="line">                logger.list_of_scalars_summary(tensorboard_log, batches_done)</span><br><span class="line"> </span><br><span class="line">            log_str += AsciiTable(metric_table).table</span><br><span class="line">            log_str += <span class="string">f"\nTotal loss <span class="subst">&#123;loss.item()&#125;</span>"</span></span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Determine approximate time left for epoch</span></span><br><span class="line">            epoch_batches_left = len(dataloader) - (batch_i + <span class="number">1</span>)</span><br><span class="line">            time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + <span class="number">1</span>))</span><br><span class="line">            log_str += <span class="string">f"\n---- ETA <span class="subst">&#123;time_left&#125;</span>"</span></span><br><span class="line"> </span><br><span class="line">            print(log_str)</span><br><span class="line"> </span><br><span class="line">            model.seen += imgs.size(<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> epoch % opt.evaluation_interval == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\n---- Evaluating Model ----"</span>)</span><br><span class="line">            <span class="comment"># Evaluate the model on the validation set</span></span><br><span class="line">            precision, recall, AP, f1, ap_class = evaluate(</span><br><span class="line">                model,</span><br><span class="line">                path=valid_path,</span><br><span class="line">                iou_thres=<span class="number">0.5</span>,</span><br><span class="line">                conf_thres=<span class="number">0.5</span>,</span><br><span class="line">                nms_thres=<span class="number">0.5</span>,</span><br><span class="line">                img_size=opt.img_size,</span><br><span class="line">                batch_size=<span class="number">8</span>,</span><br><span class="line">            )</span><br><span class="line">            evaluation_metrics = [</span><br><span class="line">                (<span class="string">"val_precision"</span>, precision.mean()),</span><br><span class="line">                (<span class="string">"val_recall"</span>, recall.mean()),</span><br><span class="line">                (<span class="string">"val_mAP"</span>, AP.mean()),</span><br><span class="line">                (<span class="string">"val_f1"</span>, f1.mean()),</span><br><span class="line">            ]</span><br><span class="line">            logger.list_of_scalars_summary(evaluation_metrics, epoch)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Print class APs and mAP</span></span><br><span class="line">            ap_table = [[<span class="string">"Index"</span>, <span class="string">"Class name"</span>, <span class="string">"AP"</span>]]</span><br><span class="line">            <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(ap_class):</span><br><span class="line">                ap_table += [[c, class_names[c], <span class="string">"%.5f"</span> % AP[i]]]</span><br><span class="line">            print(AsciiTable(ap_table).table)</span><br><span class="line">            print(<span class="string">f"---- mAP <span class="subst">&#123;AP.mean()&#125;</span>"</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> epoch % opt.checkpoint_interval == <span class="number">0</span>:</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">f"checkpoints/yolov3_ckpt_%d.pth"</span> % epoch)</span><br></pre></td></tr></table></figure>
<h5 id="展示训练进度"><a href="#展示训练进度" class="headerlink" title="展示训练进度"></a>展示训练进度</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log_str = <span class="string">"\n---- [Epoch %d/%d, Batch %d/%d] ----\n"</span> % (epoch, opt.epochs, batch_i, len(dataloader))</span><br><span class="line">            metric_table = [[<span class="string">"Metrics"</span>, *[<span class="string">f"YOLO Layer <span class="subst">&#123;i&#125;</span>"</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model.yolo_layers))]]]</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200924155836773.png#pic_center" alt="在这里插入图片描述"></p>
<h5 id="获取指标"><a href="#获取指标" class="headerlink" title="获取指标"></a>获取指标</h5><p>从<strong>metrics</strong>中获取指标类型，并保存到<strong>format</strong>中。<br><img src="https://img-blog.csdnimg.cn/20200924160004857.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>下一步便通过for循环获取3个yolo层的各项指标，如grid_size、loss、坐标等。并保存在metric_table列表中：<br><img src="https://img-blog.csdnimg.cn/20200924160259886.png#pic_center" alt="在这里插入图片描述"><br>并通过以下代码解析yolo层的参数，放进列表tensorboard_log中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensorboard_log = []</span><br><span class="line">                <span class="keyword">for</span> j, yolo <span class="keyword">in</span> enumerate(model.yolo_layers):</span><br><span class="line">                    <span class="keyword">for</span> name, metric <span class="keyword">in</span> yolo.metrics.items():</span><br><span class="line">                        <span class="keyword">if</span> name != <span class="string">"grid_size"</span>:</span><br><span class="line">                            tensorboard_log += [(<span class="string">f"<span class="subst">&#123;name&#125;</span>_<span class="subst">&#123;j+<span class="number">1</span>&#125;</span>"</span>, metric)]</span><br><span class="line">                tensorboard_log += [(<span class="string">"loss"</span>, loss.item())]</span><br><span class="line">                logger.list_of_scalars_summary(tensorboard_log, batches_done)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200924160407747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200924160418500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>使用log_str打印各项指标参数：<br><img src="https://img-blog.csdnimg.cn/20200924160453237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="评估训练情况"><a href="#评估训练情况" class="headerlink" title="评估训练情况"></a>评估训练情况</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">precision, recall, AP, f1, ap_class = evaluate(</span><br><span class="line">                model,</span><br><span class="line">                path=valid_path,</span><br><span class="line">                iou_thres=<span class="number">0.5</span>,</span><br><span class="line">                conf_thres=<span class="number">0.5</span>,</span><br><span class="line">                nms_thres=<span class="number">0.5</span>,</span><br><span class="line">                img_size=opt.img_size,</span><br><span class="line">                batch_size=<span class="number">8</span>,</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<p>使用<strong>evaluate函数</strong>得到各项指标，evaluate函数完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, path, iou_thres, conf_thres, nms_thres, img_size, batch_size)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Get dataloader</span></span><br><span class="line">    dataset = ListDataset(path, img_size=img_size, augment=<span class="literal">False</span>, multiscale=<span class="literal">False</span>)</span><br><span class="line">    dataloader = torch.utils.data.DataLoader(</span><br><span class="line">        dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">1</span>, collate_fn=dataset.collate_fn</span><br><span class="line">    )</span><br><span class="line"> </span><br><span class="line">    Tensor = torch.cuda.FloatTensor <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line"> </span><br><span class="line">    labels = []</span><br><span class="line">    sample_metrics = []  <span class="comment"># List of tuples (TP, confs, pred)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch_i, (_, imgs, targets) <span class="keyword">in</span> enumerate(tqdm.tqdm(dataloader, desc=<span class="string">"Detecting objects"</span>)):</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Extract labels</span></span><br><span class="line">        labels += targets[:, <span class="number">1</span>].tolist()</span><br><span class="line">        <span class="comment"># Rescale target</span></span><br><span class="line">        targets[:, <span class="number">2</span>:] = xywh2xyxy(targets[:, <span class="number">2</span>:])</span><br><span class="line">        targets[:, <span class="number">2</span>:] *= img_size</span><br><span class="line"> </span><br><span class="line">        imgs = Variable(imgs.type(Tensor), requires_grad=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            outputs = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)</span><br><span class="line"> </span><br><span class="line">        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Concatenate sample statistics</span></span><br><span class="line">    true_positives, pred_scores, pred_labels = [np.concatenate(x, <span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> list(zip(*sample_metrics))]</span><br><span class="line">    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> precision, recall, AP, f1, ap_class</span><br></pre></td></tr></table></figure>
<p>这段代码思路很清晰，加载数据和标签，这句代码是上段的核心：<code>sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)</code>。<br>评估的时候主要需要2个值，1、样本标注值。2、模型输出值。</p>
<p>1、<strong>样本的标注值</strong>。为了方便理解，这里简单回顾一下：voclabel.py会生成标注文件，保存在xxxx.txt文件中，每个.txt文件中的内容为了不混淆，我们称之为boxes，其boxes=【class id, x, y, w, h】按这种形式进行保存的。在ListDataset类中的<strong>getitem</strong>函数，会读取这个boxes，并把它从x，y，w，h（已经归一化成0~1）转换成对应特征图大小下的x，y，w，h的形式，并保存为targets。（ targets = torch.zeros((len(boxes), 6))   ；targets[:, 1:] = boxes ）。不过在评估的时候，为了方便计算IOU值，把target的坐标从x，y，w，h转换到xmin，ymin，xmax，ymax。</p>
<p>2、<strong>模型输出值</strong>。模型的输出output的shape为【batch_size,10647,5+class】。经过非极大值抑制处理之后，outputs的变成了一个列表，根据非极大值抑制处理的说明<code>Returns detections with shape: (x1, y1, x2, y2, object_conf, class_score, class_pred)</code>，output变成了一个列表，长度为batch_size(下图设置的是8)，可以看到每一个列表元素对应的张量的shape都是不一样的，这是因为每一张图片经过非极大值抑制处理之后剩下的boxes是不一样的，即tensor.shape(0)是不一样的，但tensor.shape(1)均为7，对应的是<code>(x1, y1, x2, y2, object_conf, class_score, class_pred)</code>。<br><img src="https://img-blog.csdnimg.cn/20200924161757438.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200924161814266.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200924161920323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>同时使用了<strong>get_batch_statistics</strong>函数，获取测试样本的各项指标。结合下面代码，不难理解。其完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch_statistics</span><span class="params">(outputs, targets, iou_threshold)</span>:</span></span><br><span class="line">    <span class="string">""" Compute true positives, predicted scores and predicted labels per sample """</span></span><br><span class="line">    batch_metrics = []</span><br><span class="line">    <span class="keyword">for</span> sample_i <span class="keyword">in</span> range(len(outputs)):</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> outputs[sample_i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line">        output = outputs[sample_i]</span><br><span class="line">        pred_boxes = output[:, :<span class="number">4</span>]</span><br><span class="line">        pred_scores = output[:, <span class="number">4</span>]</span><br><span class="line">        pred_labels = output[:, <span class="number">-1</span>]</span><br><span class="line"> </span><br><span class="line">        true_positives = np.zeros(pred_boxes.shape[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#这句把对应ID下的target和图像进行匹配，使用collate_fn函数给target赋予ID。</span></span><br><span class="line">        annotations = targets[targets[:, <span class="number">0</span>] == sample_i][:, <span class="number">1</span>:]</span><br><span class="line">        target_labels = annotations[:, <span class="number">0</span>] <span class="keyword">if</span> len(annotations) <span class="keyword">else</span> []</span><br><span class="line">        <span class="keyword">if</span> len(annotations):</span><br><span class="line">            detected_boxes = []</span><br><span class="line">            target_boxes = annotations[:, <span class="number">1</span>:]</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">for</span> pred_i, (pred_box, pred_label) <span class="keyword">in</span> enumerate(zip(pred_boxes, pred_labels)):</span><br><span class="line"> </span><br><span class="line">                <span class="comment"># If targets are found break</span></span><br><span class="line">                <span class="keyword">if</span> len(detected_boxes) == len(annotations):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">                <span class="comment"># Ignore if label is not one of the target labels</span></span><br><span class="line">                <span class="keyword">if</span> pred_label <span class="keyword">not</span> <span class="keyword">in</span> target_labels:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line">                iou, box_index = bbox_iou(pred_box.unsqueeze(<span class="number">0</span>), target_boxes).max(<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">if</span> iou &gt;= iou_threshold <span class="keyword">and</span> box_index <span class="keyword">not</span> <span class="keyword">in</span> detected_boxes:</span><br><span class="line">                    true_positives[pred_i] = <span class="number">1</span></span><br><span class="line">                    detected_boxes += [box_index]</span><br><span class="line">        batch_metrics.append([true_positives, pred_scores, pred_labels])</span><br><span class="line">    <span class="keyword">return</span> batch_metrics</span><br></pre></td></tr></table></figure>
<p>回到evaluate函数：<br><strong>precision, recall, AP, f1, ap_class</strong>值则是使用<strong>ap_per_class</strong>函数进行计算，完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ap_per_class</span><span class="params">(tp, conf, pred_cls, target_cls)</span>:</span></span><br><span class="line">    <span class="string">""" Compute the average precision, given the recall and precision curves.</span></span><br><span class="line"><span class="string">    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.</span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        tp:    True positives (list).</span></span><br><span class="line"><span class="string">        conf:  Objectness value from 0-1 (list).</span></span><br><span class="line"><span class="string">        pred_cls: Predicted object classes (list).</span></span><br><span class="line"><span class="string">        target_cls: True object classes (list).</span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        The average precision as computed in py-faster-rcnn.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Sort by objectness</span></span><br><span class="line">    i = np.argsort(-conf)</span><br><span class="line">    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Find unique classes</span></span><br><span class="line">    unique_classes = np.unique(target_cls)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Create Precision-Recall curve and compute AP for each class</span></span><br><span class="line">    ap, p, r = [], [], []</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> tqdm.tqdm(unique_classes, desc=<span class="string">"Computing AP"</span>):</span><br><span class="line">        i = pred_cls == c</span><br><span class="line">        n_gt = (target_cls == c).sum()  <span class="comment"># Number of ground truth objects</span></span><br><span class="line">        n_p = i.sum()  <span class="comment"># Number of predicted objects</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> n_p == <span class="number">0</span> <span class="keyword">and</span> n_gt == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> n_p == <span class="number">0</span> <span class="keyword">or</span> n_gt == <span class="number">0</span>:</span><br><span class="line">            ap.append(<span class="number">0</span>)</span><br><span class="line">            r.append(<span class="number">0</span>)</span><br><span class="line">            p.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Accumulate FPs and TPs</span></span><br><span class="line">            fpc = (<span class="number">1</span> - tp[i]).cumsum()</span><br><span class="line">            tpc = (tp[i]).cumsum()</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Recall</span></span><br><span class="line">            recall_curve = tpc / (n_gt + <span class="number">1e-16</span>)</span><br><span class="line">            r.append(recall_curve[<span class="number">-1</span>])</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Precision</span></span><br><span class="line">            precision_curve = tpc / (tpc + fpc)</span><br><span class="line">            p.append(precision_curve[<span class="number">-1</span>])</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># AP from recall-precision curve</span></span><br><span class="line">            ap.append(compute_ap(recall_curve, precision_curve))</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Compute F1 score (harmonic mean of precision and recall)</span></span><br><span class="line">    p, r, ap = np.array(p), np.array(r), np.array(ap)</span><br><span class="line">    f1 = <span class="number">2</span> * p * r / (p + r + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> p, r, ap, f1, unique_classes.astype(<span class="string">"int32"</span>)</span><br></pre></td></tr></table></figure>
<p>最后，在训练到一定程度的时候，便保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % opt.checkpoint_interval == <span class="number">0</span>:</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">f"checkpoints/yolov3_ckpt_%d.pth"</span> % epoch)</span><br></pre></td></tr></table></figure>
<p>以上，train.py基本分析完毕。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后的最后，真的是最后了！总结一下！</p>
<h4 id="Yolo-v3-Structure"><a href="#Yolo-v3-Structure" class="headerlink" title="Yolo_v3_Structure"></a>Yolo_v3_Structure</h4><p><img src="https://img-blog.csdnimg.cn/20200924163036860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>（图借鉴自<a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a>）</p>
<h4 id="检测流程（detect-py"><a href="#检测流程（detect-py" class="headerlink" title="检测流程（detect.py)"></a>检测流程（detect.py)</h4><p><img src="https://img-blog.csdnimg.cn/20200924163311998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="训练流程（train-py"><a href="#训练流程（train-py" class="headerlink" title="训练流程（train.py)"></a>训练流程（train.py)</h4><p><img src="https://img-blog.csdnimg.cn/2020092416341434.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="损失函数（train-py"><a href="#损失函数（train-py" class="headerlink" title="损失函数（train.py)"></a>损失函数（train.py)</h4><p><img src="https://img-blog.csdnimg.cn/20200924163603334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><p>搭建YOLOv3入门教程  <a href="https://link.zhihu.com/?target=https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/">https://link.zhihu.com/?target=https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/</a></p>
</li>
<li><p>How to implement a YOLO (v3) object detector from scratch in <a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/" target="_blank" rel="noopener">PyTorchhttps://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/</a><br>附中文翻译：<a href="https://link.zhihu.com/?target=https://www.jiqizhixin.com/articles/2018-04-23-3">上部分</a> <a href="https://link.zhihu.com/?target=https://www.jiqizhixin.com/articles/042602?from=synced&keyword=%25E4%25BB%258E%25E9%259B%25B6%25E5%25BC%2580%25E5%25A7%258BPyTorch%25E9%25A1%25B9%25E7%259B%25AE%25EF%25BC%259AYOLO%2520v3%25E7%259B%25AE%25E6%25A0%2587%25E6%25A3%2580%25E6%25B5%258B%25E5%25AE%259E%25E7%258E%25B0">下部分</a></p>
</li>
<li><p>Pytorch | yolov3原理及代码详解(有四个系列，文章里面有链接）<a href="https://blog.csdn.net/qq_24739717/article/details/96705055" target="_blank" rel="noopener">https://blog.csdn.net/qq_24739717/article/details/96705055</a></p>
</li>
<li><p>超详细的Pytorch版yolov3代码中文注释详解 <a href="https://zhuanlan.zhihu.com/p/49981816" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49981816</a></p>
</li>
<li><p>史上最详细的Yolov3边框预测分析 <a href="https://blog.csdn.net/qq_34199326/article/details/84109828" target="_blank" rel="noopener">https://blog.csdn.net/qq_34199326/article/details/84109828</a></p>
</li>
</ol>

    </article>
    <!-- license  -->
    
        <div class="license-wrapper">
        </div>
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href= "/2020/10/19/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E7%94%A8imgaug%E5%AF%B9%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" title= "大创学习记录（四）之用imgaug对图片进行数据增强">
                    <div class="nextTitle">大创学习记录（四）之用imgaug对图片进行数据增强</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2020/09/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0Week1%EF%BC%9AIntroduction%20/" title= "吴恩达神经网络与深度学习课程笔记Week1：Introduction">
                    <div class="prevTitle">吴恩达神经网络与深度学习课程笔记Week1：Introduction</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!-- gitalk评论 -->

    <!-- utteranc评论 -->

    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:1419009880@qq.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="https://github.com/Phoebeyu731" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
            
                <span class="iconfont-archer wechat" title=wechat>
                  
                  <img class="profile-qr" src="/intro/wechat.jpg" />
                </span>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-YOLOv3项目训练与代码学习"><span class="toc-number">1.</span> <span class="toc-text">PyTorch-YOLOv3项目训练与代码学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch"><span class="toc-number">1.1.</span> <span class="toc-text">PyTorch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO"><span class="toc-number">1.2.</span> <span class="toc-text">YOLO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-YOLOv3"><span class="toc-number">1.3.</span> <span class="toc-text">PyTorch-YOLOv3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#所需环境"><span class="toc-number">1.3.1.</span> <span class="toc-text">所需环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文件下载"><span class="toc-number">1.3.2.</span> <span class="toc-text">文件下载</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Run-the-detector"><span class="toc-number">1.4.</span> <span class="toc-text">Run the detector</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLOv3-Keras（训练自己的权重来预测）"><span class="toc-number">1.5.</span> <span class="toc-text">YOLOv3-Keras（训练自己的权重来预测）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码详细分析"><span class="toc-number">1.6.</span> <span class="toc-text">代码详细分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#detect-py"><span class="toc-number">1.6.1.</span> <span class="toc-text">detect.py</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#模型初始化"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#YOLOv3（darknet模型）"><span class="toc-number">1.6.1.1.1.</span> <span class="toc-text">YOLOv3（darknet模型）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#模型预测"><span class="toc-number">1.6.1.2.</span> <span class="toc-text">模型预测</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#获取检测框"><span class="toc-number">1.6.1.2.1.</span> <span class="toc-text">获取检测框</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#非极大值抑制"><span class="toc-number">1.6.1.2.2.</span> <span class="toc-text">非极大值抑制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#train-py"><span class="toc-number">1.6.2.</span> <span class="toc-text">train.py</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#训练前准备工作"><span class="toc-number">1.6.2.1.</span> <span class="toc-text">训练前准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#初始化"><span class="toc-number">1.6.2.1.1.</span> <span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#加载网络"><span class="toc-number">1.6.2.1.2.</span> <span class="toc-text">加载网络</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#放进DataLoader"><span class="toc-number">1.6.2.1.3.</span> <span class="toc-text">放进DataLoader</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#训练并计算loss"><span class="toc-number">1.6.2.2.</span> <span class="toc-text">训练并计算loss</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#开始迭代"><span class="toc-number">1.6.2.2.1.</span> <span class="toc-text">开始迭代</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#从batch中获取图片，从label中获取标签"><span class="toc-number">1.6.2.2.2.</span> <span class="toc-text">从batch中获取图片，从label中获取标签</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#计算Loss"><span class="toc-number">1.6.2.2.3.</span> <span class="toc-text">计算Loss</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#坐标预测"><span class="toc-number">1.6.2.2.3.1.</span> <span class="toc-text">坐标预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#查看训练指标并评估"><span class="toc-number">1.6.2.3.</span> <span class="toc-text">查看训练指标并评估</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#展示训练进度"><span class="toc-number">1.6.2.3.1.</span> <span class="toc-text">展示训练进度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#获取指标"><span class="toc-number">1.6.2.3.2.</span> <span class="toc-text">获取指标</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#评估训练情况"><span class="toc-number">1.6.2.3.3.</span> <span class="toc-text">评估训练情况</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-number">1.6.3.</span> <span class="toc-text">总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Yolo-v3-Structure"><span class="toc-number">1.6.3.1.</span> <span class="toc-text">Yolo_v3_Structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#检测流程（detect-py"><span class="toc-number">1.6.3.2.</span> <span class="toc-text">检测流程（detect.py)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#训练流程（train-py"><span class="toc-number">1.6.3.3.</span> <span class="toc-text">训练流程（train.py)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#损失函数（train-py"><span class="toc-number">1.6.3.4.</span> <span class="toc-text">损失函数（train.py)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">1.7.</span> <span class="toc-text">Reference</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 8
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2020/10/19/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E7%94%A8imgaug%E5%AF%B9%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" >大创学习记录（四）之用imgaug对图片进行数据增强</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2020 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/24</span><a class="archive-post-title" href= "/2020/09/24/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8Byolov3%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" >大创学习记录（三）之yolov3代码学习</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span><a class="archive-post-title" href= "/2020/09/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0Week1%EF%BC%9AIntroduction%20/" >吴恩达神经网络与深度学习课程笔记Week1：Introduction</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/05</span><a class="archive-post-title" href= "/2020/07/05/%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87%E4%B9%8B%E7%A0%94%E8%AF%BB%E8%AE%BA%E6%96%87/" >论文准备之研读论文</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/29</span><a class="archive-post-title" href= "/2020/06/29/%E5%A4%A7%E4%BA%8C%E4%B8%8B%E5%AD%A6%E6%9C%9F%E6%9C%AB%E6%80%BB%E7%BB%93/" >大二下学期末总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/28</span><a class="archive-post-title" href= "/2020/06/28/how%20to%20build%20your%20blog_/" >build your blog</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/06</span><a class="archive-post-title" href= "/2020/05/06/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8BCNN-1%20LeNet5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E5%AD%A6%E4%B9%A0/" >大创学习记录（二）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/02</span><a class="archive-post-title" href= "/2020/05/02/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" >大创学习记录（一）</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="hexo blog"><span class="iconfont-archer">&#xe606;</span>hexo blog</span>
    
        <span class="sidebar-tag-name" data-tags="大二下"><span class="iconfont-archer">&#xe606;</span>大二下</span>
    
        <span class="sidebar-tag-name" data-tags="tensorflow"><span class="iconfont-archer">&#xe606;</span>tensorflow</span>
    
        <span class="sidebar-tag-name" data-tags="LeNet-5"><span class="iconfont-archer">&#xe606;</span>LeNet-5</span>
    
        <span class="sidebar-tag-name" data-tags="refined box"><span class="iconfont-archer">&#xe606;</span>refined box</span>
    
        <span class="sidebar-tag-name" data-tags="深度学习"><span class="iconfont-archer">&#xe606;</span>深度学习</span>
    
        <span class="sidebar-tag-name" data-tags="PyTorch-YOLOv3"><span class="iconfont-archer">&#xe606;</span>PyTorch-YOLOv3</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="小白的技术成长之路"><span class="iconfont-archer">&#xe60a;</span>小白的技术成长之路</span>
    
        <span class="sidebar-category-name" data-categories="本科学期总结"><span class="iconfont-archer">&#xe60a;</span>本科学期总结</span>
    
        <span class="sidebar-category-name" data-categories="大创学习体会"><span class="iconfont-archer">&#xe60a;</span>大创学习体会</span>
    
        <span class="sidebar-category-name" data-categories="论文"><span class="iconfont-archer">&#xe60a;</span>论文</span>
    
        <span class="sidebar-category-name" data-categories="吴恩达神经网络与深度学习课程笔记"><span class="iconfont-archer">&#xe60a;</span>吴恩达神经网络与深度学习课程笔记</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "John Doe"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    </body>
</html>


