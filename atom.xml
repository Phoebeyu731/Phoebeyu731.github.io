<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-09-24T09:52:41.941Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大创学习记录（三）之yolov3代码学习</title>
    <link href="http://yoursite.com/2020/09/24/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8Byolov3%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/09/24/大创学习记录（三）之yolov3代码学习/</id>
    <published>2020-09-24T09:51:55.000Z</published>
    <updated>2020-09-24T09:52:41.941Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch-YOLOv3项目训练与代码学习"><a href="#PyTorch-YOLOv3项目训练与代码学习" class="headerlink" title="PyTorch-YOLOv3项目训练与代码学习"></a>PyTorch-YOLOv3项目训练与代码学习</h1><blockquote><p>借助从零开始的PyTorch项目理解YOLOv3目标检测的实现</p></blockquote><h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><p>对于PyTorch就不用多说了，目前最灵活、最容易掌握的深度学习库，它有诸多优点，举下面三个例子：</p><ul><li><strong>易于使用的API</strong>-它就像Python一样简单。</li><li><strong>Python的支持</strong>—如上所述，PyTorch可以顺利地与Python数据科学栈集成。</li><li><strong>动态计算图</strong>—取代了具有特定功能的预定义图形，PyTorch为我们提供了一个框架，以便可以在运行时构建计算图，甚至在运行时更改它们。在不知道创建神经网络需要多少内存的情况下这非常有价值。<br>还有比如多gpu支持，自定义数据加载器和简化的预处理器等优点，若想要了解更多细节，可参考<a href="https://www.jianshu.com/p/d66319506dd7" target="_blank" rel="noopener">PyTorch入门</a></li></ul><p><img src="https://img-blog.csdnimg.cn/20200920111240441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p>上次大创启动会的时候简单给大家分享了YOLO算法的原理，这里放上自己参考的文章<a href="https://www.cnblogs.com/ywheunji/p/10761239.html" target="_blank" rel="noopener">YOLOv1到YOLOv3的演变过程</a><br>以及一些详细讲述了工作原理、训练过程和与其他检测器的性能规避的原始论文：</p><ul><li><a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="noopener">YOLO V1: You Only Look Once: Unified, Real-Time Object Detection</a></li><li><a href="https://arxiv.org/pdf/1612.08242.pdf" target="_blank" rel="noopener">YOLO V2: YOLO9000: Better, Faster, Stronger</a></li><li><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">YOLO V3: An Incremental Improvement</a></li><li><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">Convolutional Neural Networks</a></li><li><a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Bounding Box Regression (Appendix C)</a></li><li><a href="https://www.youtube.com/watch?v=DNEm4fJ-rto" target="_blank" rel="noopener">IoU</a></li><li><a href="https://www.youtube.com/watch?v=A46HZGR5fMw" target="_blank" rel="noopener">Non maximum suppresion</a></li><li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">PyTorch Official Tutorial</a></li></ul><h2 id="PyTorch-YOLOv3"><a href="#PyTorch-YOLOv3" class="headerlink" title="PyTorch-YOLOv3"></a>PyTorch-YOLOv3</h2><h3 id="所需环境"><a href="#所需环境" class="headerlink" title="所需环境"></a>所需环境</h3><p>==在这里先记录一个创建环境的问题==，（要是创建环境的时候遇到问题可以拿去用一用）打开anaconda的时候想要创建一个yolo的环境，就用了最简单的创建环境的命令行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n yolo python=3.7</span><br></pre></td></tr></table></figure><p>但是创建了一晚上，需要的那些包都没下载下来，并且报错：</p><blockquote><p>“Multiple Errors Encountered”</p></blockquote><p>`解决办法：更换下载源（更换了以后，下载速度快到飞起，我还一直以为是网络问题）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>回到正题，需要的环境有：</p><p>==PyTorch==环境搭建（具体搭建过程可以参考我的一篇博客<a href="https://phoebeyu731.github.io/2020/05/06/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/" target="_blank" rel="noopener">大创学习记录（二）</a>）<br>==Python== 3.5<br>==OpenCV==（在搭建好的PyTorch环境下输入命令<code>pip install opencv-python</code>即可）</p><h3 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h3><p>权重文件下载<a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener">yolov3_weights</a><br>或者使用的是Linux系统，可以在终端输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://pjreddie.com/media/files/yolov3.weights</span><br></pre></td></tr></table></figure><h2 id="Run-the-detector"><a href="#Run-the-detector" class="headerlink" title="Run the detector"></a>Run the detector</h2><ol><li>下载yolo源代码，进入到代码存放的目录（<a href="https://github.com/ayooshkathuria/pytorch-yolo-v3" target="_blank" rel="noopener">github地址</a>）</li><li>下载权重文件，并且放到源代码下载的目录下<br><img src="https://img-blog.csdnimg.cn/20200920123016892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>运行<code>python detect.py --images imgs --det det</code></li></ol><p> –images标志定义从中加载图像的目录或单个图像文件，而–det是将图像保存到的目录<br><img src="https://img-blog.csdnimg.cn/20200920164849390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对应的结果：<br><img src="https://img-blog.csdnimg.cn/20200920165141558.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>还可以通过更多flag改变精度和速度，输入<code>python detect.py -h</code>可以查看<br><img src="https://img-blog.csdnimg.cn/20200920165515522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>==需要注意：==<br>可以通过–reso标志更改输入图像的分辨率。 默认值为416。无论您选择什么值，都请记住它应该是32的倍数并且大于32（比如<code>python detect.py --images imgs --det det --reso 320</code>）。<br> 4. <strong>On Video</strong><br> 运行<code>python video_demo.py --video video.avi</code><br> 视频文件应为.avi格式，因为openCV仅接受avi作为输入格式。<br> 5. <strong>On a Camera</strong><br> 运行<code>python cam_demo.py</code><br> 这里会打开电脑的摄像头，进行识别（因为懒得给自己打码了，就不把照片po上来了，有兴趣的可以自己去运行一下）。</p><p>==改变训练权重：== 一些权重文件的下载地址：<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">yolo website</a><br>==改变物体检测规模：== YOLO v3进行跨不同级别的检测，每种检测都代表检测不同大小的对象，可以通过–scales标志来尝试这些比例，比如输入<code>python detect.py --scales 1,3</code></p><h2 id="YOLOv3-Keras（训练自己的权重来预测）"><a href="#YOLOv3-Keras（训练自己的权重来预测）" class="headerlink" title="YOLOv3-Keras（训练自己的权重来预测）"></a>YOLOv3-Keras（训练自己的权重来预测）</h2><p>本文是基于PyTorch的环境下训练的，另一个基于Keras的也是十分重要的，参考文章<a href="https://blog.csdn.net/Patrick_Lxc/article/details/80615433" target="_blank" rel="noopener">Keras/Tensorflow+python+yolo3训练自己的数据集</a><br>以及<a href="https://jennyvanessa.github.io/2020/09/21/%E5%88%A9%E7%94%A8Keras%E5%AE%9E%E7%8E%B0Yolov3/" target="_blank" rel="noopener">jennyvanessa的blog之利用Keras实现Yolov3</a></p><h2 id="代码详细分析"><a href="#代码详细分析" class="headerlink" title="代码详细分析"></a>代码详细分析</h2><p>接下来仔细看一下YOLOv3代码的细节，只有在代码中才能完全理解YOLOv3的思想。但是前面那个代码我跑的那个代码只有官方提供的测试的部分，并不包含训练部分，所以又去找了一个完整的代码，附上地址<a href="https://github.com/eriklindernoren/PyTorch-YOLOv3" target="_blank" rel="noopener">PyTorch-YOLOv3github地址</a>，关于这个项目详细的使用以及测试过程在相应的github地址的readme的文档中已经列出，我也已经完全按照上面的过程跑过一遍代码了，没有问题，接下来分析它的代码。</p><h3 id="detect-py"><a href="#detect-py" class="headerlink" title="detect.py"></a>detect.py</h3><h4 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.datasets <span class="keyword">import</span> *</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="keyword">from</span> matplotlib.ticker <span class="keyword">import</span> NullLocator</span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">（1）import argparse    首先导入模块</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   （2）parser = argparse.ArgumentParser（）    创建一个解析对象</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    （3）parser.add_argument()    向该对象中添加你要关注的命令行参数和选项</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser.add_argument(<span class="string">"--image_folder"</span>, type=str, default=<span class="string">"data/samples"</span>, help=<span class="string">"path to dataset"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--model_def"</span>, type=str, default=<span class="string">"config/yolov3.cfg"</span>, help=<span class="string">"path to model definition file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--weights_path"</span>, type=str, default=<span class="string">"weights/yolov3.weights"</span>, help=<span class="string">"path to weights file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--class_path"</span>, type=str, default=<span class="string">"data/coco.names"</span>, help=<span class="string">"path to class label file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--conf_thres"</span>, type=float, default=<span class="number">0.8</span>, help=<span class="string">"object confidence threshold"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--nms_thres"</span>, type=float, default=<span class="number">0.4</span>, help=<span class="string">"iou thresshold for non-maximum suppression"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--batch_size"</span>, type=int, default=<span class="number">1</span>, help=<span class="string">"size of the batches"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--n_cpu"</span>, type=int, default=<span class="number">0</span>, help=<span class="string">"number of cpu threads to use during batch generation"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--img_size"</span>, type=int, default=<span class="number">416</span>, help=<span class="string">"size of each image dimension"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--checkpoint_model"</span>, type=str, help=<span class="string">"path to checkpoint model"</span>)</span><br><span class="line">     <span class="string">"""</span></span><br><span class="line"><span class="string">    （4）parser.parse_args()    进行解析</span></span><br><span class="line"><span class="string">     """</span></span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line">    print(opt)</span><br><span class="line">    <span class="comment">#选择是否使用GPU设备</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">     <span class="comment">#创建多级目录</span></span><br><span class="line">    os.makedirs(<span class="string">"output"</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Set up model  调用darknet模型</span></span><br><span class="line">    model = Darknet(opt.model_def, img_size=opt.img_size).to(device)</span><br></pre></td></tr></table></figure><p>最后这句话，model = Darknet(opt.model_def, img_size=opt.img_size).to(device)，这条语句加载了==darknet==模型，即==YOLOv3==模型，所以接下来我们再看Darknet模型，这个模型在==model.py==中定义。</p><h5 id="YOLOv3（darknet模型）"><a href="#YOLOv3（darknet模型）" class="headerlink" title="YOLOv3（darknet模型）"></a>YOLOv3（darknet模型）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Darknet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""YOLOv3 object detection model"""</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config_path, img_size=<span class="number">416</span>)</span>:</span></span><br><span class="line">        super(Darknet, self).__init__()</span><br><span class="line">        <span class="comment">#解析cfg文件</span></span><br><span class="line">        self.module_defs = parse_model_config(config_path)</span><br><span class="line">        <span class="comment">#print("module_defs   : ",self.module_defs)</span></span><br><span class="line">        self.hyperparams, self.module_list = create_modules(self.module_defs)</span><br><span class="line">        <span class="comment">#print("module_list   : ",self.module_list)</span></span><br><span class="line">        <span class="comment"># hasattr() 函数用于判断对象是否包含对应的属性。</span></span><br><span class="line">        <span class="comment"># yolo层有 metrics 属性</span></span><br><span class="line">        self.yolo_layers = [layer[<span class="number">0</span>] <span class="keyword">for</span> layer <span class="keyword">in</span> self.module_list <span class="keyword">if</span> hasattr(layer[<span class="number">0</span>], <span class="string">"metrics"</span>)]</span><br><span class="line">        <span class="comment">#print("self.yolo_layers:\n",self.yolo_layers)</span></span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.seen = <span class="number">0</span></span><br><span class="line">        self.header_info = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, self.seen, <span class="number">0</span>], dtype=np.int32)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, targets=None)</span>:</span></span><br><span class="line">        img_dim = x.shape[<span class="number">2</span>]</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        layer_outputs, yolo_outputs = [], []</span><br><span class="line">        print(<span class="string">"x.shape: "</span>,x.shape)</span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">            <span class="comment">#print("module_defs   : ",module_def)</span></span><br><span class="line">            <span class="comment">#print("module   : ",module)</span></span><br><span class="line">            <span class="comment">#print("i: ",i," x.shape: ",x.shape)</span></span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] <span class="keyword">in</span> [<span class="string">"convolutional"</span>, <span class="string">"upsample"</span>, <span class="string">"maxpool"</span>]:</span><br><span class="line">                x = module(x)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"route"</span>:</span><br><span class="line">                print(<span class="string">"i: "</span>,i,<span class="string">" x.shape: "</span>,x.shape)</span><br><span class="line">                <span class="keyword">for</span> layer_i <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>):</span><br><span class="line">                    print(<span class="string">"layer_i:\n"</span>,layer_i)</span><br><span class="line">                x = torch.cat([layer_outputs[int(layer_i)] <span class="keyword">for</span> layer_i <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>)], <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">                layer_i = int(module_def[<span class="string">"from"</span>])</span><br><span class="line">                x = layer_outputs[<span class="number">-1</span>] + layer_outputs[layer_i]</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">                x, layer_loss = module[<span class="number">0</span>](x, targets, img_dim)</span><br><span class="line">                loss += layer_loss</span><br><span class="line">                yolo_outputs.append(x)</span><br><span class="line">            layer_outputs.append(x)</span><br><span class="line">        yolo_outputs = to_cpu(torch.cat(yolo_outputs, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> yolo_outputs <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> (loss, yolo_outputs)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_darknet_weights</span><span class="params">(self, weights_path)</span>:</span></span><br><span class="line">        <span class="string">"""Parses and loads the weights stored in 'weights_path'"""</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Open the weights file</span></span><br><span class="line">        <span class="keyword">with</span> open(weights_path, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            header = np.fromfile(f, dtype=np.int32, count=<span class="number">5</span>)  <span class="comment"># First five are header values</span></span><br><span class="line">            self.header_info = header  <span class="comment"># Needed to write header when saving weights</span></span><br><span class="line">            self.seen = header[<span class="number">3</span>]  <span class="comment"># number of images seen during training</span></span><br><span class="line">            weights = np.fromfile(f, dtype=np.float32)  <span class="comment"># The rest are weights</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("------------------------------------")</span></span><br><span class="line"><span class="string">            print("header:\n",header)</span></span><br><span class="line"><span class="string">            print("weights:\n",weights)</span></span><br><span class="line"><span class="string">            print("weights.shape:\n",weights.shape)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">        <span class="comment"># Establish cutoff for loading backbone weights</span></span><br><span class="line">        cutoff = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"darknet53.conv.74"</span> <span class="keyword">in</span> weights_path:</span><br><span class="line">            cutoff = <span class="number">75</span></span><br><span class="line"> </span><br><span class="line">        ptr = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">            <span class="comment">#print("i:\n",i)</span></span><br><span class="line">            <span class="comment">#print("module_def:\n",module_def)</span></span><br><span class="line">            <span class="comment">#print("module:\n",module)</span></span><br><span class="line">            <span class="keyword">if</span> i == cutoff:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] == <span class="string">"convolutional"</span>:</span><br><span class="line">                conv_layer = module[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> module_def[<span class="string">"batch_normalize"</span>]:</span><br><span class="line">                    <span class="comment"># Load BN bias, weights, running mean and running variance</span></span><br><span class="line">                    bn_layer = module[<span class="number">1</span>]</span><br><span class="line">                    num_b = bn_layer.bias.numel()  <span class="comment"># Number of biases</span></span><br><span class="line">                    <span class="comment">#print("bn_layer:\n",bn_layer)</span></span><br><span class="line">                    <span class="comment">#print("num_b:\n",num_b)</span></span><br><span class="line">                    <span class="comment"># Bias</span></span><br><span class="line">                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)</span><br><span class="line">                    bn_layer.bias.data.copy_(bn_b)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                    <span class="comment"># Weight</span></span><br><span class="line">                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)</span><br><span class="line">                    bn_layer.weight.data.copy_(bn_w)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                    <span class="comment"># Running Mean</span></span><br><span class="line">                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)</span><br><span class="line">                    bn_layer.running_mean.data.copy_(bn_rm)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                    <span class="comment"># Running Var</span></span><br><span class="line">                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)</span><br><span class="line">                    bn_layer.running_var.data.copy_(bn_rv)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># Load conv. bias</span></span><br><span class="line">                    num_b = conv_layer.bias.numel()</span><br><span class="line">                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)</span><br><span class="line">                    conv_layer.bias.data.copy_(conv_b)</span><br><span class="line">                    ptr += num_b</span><br><span class="line">                <span class="comment"># Load conv. weights</span></span><br><span class="line">                num_w = conv_layer.weight.numel()</span><br><span class="line">                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)</span><br><span class="line">                conv_layer.weight.data.copy_(conv_w)</span><br><span class="line">                ptr += num_w</span><br><span class="line">                <span class="comment">#print("conv_w:\n",conv_w)</span></span><br><span class="line">                <span class="comment">#print("num_w:\n",num_w)</span></span><br><span class="line">                <span class="comment">#print("ptr:\n",ptr)</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_darknet_weights</span><span class="params">(self, path, cutoff=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            @:param path    - path of the new weights file</span></span><br><span class="line"><span class="string">            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -&gt; all are saved)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        fp = open(path, <span class="string">"wb"</span>)</span><br><span class="line">        self.header_info[<span class="number">3</span>] = self.seen</span><br><span class="line">        self.header_info.tofile(fp)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Iterate through layers</span></span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):</span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] == <span class="string">"convolutional"</span>:</span><br><span class="line">                conv_layer = module[<span class="number">0</span>]</span><br><span class="line">                <span class="comment"># If batch norm, load bn first</span></span><br><span class="line">                <span class="keyword">if</span> module_def[<span class="string">"batch_normalize"</span>]:</span><br><span class="line">                    bn_layer = module[<span class="number">1</span>]</span><br><span class="line">                    bn_layer.bias.data.cpu().numpy().tofile(fp)</span><br><span class="line">                    bn_layer.weight.data.cpu().numpy().tofile(fp)</span><br><span class="line">                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)</span><br><span class="line">                    bn_layer.running_var.data.cpu().numpy().tofile(fp)</span><br><span class="line">                <span class="comment"># Load conv bias</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    conv_layer.bias.data.cpu().numpy().tofile(fp)</span><br><span class="line">                <span class="comment"># Load conv weights</span></span><br><span class="line">                conv_layer.weight.data.cpu().numpy().tofile(fp)</span><br><span class="line"> </span><br><span class="line">        fp.close()</span><br></pre></td></tr></table></figure><p>首先看<code>__init__()</code>函数，大致流程是从.cfg中解析文件，然后根据文件内容生成相关的网络结构。<br>解析后会生成一个列表，存储网络结构的各种属性，通过遍历这个列表便可以得到网络结构，解析后的列表如下图所示（部分）：<br><img src="https://img-blog.csdnimg.cn/20200921175928444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><code>self.hyperparams, self.module_list = create_modules(self.module_defs)</code>，这条语句会根据生成的列表构建网络结构，<code>create_modules（）</code>函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_modules</span><span class="params">(module_defs)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Constructs module list of layer blocks from module configuration in module_defs</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。</span></span><br><span class="line">    hyperparams = module_defs.pop(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#初始值对应于输入数据通道，"channels"，用来存储我们需要持续追踪被应用卷积层的卷积核数量（上一层的卷积核数量（或特征图深度））,并且我们不仅需要追踪前一层的卷积核数量，还需要追踪之前每个层。随着不断地迭代，我们将每个模块的输出卷积核数量添加到 output_filters 列表上。</span></span><br><span class="line">    output_filters = [int(hyperparams[<span class="string">"channels"</span>])]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># module_list用于存储每个block,每个block对应cfg文件中一个块，类似[convolutional]里面就对应一个卷积块</span></span><br><span class="line">    module_list = nn.ModuleList()</span><br><span class="line">    <span class="comment">#这里，我们迭代module_defs</span></span><br><span class="line">    <span class="keyword">for</span> module_i, module_def <span class="keyword">in</span> enumerate(module_defs):</span><br><span class="line">    <span class="comment"># 这里每个block用nn.sequential()创建为了一个module,一个module有多个层</span></span><br><span class="line">            modules = nn.Sequential()</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> module_def[<span class="string">"type"</span>] == <span class="string">"convolutional"</span>:</span><br><span class="line">        <span class="comment">#设置filter尺寸、数量，添加batch normalize层（在.cfg文件中batch_normalize=1），以及pad层</span></span><br><span class="line">            bn = int(module_def[<span class="string">"batch_normalize"</span>])</span><br><span class="line">            filters = int(module_def[<span class="string">"filters"</span>])</span><br><span class="line">            kernel_size = int(module_def[<span class="string">"size"</span>])</span><br><span class="line">            pad = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># 开始创建并添加相应层</span></span><br><span class="line">            <span class="comment"># Add the convolutional layer</span></span><br><span class="line">            <span class="comment"># nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True)</span></span><br><span class="line">            modules.add_module(</span><br><span class="line">                <span class="string">f"conv_<span class="subst">&#123;module_i&#125;</span>"</span>,</span><br><span class="line">                nn.Conv2d(</span><br><span class="line">                    in_channels=output_filters[<span class="number">-1</span>],</span><br><span class="line">                    out_channels=filters,</span><br><span class="line">                    kernel_size=kernel_size,</span><br><span class="line">                    stride=int(module_def[<span class="string">"stride"</span>]),</span><br><span class="line">                    padding=pad,</span><br><span class="line">                    bias=<span class="keyword">not</span> bn,</span><br><span class="line">                ),</span><br><span class="line">            )</span><br><span class="line">            <span class="comment">#Add the Batch Norm Layer</span></span><br><span class="line">            <span class="keyword">if</span> bn:</span><br><span class="line">                modules.add_module(<span class="string">f"batch_norm_<span class="subst">&#123;module_i&#125;</span>"</span>, nn.BatchNorm2d(filters, momentum=<span class="number">0.9</span>, eps=<span class="number">1e-5</span>))</span><br><span class="line">            <span class="comment">#检查激活函数 </span></span><br><span class="line">            <span class="comment">#It is either Linear or a Leaky ReLU for YOLO</span></span><br><span class="line">            <span class="comment"># 给定参数负轴系数0.1</span></span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"activation"</span>] == <span class="string">"leaky"</span>:</span><br><span class="line">                modules.add_module(<span class="string">f"leaky_<span class="subst">&#123;module_i&#125;</span>"</span>, nn.LeakyReLU(<span class="number">0.1</span>))</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"maxpool"</span>:</span><br><span class="line">            kernel_size = int(module_def[<span class="string">"size"</span>])</span><br><span class="line">            stride = int(module_def[<span class="string">"stride"</span>])</span><br><span class="line">            <span class="keyword">if</span> kernel_size == <span class="number">2</span> <span class="keyword">and</span> stride == <span class="number">1</span>:</span><br><span class="line">                <span class="comment">#保证输出是偶数</span></span><br><span class="line">                modules.add_module(<span class="string">f"_debug_padding_<span class="subst">&#123;module_i&#125;</span>"</span>, nn.ZeroPad2d((<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - <span class="number">1</span>) // <span class="number">2</span>))</span><br><span class="line">            modules.add_module(<span class="string">f"maxpool_<span class="subst">&#123;module_i&#125;</span>"</span>, maxpool)</span><br><span class="line"> </span><br><span class="line">            <span class="string">'''</span></span><br><span class="line"><span class="string">            upsampling layer</span></span><br><span class="line"><span class="string">            没有使用 Bilinear2dUpsampling</span></span><br><span class="line"><span class="string">            实际使用的为最近邻插值</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"upsample"</span>:</span><br><span class="line">            upsample = Upsample(scale_factor=int(module_def[<span class="string">"stride"</span>]), mode=<span class="string">"nearest"</span>)</span><br><span class="line">            <span class="comment">#这个stride在cfg中就是2，所以下面的scale_factor写2或者stride是等价的</span></span><br><span class="line">            modules.add_module(<span class="string">f"upsample_<span class="subst">&#123;module_i&#125;</span>"</span>, upsample)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># route layer -&gt; Empty layer</span></span><br><span class="line">        <span class="comment"># route层的作用：当layer取值为正时，输出这个正数对应的层的特征，如果layer取值为负数，输出route层向后退layer层对应层的特征</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"route"</span>:</span><br><span class="line">            layers = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            filters = sum([output_filters[<span class="number">1</span>:][i] <span class="keyword">for</span> i <span class="keyword">in</span> layers])</span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("------------------------------------")</span></span><br><span class="line"><span class="string">            print("layers:  \n",layers)</span></span><br><span class="line"><span class="string">            print("output_filters:\n",output_filters)</span></span><br><span class="line"><span class="string">            print("output_filters[1:][i] :\n",[output_filters[1:][i] for i in layers])</span></span><br><span class="line"><span class="string">            print("output_filters[1:]:\n",output_filters[1:])</span></span><br><span class="line"><span class="string">            print("output_filters[1:][1]:\n",output_filters[1:][1])</span></span><br><span class="line"><span class="string">            print("output_filters[1:][3]:\n",output_filters[1:][3])</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            modules.add_module(<span class="string">f"route_<span class="subst">&#123;module_i&#125;</span>"</span>, EmptyLayer())</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#shortcut corresponds to skip connection</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">            filters = output_filters[<span class="number">1</span>:][int(module_def[<span class="string">"from"</span>])]</span><br><span class="line">            <span class="comment">#使用空的层，因为它还要执行一个非常简单的操作（加）。没必要更新 filters 变量,因为它只是将前一层的特征图添加到后面的层上而已。</span></span><br><span class="line">            modules.add_module(<span class="string">f"shortcut_<span class="subst">&#123;module_i&#125;</span>"</span>, EmptyLayer())</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#Yolo is the detection layer</span></span><br><span class="line">        <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">            anchor_idxs = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"mask"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            <span class="comment"># Extract anchors</span></span><br><span class="line">            <span class="comment">#print("----------------------------------")</span></span><br><span class="line">            <span class="comment">#print("anchor_idxs\n:",anchor_idxs)</span></span><br><span class="line">            anchors = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"anchors"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            <span class="comment">#print("1. anchors \n:",anchors)</span></span><br><span class="line">            anchors = [(anchors[i], anchors[i + <span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(anchors), <span class="number">2</span>)]</span><br><span class="line">            <span class="comment">#print("2. anchors \n:",anchors)</span></span><br><span class="line">            anchors = [anchors[i] <span class="keyword">for</span> i <span class="keyword">in</span> anchor_idxs]</span><br><span class="line">            <span class="comment">#print("3. anchors \n:",anchors)</span></span><br><span class="line">            num_classes = int(module_def[<span class="string">"classes"</span>])</span><br><span class="line">            img_size = int(hyperparams[<span class="string">"height"</span>])</span><br><span class="line">            <span class="comment"># Define detection layer</span></span><br><span class="line">            <span class="comment"># 锚点,检测,位置回归,分类，这个类会在后面分析</span></span><br><span class="line">            yolo_layer = YOLOLayer(anchors, num_classes, img_size)</span><br><span class="line">            modules.add_module(<span class="string">f"yolo_<span class="subst">&#123;module_i&#125;</span>"</span>, yolo_layer)</span><br><span class="line">        <span class="comment"># Register module list and number of output filters</span></span><br><span class="line">        module_list.append(modules)</span><br><span class="line">        output_filters.append(filters)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> hyperparams, module_list</span><br></pre></td></tr></table></figure><p>create_module()传入配置文件中网络结构的定义的属性，根据列表会生成相应的网络结构，我们使用的配置文件定义了6中不同的type，convolutional、maxpool、upsample、route、shortcut、yolo层。</p><p>==convolutional层==构建方法很常规：设置filter尺寸、数量，添加batch normalize层（在.cfg文件中batch_normalize=1），以及pad层，使用leaky激活函数。</p><p>==maxpool层==，不过在YOLOv3中没有使用最大池化来进行下采样，是使用的3*3的卷积核，步长=2的卷积操作进行下采样，一共5次，下采样2^5=32倍数。<br><img src="https://img-blog.csdnimg.cn/20200922165908202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>==upsample层==，上采样层。<br>==route层==，这层十分重要。这层的作用相当于把前面的特征图进行相融合。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[route]</span><br><span class="line">layers = -4      <span class="comment"># 只有一个值，一个路径</span></span><br><span class="line"> </span><br><span class="line">[route]</span><br><span class="line">layers = -1, 61  <span class="comment"># 两个值，两个路径，两个特征图进行特征融合</span></span><br></pre></td></tr></table></figure><p>==shortcut层==，直连层，借鉴于ResNet网络。关于ResNet网络更多细节可以查看<a href="https://cloud.tencent.com/developer/article/1148375" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1148375</a>和<a href="https://blog.csdn.net/u014665013/article/details/81985082" target="_blank" rel="noopener">https://blog.csdn.net/u014665013/article/details/81985082</a><br>YOLOv3完整的结构有100+层，所以采用直连的方式来优化网络结构，能使网络更好的训练、更快的收敛。值得注意的是，YOLOv3的shortcut层是把网络的值进行叠加，没有改变特征图的大小，所以仔细会发现在shortcut层的前后，输入输出大小没变。<br> <img src="https://img-blog.csdnimg.cn/20200922171331565.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200922171300997.png#pic_center" alt="在这里插入图片描述"><br>==yolo层==（重点！）<br>仔细看上图的五次采样，会发现有三个Scale，分别是Scale1（下采样8倍）,Scale2（下采样16倍），Scale3（下采样2^5=32倍），此时网络默认的尺寸是416<em>416，对应的feature map为52</em>52，26<em>26，13</em>13。这里借用一幅图：<br><a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a><br><img src="https://img-blog.csdnimg.cn/20200922172118534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里是YOLOv3的多尺度检测的思想的体现，使用3种尺度，是为了加强对小目标的检测，这个应该是借鉴SSD的思想。比较大的特征图来检测相对较小的目标，而小的特征图负责检测大目标。<br>在有多尺度的概念下，使用k-means得到9个先验框的尺寸（416<em>416的尺寸下）。<br>*</em>解析yolo层代码**（加入代码，将每一层的参数打印出来观察）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">            anchor_idxs = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"mask"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            <span class="comment"># Extract anchors</span></span><br><span class="line">            print(<span class="string">"----------------------------------"</span>)</span><br><span class="line">            print(<span class="string">"anchor_idxs\n:"</span>,anchor_idxs)</span><br><span class="line">            anchors = [int(x) <span class="keyword">for</span> x <span class="keyword">in</span> module_def[<span class="string">"anchors"</span>].split(<span class="string">","</span>)]</span><br><span class="line">            print(<span class="string">"1. anchors \n:"</span>,anchors)</span><br><span class="line">            anchors = [(anchors[i], anchors[i + <span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(anchors), <span class="number">2</span>)]</span><br><span class="line">            print(<span class="string">"2. anchors \n:"</span>,anchors)</span><br><span class="line">            anchors = [anchors[i] <span class="keyword">for</span> i <span class="keyword">in</span> anchor_idxs]</span><br><span class="line">            print(<span class="string">"3. anchors \n:"</span>,anchors)</span><br><span class="line">            num_classes = int(module_def[<span class="string">"classes"</span>])</span><br><span class="line">            img_size = int(hyperparams[<span class="string">"height"</span>])</span><br><span class="line">            <span class="comment"># Define detection layer</span></span><br><span class="line">            yolo_layer = YOLOLayer(anchors, num_classes, img_size)</span><br><span class="line">            modules.add_module(<span class="string">f"yolo_<span class="subst">&#123;module_i&#125;</span>"</span>, yolo_layer)</span><br></pre></td></tr></table></figure><p>可以看到输出：<br><img src="https://img-blog.csdnimg.cn/20200922172805375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>可以看到yolo层搭建了三次，第一个yolo层是下采样2^5=32倍，特征图尺寸是13*13（默认输入416 * 416，下同）。这层选择mask的ID是6，7，8，对应的anchor box尺寸是（116， 90）、（156， 198）、（373， 326）。这对应了上面所说的，小的特征图检测大目标，所以使用的anchor box最大。</p><p>至此，Darknet(YOLOv3)模型基本加载完毕，接下来就是，加载权重.weights文件，进行预测。</p><h4 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h4><h5 id="获取检测框"><a href="#获取检测框" class="headerlink" title="获取检测框"></a>获取检测框</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查找weights_path路径下的.weights的文件</span></span><br><span class="line">    <span class="keyword">if</span> opt.weights_path.endswith(<span class="string">".weights"</span>):</span><br><span class="line">        <span class="comment"># Load darknet weights</span></span><br><span class="line">        model.load_darknet_weights(opt.weights_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Load checkpoint weights</span></span><br><span class="line">        model.load_state_dict(torch.load(opt.weights_path))</span><br><span class="line">    <span class="comment"># model.eval()，让model变成测试模式，这主要是对dropout和batch normalization的操作在训练和测试的时候是不一样的</span></span><br><span class="line">    model.eval()  <span class="comment"># Set in evaluation mode</span></span><br><span class="line"> </span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        ImageFolder(opt.image_folder, img_size=opt.img_size),</span><br><span class="line">        batch_size=opt.batch_size,</span><br><span class="line">        shuffle=<span class="literal">False</span>,</span><br><span class="line">        num_workers=opt.n_cpu,</span><br><span class="line">    )</span><br><span class="line"> </span><br><span class="line">    classes = load_classes(opt.class_path)  <span class="comment"># Extracts class labels from file</span></span><br><span class="line"> </span><br><span class="line">    Tensor = torch.cuda.FloatTensor <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line"> </span><br><span class="line">    imgs = []  <span class="comment"># Stores image paths</span></span><br><span class="line">    img_detections = []  <span class="comment"># Stores detections for each image index</span></span><br><span class="line"> </span><br><span class="line">    print(<span class="string">"\nPerforming object detection:"</span>)</span><br><span class="line">    <span class="comment">#返回当前时间的时间戳</span></span><br><span class="line">    prev_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> batch_i, (img_paths, input_imgs) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        <span class="comment"># Configure input</span></span><br><span class="line">        input_imgs = Variable(input_imgs.type(Tensor))</span><br><span class="line">        <span class="comment">#print("img_paths:\n",img_paths)</span></span><br><span class="line">        <span class="comment"># Get detections</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment">#52*52+26*26+13*13）*3=10647</span></span><br><span class="line">            <span class="comment"># 5 + 80 =85</span></span><br><span class="line">            <span class="comment"># detections : 10647*85</span></span><br><span class="line">            detections = model(input_imgs)            </span><br><span class="line">            <span class="comment">#非极大值抑制</span></span><br><span class="line">            detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)</span><br><span class="line">            <span class="comment">#print("detections:\n",detections)</span></span><br><span class="line">        <span class="comment"># Log progress</span></span><br><span class="line">        current_time = time.time()</span><br><span class="line">        <span class="comment">#timedelta代表两个datetime之间的时间差</span></span><br><span class="line">        inference_time = datetime.timedelta(seconds=current_time - prev_time)</span><br><span class="line">        prev_time = current_time</span><br><span class="line">        print(<span class="string">"\t+ Batch %d, Inference Time: %s"</span> % (batch_i, inference_time))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Save image and detections</span></span><br><span class="line">        <span class="comment">#extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。</span></span><br><span class="line">        imgs.extend(img_paths)</span><br><span class="line">        img_detections.extend(detections)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Bounding-box colors</span></span><br><span class="line">    cmap = plt.get_cmap(<span class="string">"tab20b"</span>)</span><br><span class="line">    colors = [cmap(i) <span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">20</span>)]</span><br><span class="line"> </span><br><span class="line">    print(<span class="string">"\nSaving images:"</span>)</span><br><span class="line">    <span class="comment"># Iterate through images and save plot of detections</span></span><br><span class="line">    <span class="keyword">for</span> img_i, (path, detections) <span class="keyword">in</span> enumerate(zip(imgs, img_detections)):</span><br><span class="line"> </span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"(%d) Image: '%s'"</span> % (img_i, path))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Create plot</span></span><br><span class="line">        img = np.array(Image.open(path))</span><br><span class="line">        plt.figure()</span><br><span class="line">        fig, ax = plt.subplots(<span class="number">1</span>)</span><br><span class="line">        ax.imshow(img)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Draw bounding boxes and labels of detections</span></span><br><span class="line">        <span class="keyword">if</span> detections <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Rescale boxes to original image</span></span><br><span class="line">            detections  = rescale_boxes(detections, opt.img_size, img.shape[:<span class="number">2</span>])</span><br><span class="line">            unique_labels = detections[:, <span class="number">-1</span>].cpu().unique()</span><br><span class="line">            n_cls_preds = len(unique_labels)</span><br><span class="line">            bbox_colors = random.sample(colors, n_cls_preds)</span><br><span class="line">            <span class="keyword">for</span> x1, y1, x2, y2, conf, cls_conf, cls_pred <span class="keyword">in</span> detections:</span><br><span class="line"> </span><br><span class="line">                print(<span class="string">"\t+ Label: %s, Conf: %.5f"</span> % (classes[int(cls_pred)], cls_conf.item()))</span><br><span class="line"> </span><br><span class="line">                box_w = x2 - x1</span><br><span class="line">                box_h = y2 - y1</span><br><span class="line"> </span><br><span class="line">                color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[<span class="number">0</span>])]</span><br><span class="line">                <span class="comment"># Create a Rectangle patch</span></span><br><span class="line">                bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=<span class="number">2</span>, edgecolor=color, facecolor=<span class="string">"none"</span>)</span><br><span class="line">                <span class="comment"># Add the bbox to the plot</span></span><br><span class="line">                ax.add_patch(bbox)</span><br><span class="line">                <span class="comment"># Add label</span></span><br><span class="line">                plt.text(</span><br><span class="line">                    x1,</span><br><span class="line">                    y1,</span><br><span class="line">                    s=classes[int(cls_pred)],</span><br><span class="line">                    color=<span class="string">"white"</span>,</span><br><span class="line">                    verticalalignment=<span class="string">"top"</span>,</span><br><span class="line">                    bbox=&#123;<span class="string">"color"</span>: color, <span class="string">"pad"</span>: <span class="number">0</span>&#125;,</span><br><span class="line">                )</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Save generated image with detections</span></span><br><span class="line">        plt.axis(<span class="string">"off"</span>)</span><br><span class="line">        plt.gca().xaxis.set_major_locator(NullLocator())</span><br><span class="line">        plt.gca().yaxis.set_major_locator(NullLocator())</span><br><span class="line">        filename = path.split(<span class="string">"/"</span>)[<span class="number">-1</span>].split(<span class="string">"."</span>)[<span class="number">0</span>]</span><br><span class="line">        plt.savefig(<span class="string">f"output/<span class="subst">&#123;filename&#125;</span>.jpg"</span>, bbox_inches=<span class="string">"tight"</span>, pad_inches=<span class="number">0.0</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.close()</span><br></pre></td></tr></table></figure><p><code>model.load_darknet_weights(opt.weights_path)</code>,通过这个语句加载yolov3.weights。加载完.weights文件之后，便开始加载测试图片数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(</span><br><span class="line">        ImageFolder(opt.image_folder, img_size=opt.img_size),</span><br><span class="line">        batch_size=opt.batch_size,</span><br><span class="line">        shuffle=<span class="literal">False</span>,</span><br><span class="line">        num_workers=opt.n_cpu,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>ImageFolder是遍历文件夹下的测试图片，完整定义如下。ImageFolder中的<strong>getitem</strong>()函数会把图像归一化处理成img_size(默认416)大小的图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageFolder</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, folder_path, img_size=<span class="number">416</span>)</span>:</span></span><br><span class="line">        <span class="comment">#sorted(iterable[, cmp[, key[, reverse]]])</span></span><br><span class="line">        <span class="comment">#sorted() 函数对所有可迭代的对象进行排序操作</span></span><br><span class="line">        <span class="comment">##获取指定目录下的所有文件</span></span><br><span class="line">        self.files = sorted(glob.glob(<span class="string">"%s/*.*"</span> % folder_path))</span><br><span class="line">        self.img_size = img_size</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img_path = self.files[index % len(self.files)]</span><br><span class="line">        <span class="comment"># Extract image as PyTorch tensor</span></span><br><span class="line">        img = transforms.ToTensor()(Image.open(img_path))</span><br><span class="line">        <span class="comment"># Pad to square resolution 变成方形</span></span><br><span class="line">        img, _ = pad_to_square(img, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Resize</span></span><br><span class="line">        img = resize(img, self.img_size)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> img_path, img</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.files)</span><br></pre></td></tr></table></figure><p>回到==detect.py==中，<code>detections = model(input_imgs)</code>，把图像放进模型中，得到检测结果。这里是通过Darknet的forward()函数得到检测结果。其完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, targets=None)</span>:</span></span><br><span class="line">        img_dim = x.shape[<span class="number">2</span>]</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        layer_outputs, yolo_outputs = [], []</span><br><span class="line">        <span class="keyword">for</span> i, (module_def, module) <span class="keyword">in</span> enumerate(zip(self.module_defs, self.module_list)):</span><br><span class="line">            <span class="keyword">if</span> module_def[<span class="string">"type"</span>] <span class="keyword">in</span> [<span class="string">"convolutional"</span>, <span class="string">"upsample"</span>, <span class="string">"maxpool"</span>]:</span><br><span class="line">                x = module(x)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"route"</span>:</span><br><span class="line">                x = torch.cat([layer_outputs[int(layer_i)] <span class="keyword">for</span> layer_i <span class="keyword">in</span> module_def[<span class="string">"layers"</span>].split(<span class="string">","</span>)], <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">                layer_i = int(module_def[<span class="string">"from"</span>])</span><br><span class="line">                x = layer_outputs[<span class="number">-1</span>] + layer_outputs[layer_i]</span><br><span class="line">            <span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"yolo"</span>:</span><br><span class="line">                x, layer_loss = module[<span class="number">0</span>](x, targets, img_dim)</span><br><span class="line">                loss += layer_loss</span><br><span class="line">                yolo_outputs.append(x)</span><br><span class="line">            layer_outputs.append(x)</span><br><span class="line">        yolo_outputs = to_cpu(torch.cat(yolo_outputs, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> yolo_outputs <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> (loss, yolo_outputs)</span><br></pre></td></tr></table></figure><p>通过遍历==self.module_defs==,与==self.module_list==，来完成网络的前向传播。<br>如果是”<strong>convolutional</strong>“, “<strong>upsample</strong>“, “<strong>maxpool</strong>“层，则直接使用前向传播即可。<br>如果是<strong>route</strong>层，则使用torch.cat()完成特征图的融合（拼接）。<br>比如，我前面用来测试的一张图：</p><p><img src="https://img-blog.csdnimg.cn/20200922174400667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200922174423704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这张图的尺寸为3 * 768 * 576，我们看看放进模型进行测试的时候，其shape是如何变化的。图像会根据cfg归一化成416 * 416.<br><img src="https://img-blog.csdnimg.cn/20200922174534924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>接下来查看一下route层对应的ID以及shape：<br><img src="https://img-blog.csdnimg.cn/20200922174656406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>该模型的每一层的输出通过<strong>layer_outputs.append(x)</strong>，保存在<strong>layer_outputs</strong>列表中，本次结构完全符合本文前面所论述的部分。如果layer只有一个值，那么该<strong>route</strong>层的输出就是该层。如果layer有两个值，则route层输出是对应两个层的特征图的融合。</p><p>如果是<strong>shortcut</strong>层，则特别清晰，直接对应两层相叠加即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> module_def[<span class="string">"type"</span>] == <span class="string">"shortcut"</span>:</span><br><span class="line">               layer_i = int(module_def[<span class="string">"from"</span>])</span><br><span class="line">               x = layer_outputs[<span class="number">-1</span>] + layer_outputs[layer_i]</span><br></pre></td></tr></table></figure><p>如果是yolo层，yolo层有三个，分别对应的特征图大小为13<em>13，26</em>26，52*52。每一个特征图的每一个cell会预测3个bounding boxes。每一个bounding box会预测预测三类值：</p><ol><li>每个框的位置（4个值，中心坐标tx和ty，，框的高度bh和宽度bw），</li><li>一个objectness prediction ，一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量</li><li>N个类别，COCO有80类，VOC有20类。</li></ol><p>所以不难理解，在这里是COCO数据集，在13<em>13的特征图中，一共有*</em>13 * 13 * 3=507**个bounding boxes，每一个bounding box预测（4+1+80=85）个值，用张量的形式表示为[1, 507, 85]，那个1表示的是batch size。同理，其余张量的shape不难理解。</p><p><img src="https://img-blog.csdnimg.cn/2020092217515347.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>那么如何得到这个张量呢</strong>，主要要了解yolo层的==forward()== 和 ==compute_grid_offstes==，其完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YOLOLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Detection layer"""</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, anchors, num_classes, img_dim=<span class="number">416</span>)</span>:</span></span><br><span class="line">        super(YOLOLayer, self).__init__()</span><br><span class="line">        self.anchors = anchors</span><br><span class="line">        self.num_anchors = len(anchors)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.ignore_thres = <span class="number">0.5</span></span><br><span class="line">        self.mse_loss = nn.MSELoss()</span><br><span class="line">        self.bce_loss = nn.BCELoss()</span><br><span class="line">        self.obj_scale = <span class="number">1</span></span><br><span class="line">        self.noobj_scale = <span class="number">100</span></span><br><span class="line">        self.metrics = &#123;&#125;</span><br><span class="line">        self.img_dim = img_dim</span><br><span class="line">        self.grid_size = <span class="number">0</span>  <span class="comment"># grid size</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_grid_offsets</span><span class="params">(self, grid_size, cuda=True)</span>:</span></span><br><span class="line">        self.grid_size = grid_size</span><br><span class="line">        g = self.grid_size</span><br><span class="line">        FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">        self.stride = self.img_dim / self.grid_size</span><br><span class="line">        <span class="comment"># Calculate offsets for each grid</span></span><br><span class="line">        <span class="comment">#repeat 相当于一个broadcasting的机制repeat(*sizes)</span></span><br><span class="line">        <span class="comment">#沿着指定的维度重复tensor。不同与expand()，本函数复制的是tensor中的数据。</span></span><br><span class="line">        self.grid_x = torch.arange(g).repeat(g, <span class="number">1</span>).view([<span class="number">1</span>, <span class="number">1</span>, g, g]).type(FloatTensor)</span><br><span class="line">        self.grid_y = torch.arange(g).repeat(g, <span class="number">1</span>).t().view([<span class="number">1</span>, <span class="number">1</span>, g, g]).type(FloatTensor)</span><br><span class="line">        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) <span class="keyword">for</span> a_w, a_h <span class="keyword">in</span> self.anchors])</span><br><span class="line">        self.anchor_w = self.scaled_anchors[:, <span class="number">0</span>:<span class="number">1</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.anchor_h = self.scaled_anchors[:, <span class="number">1</span>:<span class="number">2</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, targets=None, img_dim=None)</span>:</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Tensors for cuda support</span></span><br><span class="line">        FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">        LongTensor = torch.cuda.LongTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.LongTensor</span><br><span class="line">        ByteTensor = torch.cuda.ByteTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.ByteTensor</span><br><span class="line"> </span><br><span class="line">        self.img_dim = img_dim</span><br><span class="line">        num_samples = x.size(<span class="number">0</span>)</span><br><span class="line">        grid_size = x.size(<span class="number">2</span>)</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        所以在输入为416*416时，每个cell的三个anchor box为(116 ,90);</span></span><br><span class="line"><span class="string">        (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为</span></span><br><span class="line"><span class="string">        (30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，</span></span><br><span class="line"><span class="string">        因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416*416时，</span></span><br><span class="line"><span class="string">        实际总共有（52*52+26*26+13*13）*3=10647个proposal box。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        prediction = (</span><br><span class="line">            x.view(num_samples, self.num_anchors, self.num_classes + <span class="number">5</span>, grid_size, grid_size)</span><br><span class="line">            .permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">        )</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("----------------------------------")</span></span><br><span class="line"><span class="string">        print("num_samples:\n",num_samples)</span></span><br><span class="line"><span class="string">        print("self.num_anchors:\n",self.num_anchors)</span></span><br><span class="line"><span class="string">        print("self.grid_size:\n",self.grid_size)</span></span><br><span class="line"><span class="string">        print("grid_size:\n",grid_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#print("x:\n",x)</span></span><br><span class="line">        <span class="comment">#print("prediction:\n",prediction)</span></span><br><span class="line">        <span class="comment"># Get outputs</span></span><br><span class="line">        <span class="comment">#print("prediction\n:",prediction)</span></span><br><span class="line">        <span class="comment">#print("prediction.shape:\n",prediction.shape)</span></span><br><span class="line">        x = torch.sigmoid(prediction[..., <span class="number">0</span>])  <span class="comment"># Center x</span></span><br><span class="line">        </span><br><span class="line">        y = torch.sigmoid(prediction[..., <span class="number">1</span>])  <span class="comment"># Center y</span></span><br><span class="line">        w = prediction[..., <span class="number">2</span>]  <span class="comment"># Width</span></span><br><span class="line">        h = prediction[..., <span class="number">3</span>]  <span class="comment"># Height</span></span><br><span class="line">        pred_conf = torch.sigmoid(prediction[..., <span class="number">4</span>])  <span class="comment"># Conf</span></span><br><span class="line">        pred_cls = torch.sigmoid(prediction[..., <span class="number">5</span>:])  <span class="comment"># Cls pred.</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("anchors \n:",self.anchors)</span></span><br><span class="line"><span class="string">        print("x.shape\n:",x.shape)</span></span><br><span class="line"><span class="string">        print("y.shape\n:",y.shape)</span></span><br><span class="line"><span class="string">        print("w.shape\n:",w.shape)</span></span><br><span class="line"><span class="string">        print("h.shape\n:",h.shape)</span></span><br><span class="line"><span class="string">        print("pred_conf.shape\n:",pred_conf.shape)</span></span><br><span class="line"><span class="string">        print("pred_cls.shape\n:",pred_cls.shape)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># If grid size does not match current we compute new offsets</span></span><br><span class="line">        <span class="keyword">if</span> grid_size != self.grid_size:</span><br><span class="line">            print(<span class="string">"··················different··················"</span>)</span><br><span class="line">            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Add offset and scale with anchors</span></span><br><span class="line">        pred_boxes = FloatTensor(prediction[..., :<span class="number">4</span>].shape)</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("prediction[..., :4].shape:\n",prediction[..., :4].shape)</span></span><br><span class="line"><span class="string">        print("self.grid_x:\n",self.grid_x)</span></span><br><span class="line"><span class="string">        print("self.grid_y:\n",self.grid_y)</span></span><br><span class="line"><span class="string">        print("self.anchor_w:\n",self.anchor_w)</span></span><br><span class="line"><span class="string">        print("self.anchor_h:\n",self.anchor_h)</span></span><br><span class="line"><span class="string">        print("self.anchors:\n",self.anchors)</span></span><br><span class="line"><span class="string">        print("self.stride:\n",self.stride)  </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pred_boxes[..., <span class="number">0</span>] = x.data + self.grid_x</span><br><span class="line">        pred_boxes[..., <span class="number">1</span>] = y.data + self.grid_y</span><br><span class="line">        pred_boxes[..., <span class="number">2</span>] = torch.exp(w.data) * self.anchor_w</span><br><span class="line">        pred_boxes[..., <span class="number">3</span>] = torch.exp(h.data) * self.anchor_h</span><br><span class="line">        <span class="comment">#torch.cat 按最后一维拼接</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("pred_boxes.view(num_samples, -1, 4).shape:\n",pred_boxes.view(num_samples, -1, 4).shape)</span></span><br><span class="line"><span class="string">        print("pred_conf.view(num_samples, -1, 1).shape:\n",pred_conf.view(num_samples, -1, 1).shape)</span></span><br><span class="line"><span class="string">        print("pred_cls.view(num_samples, -1, self.num_classes).shape:\n",pred_cls.view(num_samples, -1, self.num_classes).shape)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        output = torch.cat(</span><br><span class="line">            (</span><br><span class="line">                pred_boxes.view(num_samples, <span class="number">-1</span>, <span class="number">4</span>) * self.stride,</span><br><span class="line">                pred_conf.view(num_samples, <span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">                pred_cls.view(num_samples, <span class="number">-1</span>, self.num_classes),</span><br><span class="line">            ),</span><br><span class="line">            <span class="number">-1</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#print("output.shape:\n",output.shape)</span></span><br><span class="line">        <span class="comment">#print("targets:\n",targets)</span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output, <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(</span><br><span class="line">                pred_boxes=pred_boxes,</span><br><span class="line">                pred_cls=pred_cls,</span><br><span class="line">                target=targets,</span><br><span class="line">                anchors=self.scaled_anchors,</span><br><span class="line">                ignore_thres=self.ignore_thres,</span><br><span class="line">            )</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Loss : Mask outputs to ignore non-existing objects (except with conf. loss)</span></span><br><span class="line">            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])</span><br><span class="line">            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])</span><br><span class="line">            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])</span><br><span class="line">            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])</span><br><span class="line">            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])</span><br><span class="line">            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj</span><br><span class="line">            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])</span><br><span class="line">            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Metrics</span></span><br><span class="line">            cls_acc = <span class="number">100</span> * class_mask[obj_mask].mean()</span><br><span class="line">            conf_obj = pred_conf[obj_mask].mean()</span><br><span class="line">            conf_noobj = pred_conf[noobj_mask].mean()</span><br><span class="line">            conf50 = (pred_conf &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou50 = (iou_scores &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou75 = (iou_scores &gt; <span class="number">0.75</span>).float()</span><br><span class="line">            detected_mask = conf50 * class_mask * tconf</span><br><span class="line">            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">            self.metrics = &#123;</span><br><span class="line">                <span class="string">"loss"</span>: to_cpu(total_loss).item(),</span><br><span class="line">                <span class="string">"x"</span>: to_cpu(loss_x).item(),</span><br><span class="line">                <span class="string">"y"</span>: to_cpu(loss_y).item(),</span><br><span class="line">                <span class="string">"w"</span>: to_cpu(loss_w).item(),</span><br><span class="line">                <span class="string">"h"</span>: to_cpu(loss_h).item(),</span><br><span class="line">                <span class="string">"conf"</span>: to_cpu(loss_conf).item(),</span><br><span class="line">                <span class="string">"cls"</span>: to_cpu(loss_cls).item(),</span><br><span class="line">                <span class="string">"cls_acc"</span>: to_cpu(cls_acc).item(),</span><br><span class="line">                <span class="string">"recall50"</span>: to_cpu(recall50).item(),</span><br><span class="line">                <span class="string">"recall75"</span>: to_cpu(recall75).item(),</span><br><span class="line">                <span class="string">"precision"</span>: to_cpu(precision).item(),</span><br><span class="line">                <span class="string">"conf_obj"</span>: to_cpu(conf_obj).item(),</span><br><span class="line">                <span class="string">"conf_noobj"</span>: to_cpu(conf_noobj).item(),</span><br><span class="line">                <span class="string">"grid_size"</span>: grid_size,</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> output, total_loss</span><br></pre></td></tr></table></figure><p><strong>num_samples</strong>是每一批有多少张图片，<strong>grid_size</strong>是特征图的大小。<br><img src="https://img-blog.csdnimg.cn/20200922175739780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>使用<strong>torch.view</strong>,改变输入<strong>yolo</strong>层的张量结构（shape），以<strong>prediction</strong>命名的张量进行预测处理。<br><img src="https://img-blog.csdnimg.cn/20200922175934750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>接下来是便是对边框进行预测，具体细节可以参考：<a href="https://blog.csdn.net/qq_34199326/article/details/84109828" target="_blank" rel="noopener">https://blog.csdn.net/qq_34199326/article/details/84109828</a>。x，y坐标都是使用了sigmoid函数进行处理，置信度和类别概率使用同样的方法处理。</p><p>论文中的边界框预测：<br><img src="https://img-blog.csdnimg.cn/20200922180246468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of ﬁlter application using a sigmoid function. This ﬁgure blatantly self-plagiarized from.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.sigmoid(prediction[..., <span class="number">0</span>])  <span class="comment"># Center x</span></span><br><span class="line">        y = torch.sigmoid(prediction[..., <span class="number">1</span>])  <span class="comment"># Center y</span></span><br><span class="line">        w = prediction[..., <span class="number">2</span>]  <span class="comment"># Width</span></span><br><span class="line">        h = prediction[..., <span class="number">3</span>]  <span class="comment"># Height</span></span><br><span class="line">        pred_conf = torch.sigmoid(prediction[..., <span class="number">4</span>])  <span class="comment"># Conf</span></span><br><span class="line">        pred_cls = torch.sigmoid(prediction[..., <span class="number">5</span>:])  <span class="comment"># Cls pred.</span></span><br></pre></td></tr></table></figure><p>在3个尺度下，分别进行预测坐标、置信度、类别概率。<br><img src="https://img-blog.csdnimg.cn/20200922180614936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>从图中我们发现<strong>grid_size</strong>和<strong>self.grid_size</strong>是不相等的，所以需要进行计算偏移，即<strong>compute_grid_offsets</strong>。完整代码在==YOLOLayer==中。</p><p>以gird=13为例。此时特征图是13 * 13，但原图shape尺寸是416 * 416，所以要把416 * 416评价切成13 * 13个方格，需要得到间隔（步距<strong>self.stride</strong>=416/13=32）。相应的并把anchor的尺寸进行缩放，即<strong>116/32=3.6250，90/32=2.8125</strong>。</p><p><img src="https://img-blog.csdnimg.cn/20200922180935263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>前面已经说过每一个小方格（cell），都会预测3个边界框，同样以gird=13为列。第一个小方格（cell），会预测3个边界框，每个边界框都有坐标+置信度+类别概率。所以以下代码中的x.shape=[1, 3, 13, 13],并且与y,w,h的shape一致。<br>同时由于在最后进行拼接，得到输出output 。其<strong>507=13 * 13 * 3</strong>，<strong>2028=26 * 26 * 3</strong>，<strong>8112=52 * 52 * 3</strong>不难理解。<br><img src="https://img-blog.csdnimg.cn/20200922181448190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h5 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h5><p>代码涉及部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># detections : 10647*85</span></span><br><span class="line">            detections = model(input_imgs)            </span><br><span class="line">            <span class="comment">#非极大值抑制</span></span><br><span class="line">            detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)</span><br></pre></td></tr></table></figure><p>在获取检测框之后，需要使用非极大值抑制来筛选框。即 <code>detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)</code></p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">non_max_suppression</span><span class="params">(prediction, conf_thres=<span class="number">0.5</span>, nms_thres=<span class="number">0.4</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes detections with lower object confidence score than 'conf_thres' and performs</span></span><br><span class="line"><span class="string">    Non-Maximum Suppression to further filter detections.</span></span><br><span class="line"><span class="string">    Returns detections with shape:</span></span><br><span class="line"><span class="string">        (x1, y1, x2, y2, object_conf, class_score, class_pred)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># From (center x, center y, width, height) to (x1, y1, x2, y2)</span></span><br><span class="line">    prediction[..., :<span class="number">4</span>] = xywh2xyxy(prediction[..., :<span class="number">4</span>])</span><br><span class="line">    output = [<span class="literal">None</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(prediction))]</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> image_i, image_pred <span class="keyword">in</span> enumerate(prediction):</span><br><span class="line">        <span class="comment"># Filter out confidence scores below threshold</span></span><br><span class="line">        print(<span class="string">"------------------------------"</span>)</span><br><span class="line">        <span class="comment">#print("image_i:\n",image_i)</span></span><br><span class="line">        print(<span class="string">"image_pred.shape:\n"</span>,image_pred.shape)</span><br><span class="line">        image_pred = image_pred[image_pred[:, <span class="number">4</span>] &gt;= conf_thres]<span class="comment">#保留大于置信度的边界框</span></span><br><span class="line">        print(<span class="string">"image_pred.size(0)"</span>,image_pred.size(<span class="number">0</span>))</span><br><span class="line">        <span class="comment"># If none are remaining =&gt; process next image</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_pred.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># Object confidence times class confidence</span></span><br><span class="line">        <span class="comment"># .max(1) 返回每行tensor的最大值  .max(1)[0]具体的最大值 .max(1)[1] 最大值对应的索引</span></span><br><span class="line">        score = image_pred[:, <span class="number">4</span>] * image_pred[:, <span class="number">5</span>:].max(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("image_pred[:, 5:]:\n",image_pred[:, 5:])</span></span><br><span class="line"><span class="string">        print("image_pred[:, 5:].max(1):\n",image_pred[:, 5:].max(1))</span></span><br><span class="line"><span class="string">        print("image_pred[:, 5:].max(1)[0]:\n",image_pred[:, 5:].max(1)[0])</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Sort by it</span></span><br><span class="line">        <span class="comment"># 完成从大到小排序 </span></span><br><span class="line">        image_pred = image_pred[(-score).argsort()]</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        print("score:\n",score)</span></span><br><span class="line"><span class="string">        print("(-score).argsort():\n",(-score).argsort())</span></span><br><span class="line"><span class="string">        print("image_pred:\n",image_pred)\</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。</span></span><br><span class="line">        <span class="comment">#否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，</span></span><br><span class="line">        <span class="comment">#最终输出张量会比输入张量少一个维度。</span></span><br><span class="line">        class_confs, class_preds = image_pred[:, <span class="number">5</span>:].max(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#print("image_pred[:, 5:].max(1, keepdim=True):\n",image_pred[:, 5:].max(1, keepdim=True))        </span></span><br><span class="line">        <span class="comment">#print("image_pred[:, 5:].max(1, keepdim=False):\n",image_pred[:, 5:].max(1, keepdim=False))        </span></span><br><span class="line">        detections = torch.cat((image_pred[:, :<span class="number">5</span>], class_confs.float(), class_preds.float()), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Perform non-maximum suppression</span></span><br><span class="line">        <span class="comment">#print("detections.size():\n",detections.size())</span></span><br><span class="line">        <span class="comment">#print("detections.size(0):\n",detections.size(0))    </span></span><br><span class="line">        <span class="comment">#print("image_pred[:, :5]:\n",image_pred[:, :5]) </span></span><br><span class="line">        keep_boxes = []</span><br><span class="line">        <span class="keyword">while</span> detections.size(<span class="number">0</span>):</span><br><span class="line">            <span class="comment">#torch.unsqueeze()这个函数主要是对数据维度进行扩充</span></span><br><span class="line">            large_overlap = bbox_iou(detections[<span class="number">0</span>, :<span class="number">4</span>].unsqueeze(<span class="number">0</span>), detections[:, :<span class="number">4</span>]) &gt; nms_thres</span><br><span class="line">            label_match = detections[<span class="number">0</span>, <span class="number">-1</span>] == detections[:, <span class="number">-1</span>]</span><br><span class="line">            <span class="comment"># Indices of boxes with lower confidence scores, large IOUs and matching labels</span></span><br><span class="line">            invalid = large_overlap &amp; label_match</span><br><span class="line">            weights = detections[invalid, <span class="number">4</span>:<span class="number">5</span>]<span class="comment">#置信度</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("1.detections:\n",detections)</span></span><br><span class="line"><span class="string">            print("large_overlap:\n",large_overlap)</span></span><br><span class="line"><span class="string">            print("detections[0, -1]:\n",detections[0, -1])</span></span><br><span class="line"><span class="string">            print("detections[:, -1]:\n",detections[:, -1])</span></span><br><span class="line"><span class="string">            print("label_match:\n",label_match)</span></span><br><span class="line"><span class="string">            print("invalid:\n",invalid)</span></span><br><span class="line"><span class="string">            print("weights:\n",weights)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment"># Merge overlapping bboxes by order of confidence</span></span><br><span class="line">            detections[<span class="number">0</span>, :<span class="number">4</span>] = (weights * detections[invalid, :<span class="number">4</span>]).sum(<span class="number">0</span>) / weights.sum()</span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            print("detections[invalid, :4]:\n",detections[invalid, :4])</span></span><br><span class="line"><span class="string">            print("weights * detections[invalid, :4]:\n",weights * detections[invalid, :4])</span></span><br><span class="line"><span class="string">            print("detections[invalid, :4].sum(0):\n",detections[invalid, :4].sum(0))</span></span><br><span class="line"><span class="string">            print("weights * detections[invalid, :4].sum(0):\n",weights * detections[invalid, :4].sum(0))</span></span><br><span class="line"><span class="string">            print("2.detections:\n",detections)</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            keep_boxes += [detections[<span class="number">0</span>]]</span><br><span class="line">            detections = detections[~invalid]</span><br><span class="line">            <span class="comment">#print("3.detections:\n",detections)</span></span><br><span class="line">        <span class="keyword">if</span> keep_boxes:</span><br><span class="line">            output[image_i] = torch.stack(keep_boxes)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>非极大值抑制算法可参考：<br><a href="https://www.cnblogs.com/makefile/p/nms.html" target="_blank" rel="noopener">https://www.cnblogs.com/makefile/p/nms.html</a><br><a href="https://www.jianshu.com/p/d452b5615850" target="_blank" rel="noopener">https://www.jianshu.com/p/d452b5615850</a><br>在经过非极大值抑制处理之后，在这里唯一有一点不同的是，这里采取了边界框“融合”的策略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge overlapping bboxes by order of confidence            </span></span><br><span class="line">detections[<span class="number">0</span>, :<span class="number">4</span>] = (weights * detections[invalid, :<span class="number">4</span>]).sum(<span class="number">0</span>) / weights.sum()</span><br></pre></td></tr></table></figure><p>最终可以得到我们的检验结果。</p><h3 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h3><h4 id="训练前准备工作"><a href="#训练前准备工作" class="headerlink" title="训练前准备工作"></a>训练前准备工作</h4><h5 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.logger <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.datasets <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils.parse_config <span class="keyword">import</span> *</span><br><span class="line"><span class="comment">#from test import evaluate</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> terminaltables <span class="keyword">import</span> AsciiTable</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">"--epochs"</span>, type=int, default=<span class="number">100</span>, help=<span class="string">"number of epochs"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--batch_size"</span>, type=int, default=<span class="number">8</span>, help=<span class="string">"size of each image batch"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--gradient_accumulations"</span>, type=int, default=<span class="number">2</span>, help=<span class="string">"number of gradient accums before step"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--model_def"</span>, type=str, default=<span class="string">"config/yolov3_myself.cfg"</span>, help=<span class="string">"path to model definition file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--data_config"</span>, type=str, default=<span class="string">"config/voc_myself.data"</span>, help=<span class="string">"path to data config file"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--pretrained_weights"</span>, type=str, default=<span class="string">"weights/darknet53.conv.74"</span>, help=<span class="string">"if specified starts from checkpoint model"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--n_cpu"</span>, type=int, default=<span class="number">0</span>, help=<span class="string">"number of cpu threads to use during batch generation"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--img_size"</span>, type=int, default=<span class="number">416</span>, help=<span class="string">"size of each image dimension"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--checkpoint_interval"</span>, type=int, default=<span class="number">1</span>, help=<span class="string">"interval between saving model weights"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--evaluation_interval"</span>, type=int, default=<span class="number">1</span>, help=<span class="string">"interval evaluations on validation set"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--compute_map"</span>, default=<span class="literal">False</span>, help=<span class="string">"if True computes mAP every tenth batch"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--multiscale_training"</span>, default=<span class="literal">True</span>, help=<span class="string">"allow for multi-scale training"</span>)</span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line">    print(opt)</span><br><span class="line"> </span><br><span class="line">    logger = Logger(<span class="string">"logs"</span>)</span><br><span class="line"> </span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"> </span><br><span class="line">    os.makedirs(<span class="string">"output"</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    os.makedirs(<span class="string">"checkpoints"</span>, exist_ok=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h5 id="加载网络"><a href="#加载网络" class="headerlink" title="加载网络"></a>加载网络</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get data configuration</span></span><br><span class="line"><span class="comment">#从.cfg文件中解析出路径，包括训练路径、验证路径、训练类别。同时加载Darknet（YOLOv3）模型到model中</span></span><br><span class="line">    data_config = parse_data_config(opt.data_config)</span><br><span class="line">    train_path = data_config[<span class="string">"train"</span>]</span><br><span class="line">    valid_path = data_config[<span class="string">"valid"</span>]</span><br><span class="line">    class_names = load_classes(data_config[<span class="string">"names"</span>])</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Initiate model</span></span><br><span class="line">    <span class="comment">#model.apply(weights_init_normal)**，自定义初始化方式。</span></span><br><span class="line">    model = Darknet(opt.model_def).to(device)</span><br><span class="line">    model.apply(weights_init_normal)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># If specified we start from checkpoint</span></span><br><span class="line">    <span class="keyword">if</span> opt.pretrained_weights:</span><br><span class="line">        <span class="keyword">if</span> opt.pretrained_weights.endswith(<span class="string">".pth"</span>):</span><br><span class="line">            <span class="comment">#通常训练的时候，会加载预训练模型model.load_state_dict(torch.load(opt.pretrained_weights))。</span></span><br><span class="line">            model.load_state_dict(torch.load(opt.pretrained_weights))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model.load_darknet_weights(opt.pretrained_weights)</span><br></pre></td></tr></table></figure><p>从.cfg文件中解析出路径，包括训练路径、验证路径、训练类别。同时加载Darknet（YOLOv3）模型到model中。<code>model.apply(weights_init_normal)</code>，自定义初始化方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init_normal</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">"Conv"</span>) != <span class="number">-1</span>:</span><br><span class="line">        torch.nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">"BatchNorm2d"</span>) != <span class="number">-1</span>:</span><br><span class="line">        torch.nn.init.normal_(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br></pre></td></tr></table></figure><h5 id="放进DataLoader"><a href="#放进DataLoader" class="headerlink" title="放进DataLoader"></a>放进DataLoader</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DataLoader的collate_fn参数，实现自定义的batch输出</span></span><br><span class="line">    <span class="comment">#- shuffle：设置为True的时候，每个世代都会打乱数据集 </span></span><br><span class="line">    <span class="comment">#- collate_fn：如何取样本的，我们可以定义自己的函数来准确地实现想要的功能 </span></span><br><span class="line">    <span class="comment">#- drop_last：告诉如何处理数据集长度除于batch_size余下的数据。True就抛弃，否则保留</span></span><br><span class="line">    dataloader = torch.utils.data.DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=opt.batch_size,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        num_workers=opt.n_cpu,</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        collate_fn=dataset.collate_fn,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#使用优化器</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"> </span><br><span class="line">    metrics = [</span><br><span class="line">        <span class="string">"grid_size"</span>,</span><br><span class="line">        <span class="string">"loss"</span>,</span><br><span class="line">        <span class="string">"x"</span>,</span><br><span class="line">        <span class="string">"y"</span>,</span><br><span class="line">        <span class="string">"w"</span>,</span><br><span class="line">        <span class="string">"h"</span>,</span><br><span class="line">        <span class="string">"conf"</span>,</span><br><span class="line">        <span class="string">"cls"</span>,</span><br><span class="line">        <span class="string">"cls_acc"</span>,</span><br><span class="line">        <span class="string">"recall50"</span>,</span><br><span class="line">        <span class="string">"recall75"</span>,</span><br><span class="line">        <span class="string">"precision"</span>,</span><br><span class="line">        <span class="string">"conf_obj"</span>,</span><br><span class="line">        <span class="string">"conf_noobj"</span>,</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><h4 id="训练并计算loss"><a href="#训练并计算loss" class="headerlink" title="训练并计算loss"></a>训练并计算loss</h4><h5 id="开始迭代"><a href="#开始迭代" class="headerlink" title="开始迭代"></a>开始迭代</h5><p>加载所有的图片，迭代的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        print(<span class="string">"len(dataloader):\n"</span>,len(dataloader))</span><br><span class="line">        <span class="keyword">for</span> batch_i, (_, imgs, targets) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">            batches_done = len(dataloader) * epoch + batch_i</span><br><span class="line">            print(<span class="string">"batch_i:\n"</span>,batch_i)</span><br><span class="line">            print(<span class="string">"imgs.shape:\n"</span>,imgs.shape)</span><br><span class="line">            print(<span class="string">"batches_done:\n"</span>,batches_done)</span><br><span class="line">            imgs = Variable(imgs.to(device))</span><br><span class="line">            targets = Variable(targets.to(device), requires_grad=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">            loss, outputs = model(imgs, targets)</span><br><span class="line">            loss.backward()</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> batches_done % opt.gradient_accumulations:</span><br><span class="line">                <span class="comment"># Accumulates gradient before each step</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad()</span><br></pre></td></tr></table></figure><h5 id="从batch中获取图片，从label中获取标签"><a href="#从batch中获取图片，从label中获取标签" class="headerlink" title="从batch中获取图片，从label中获取标签"></a>从batch中获取图片，从label中获取标签</h5><p><code>for batch_i, (_, imgs, targets) in enumerate(dataloader):</code>，这里主要要参考ListDataset中的<strong>getitem</strong>和DataLoader中的<strong>collate_fn</strong>设置。<br>ListDataset中的<strong>getitem</strong>（部分）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.exists(label_path):</span><br><span class="line">            </span><br><span class="line">            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(<span class="number">-1</span>, <span class="number">5</span>))</span><br><span class="line">            <span class="comment"># Extract coordinates for unpadded + unscaled image</span></span><br><span class="line">            x1 = w_factor * (boxes[:, <span class="number">1</span>] - boxes[:, <span class="number">3</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#xmin</span></span><br><span class="line">            y1 = h_factor * (boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">4</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#ymin</span></span><br><span class="line">            x2 = w_factor * (boxes[:, <span class="number">1</span>] + boxes[:, <span class="number">3</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#xmax</span></span><br><span class="line">            y2 = h_factor * (boxes[:, <span class="number">2</span>] + boxes[:, <span class="number">4</span>] / <span class="number">2</span>)+<span class="number">1</span><span class="comment">#ymax</span></span><br><span class="line">            <span class="comment"># Adjust for added padding</span></span><br><span class="line">            <span class="comment"># 标注的边界框根据pad进行偏移</span></span><br><span class="line">            x1 += pad[<span class="number">0</span>]<span class="comment">#左</span></span><br><span class="line">            y1 += pad[<span class="number">2</span>]<span class="comment">#上</span></span><br><span class="line">            x2 += pad[<span class="number">1</span>]<span class="comment">#右</span></span><br><span class="line">            y2 += pad[<span class="number">3</span>]<span class="comment">#下</span></span><br><span class="line">            <span class="comment"># Returns (x, y, w, h) 坐标进行微调(放缩)</span></span><br><span class="line">            boxes[:, <span class="number">1</span>] = ((x1 + x2) / <span class="number">2</span>) / padded_w</span><br><span class="line">            boxes[:, <span class="number">2</span>] = ((y1 + y2) / <span class="number">2</span>) / padded_h</span><br><span class="line">            boxes[:, <span class="number">3</span>] *= w_factor / padded_w</span><br><span class="line">            boxes[:, <span class="number">4</span>] *= h_factor / padded_h</span><br><span class="line"> </span><br><span class="line">            targets = torch.zeros((len(boxes), <span class="number">6</span>))</span><br><span class="line">            targets[:, <span class="number">1</span>:] = boxes</span><br><span class="line">            print(<span class="string">"len(boxes)："</span>,len(boxes))</span><br><span class="line">            print(<span class="string">"boxes:\n"</span>,boxes)</span><br><span class="line">            print(<span class="string">"targets:\n"</span>,targets)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200922210457663.png#pic_center" alt="在这里插入图片描述"></p><p>这里是标注的.txt文件中解析坐标，生成VOC数据集标注txt的脚本是voc_label.py。完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, getcwd</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join</span><br><span class="line"> </span><br><span class="line">sets=[(<span class="string">''</span>, <span class="string">'train'</span>), (<span class="string">''</span>, <span class="string">'val'</span>), (<span class="string">''</span>, <span class="string">'test'</span>)]</span><br><span class="line"> </span><br><span class="line">classes = [<span class="string">"nodule"</span>]</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(size, box)</span>:</span></span><br><span class="line">    dw = <span class="number">1.</span>/(size[<span class="number">0</span>])</span><br><span class="line">    dh = <span class="number">1.</span>/(size[<span class="number">1</span>])</span><br><span class="line">    x = (box[<span class="number">0</span>] + box[<span class="number">1</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    y = (box[<span class="number">2</span>] + box[<span class="number">3</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    w = box[<span class="number">1</span>] - box[<span class="number">0</span>]</span><br><span class="line">    h = box[<span class="number">3</span>] - box[<span class="number">2</span>]</span><br><span class="line">    x = x*dw</span><br><span class="line">    w = w*dw</span><br><span class="line">    y = y*dh</span><br><span class="line">    h = h*dh</span><br><span class="line">    <span class="keyword">return</span> (x,y,w,h)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_annotation</span><span class="params">(year, image_id)</span>:</span></span><br><span class="line">    in_file = open(<span class="string">'VOCdevkit/VOC%s/Annotations/%s.xml'</span>%(year, image_id))</span><br><span class="line">    out_file = open(<span class="string">'VOCdevkit/VOC%s/labels/%s.txt'</span>%(year, image_id), <span class="string">'w'</span>)</span><br><span class="line">    tree=ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    size = root.find(<span class="string">'size'</span>)</span><br><span class="line">    w = int(size.find(<span class="string">'width'</span>).text)</span><br><span class="line">    h = int(size.find(<span class="string">'height'</span>).text)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> root.iter(<span class="string">'object'</span>):</span><br><span class="line">        <span class="comment">#difficult = obj.find('difficult').text</span></span><br><span class="line">        difficult = <span class="number">0</span></span><br><span class="line">        cls = obj.find(<span class="string">'name'</span>).text</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> classes <span class="keyword">or</span> int(difficult)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        cls_id = classes.index(cls)</span><br><span class="line">        xmlbox = obj.find(<span class="string">'bndbox'</span>)</span><br><span class="line">        b = (float(xmlbox.find(<span class="string">'xmin'</span>).text), float(xmlbox.find(<span class="string">'xmax'</span>).text), float(xmlbox.find(<span class="string">'ymin'</span>).text), float(xmlbox.find(<span class="string">'ymax'</span>).text))</span><br><span class="line">        bb = convert((w,h), b)</span><br><span class="line">        out_file.write(str(cls_id) + <span class="string">" "</span> + <span class="string">" "</span>.join([str(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">'\n'</span>)</span><br><span class="line"> </span><br><span class="line">wd = getcwd()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> year, image_set <span class="keyword">in</span> sets:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'VOCdevkit/VOC%s/labels/'</span>%(year)):</span><br><span class="line">        os.makedirs(<span class="string">'VOCdevkit/VOC%s/labels/'</span>%(year))</span><br><span class="line">    image_ids = open(<span class="string">'VOCdevkit/VOC%s/ImageSets/Main/%s.txt'</span>%(year, image_set)).read().strip().split()</span><br><span class="line">    list_file = open(<span class="string">'%s_%s.txt'</span>%(year, image_set), <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">for</span> image_id <span class="keyword">in</span> image_ids:</span><br><span class="line">        list_file.write(<span class="string">'%s/VOCdevkit/VOC%s/JPEGImages/%s.png\n'</span>%(wd, year, image_id))</span><br><span class="line">        convert_annotation(year, image_id)</span><br><span class="line">    list_file.close()</span><br><span class="line"> </span><br><span class="line">os.system(<span class="string">"cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt &gt; train.txt"</span>)</span><br><span class="line">os.system(<span class="string">"cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt &gt; train.all.txt"</span>)</span><br></pre></td></tr></table></figure><p>注意其中的==convert== 函数，以及语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = (float(xmlbox.find(<span class="string">'xmin'</span>).text), float(xmlbox.find(<span class="string">'xmax'</span>).text), float(xmlbox.find(<span class="string">'ymin'</span>).text), float(xmlbox.find(<span class="string">'ymax'</span>).text))</span><br><span class="line">        bb = convert((w,h), b)</span><br><span class="line">        out_file.write(str(cls_id) + <span class="string">" "</span> + <span class="string">" "</span>.join([str(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><p>这个脚本把<strong>xmax</strong>，<strong>xmin</strong>，<strong>ymax</strong>，<strong>ymin</strong>，转换成编辑框坐标中心，并同<strong>width</strong>和<strong>height</strong>进行归一化到0~1之间。那么需要在训练的过程中解析这些边界框坐标及大小，放进名为tatgets的张量中进行训练，这个坐标如何转换计算的，可以参考下图。<br><img src="https://img-blog.csdnimg.cn/20200922213820150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>（注：<strong>getitem</strong>函数中的w_factor和h_factor是获取的图像的宽高。注意，最后放进<strong>targets</strong>的值是，<strong>groud truth</strong>的中心点坐标，以及w和h（均是在padw和padh放缩之后的值）。这里targets在下面的坐标预测的时候有用。<br>==collate_fn==函数主要是调整imgs的尺寸大小，因为YOLOv3在训练的过程中采用多尺度训练，不断的改变图像的分辨率大小，使得YOLOv3可以很好的适用于各种分辨率大小的图像检测。<strong>collate_fn</strong>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        paths, imgs, targets = list(zip(*batch))</span><br><span class="line">        <span class="comment"># Remove empty placeholder targets</span></span><br><span class="line">        targets = [boxes <span class="keyword">for</span> boxes <span class="keyword">in</span> targets <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>]</span><br><span class="line">        <span class="comment"># Add sample index to targets</span></span><br><span class="line">        <span class="keyword">for</span> i, boxes <span class="keyword">in</span> enumerate(targets):</span><br><span class="line">            boxes[:, <span class="number">0</span>] = i</span><br><span class="line">        targets = torch.cat(targets, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Selects new image size every tenth batch</span></span><br><span class="line">        <span class="keyword">if</span> self.multiscale <span class="keyword">and</span> self.batch_count % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 图像进行放缩 调整分辨率大小</span></span><br><span class="line">            self.img_size = random.choice(range(self.min_size, self.max_size + <span class="number">1</span>, <span class="number">32</span>))</span><br><span class="line">        <span class="comment"># Resize images to input shape</span></span><br><span class="line">        imgs = torch.stack([resize(img, self.img_size) <span class="keyword">for</span> img <span class="keyword">in</span> imgs])</span><br><span class="line">        self.batch_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> paths, imgs, targets</span><br></pre></td></tr></table></figure><p>需要注意的是<strong>targets</strong>的变化方式，在ListDataset类的<strong>getitem</strong>函数中，<strong>targets</strong>的第一位是0，那这个第一位是有什么用呢？<strong>targets</strong>最后输出的是一个<strong>列表</strong>，列表的每一个元素都是一张image对应的n个<strong>target</strong>（这个是张量），target[:,0]表示的是对应image的ID。在训练的时候<strong>collate_fn</strong>函数都会把所有<strong>target</strong>融合在一起成为一个张量（<code>targets = torch.cat(targets, 0)</code>），只有这个张量的第一位（<strong>target[:,0]</strong>）才可以判断这个target属于哪一张图片（即能够匹配图像ID）。<br><img src="https://img-blog.csdnimg.cn/20200922214525167.png#pic_center" alt="在这里插入图片描述"><br><strong>collate_fn</strong>函数的使用也是为什么你图像尺寸是512x512的，但是进行训练的时候却是384x384（以像素点32的进行放缩加减）。<br><img src="https://img-blog.csdnimg.cn/20200922214635950.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200922214644929.png#pic_center" alt="在这里插入图片描述"></p><h5 id="计算Loss"><a href="#计算Loss" class="headerlink" title="计算Loss"></a>计算Loss</h5><p><code>loss, outputs = model(imgs, targets)，</code>这里进行计算loss。其实这个loss的计算是在yolo层计算的，其实不难理解，yolo层是负责目标检测的层，需要输出目标的类别、坐标、大小，所以会在这一层进行loss计算。</p><p>yolo层的具体实现是在==YOLOLayer==中，可查看其forward函数得知loss计算过程，代码（YOLOLayer部分）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output, <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(</span><br><span class="line">                pred_boxes=pred_boxes,</span><br><span class="line">                pred_cls=pred_cls,</span><br><span class="line">                target=targets,</span><br><span class="line">                anchors=self.scaled_anchors,</span><br><span class="line">                ignore_thres=self.ignore_thres,</span><br><span class="line">            )</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Loss : Mask outputs to ignore non-existing objects (except with conf. loss)</span></span><br><span class="line">            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])</span><br><span class="line">            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])</span><br><span class="line">            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])</span><br><span class="line">            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])</span><br><span class="line">            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])</span><br><span class="line">            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj</span><br><span class="line">            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])</span><br><span class="line">            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Metrics</span></span><br><span class="line">            cls_acc = <span class="number">100</span> * class_mask[obj_mask].mean()</span><br><span class="line">            conf_obj = pred_conf[obj_mask].mean()</span><br><span class="line">            conf_noobj = pred_conf[noobj_mask].mean()</span><br><span class="line">            conf50 = (pred_conf &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou50 = (iou_scores &gt; <span class="number">0.5</span>).float()</span><br><span class="line">            iou75 = (iou_scores &gt; <span class="number">0.75</span>).float()</span><br><span class="line">            detected_mask = conf50 * class_mask * tconf</span><br><span class="line">            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line">            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">            self.metrics = &#123;</span><br><span class="line">                <span class="string">"loss"</span>: to_cpu(total_loss).item(),</span><br><span class="line">                <span class="string">"x"</span>: to_cpu(loss_x).item(),</span><br><span class="line">                <span class="string">"y"</span>: to_cpu(loss_y).item(),</span><br><span class="line">                <span class="string">"w"</span>: to_cpu(loss_w).item(),</span><br><span class="line">                <span class="string">"h"</span>: to_cpu(loss_h).item(),</span><br><span class="line">                <span class="string">"conf"</span>: to_cpu(loss_conf).item(),</span><br><span class="line">                <span class="string">"cls"</span>: to_cpu(loss_cls).item(),</span><br><span class="line">                <span class="string">"cls_acc"</span>: to_cpu(cls_acc).item(),</span><br><span class="line">                <span class="string">"recall50"</span>: to_cpu(recall50).item(),</span><br><span class="line">                <span class="string">"recall75"</span>: to_cpu(recall75).item(),</span><br><span class="line">                <span class="string">"precision"</span>: to_cpu(precision).item(),</span><br><span class="line">                <span class="string">"conf_obj"</span>: to_cpu(conf_obj).item(),</span><br><span class="line">                <span class="string">"conf_noobj"</span>: to_cpu(conf_noobj).item(),</span><br><span class="line">                <span class="string">"grid_size"</span>: grid_size,</span><br><span class="line">            &#125;</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">return</span> output, total_loss</span><br></pre></td></tr></table></figure><p>可以看到，batch设置的是8，看到图片的尺寸被放缩成了【352， 352】，分别进行8、16、32倍下采样，即对应的shape是【44，44】【22， 22】【11， 11】<br><img src="https://img-blog.csdnimg.cn/20200923203340676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>同时使用==build_targets==函数得到<strong>iou_scores</strong>, <strong>class_mask</strong>, <strong>obj_mask</strong>, <strong>noobj_mask</strong>, <strong>tx</strong>, <strong>ty</strong>, <strong>tw</strong>, <strong>th</strong>, <strong>tcls</strong>, <strong>tconf</strong>。<br><strong>obj_mask</strong>表示有物体落在特征图中某一个cell的索引，所以在初始化的时候置<strong>0</strong>，如果有物体落在那个cell中，那个对应的位置会置<strong>1</strong>。所以会有代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">obj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">........</span><br><span class="line">obj_mask[b, best_n, gj, gi] = <span class="number">1</span></span><br></pre></td></tr></table></figure><p>同理，表示没有物体落在特征图中某一个cell的索引,所以在初始化的时候置<strong>1</strong>，如果没有有物体落在那个cell中，那个对应的位置会置<strong>0</strong>。同时，如果预测的IOU值过大，（大于阈值ignore_thres）时，那么可以认为这个cell是有物体的，要置0。所以会有代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">1</span>)</span><br><span class="line">.......   </span><br><span class="line">noobj_mask[b, best_n, gj, gi] = <span class="number">0</span>    </span><br><span class="line"><span class="comment"># Set noobj mask to zero where iou exceeds ignore threshold    </span></span><br><span class="line"><span class="keyword">for</span> i, anchor_ious <span class="keyword">in</span> enumerate(ious.t()):        </span><br><span class="line">    noobj_mask[b[i], anchor_ious &gt; ignore_thres, gj[i], gi[i]] = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>查看==build_targets==代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_targets</span><span class="params">(pred_boxes, pred_cls, target, anchors, ignore_thres)</span>:</span></span><br><span class="line"> </span><br><span class="line">    ByteTensor = torch.cuda.ByteTensor <span class="keyword">if</span> pred_boxes.is_cuda <span class="keyword">else</span> torch.ByteTensor</span><br><span class="line">    FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> pred_boxes.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line"> </span><br><span class="line">    nB = pred_boxes.size(<span class="number">0</span>)</span><br><span class="line">    nA = pred_boxes.size(<span class="number">1</span>)</span><br><span class="line">    nC = pred_cls.size(<span class="number">-1</span>)</span><br><span class="line">    nG = pred_boxes.size(<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Output tensors</span></span><br><span class="line">    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(<span class="number">1</span>)</span><br><span class="line">    class_mask = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    tx = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    ty = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    tw = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    th = FloatTensor(nB, nA, nG, nG).fill_(<span class="number">0</span>)</span><br><span class="line">    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Convert to position relative to box</span></span><br><span class="line">    target_boxes = target[:, <span class="number">2</span>:<span class="number">6</span>] * nG</span><br><span class="line">    gxy = target_boxes[:, :<span class="number">2</span>]</span><br><span class="line">    gwh = target_boxes[:, <span class="number">2</span>:]</span><br><span class="line">    <span class="comment"># Get anchors with best iou</span></span><br><span class="line">    ious = torch.stack([bbox_wh_iou(anchor, gwh) <span class="keyword">for</span> anchor <span class="keyword">in</span> anchors])</span><br><span class="line">    best_ious, best_n = ious.max(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Separate target values</span></span><br><span class="line">    b, target_labels = target[:, :<span class="number">2</span>].long().t()</span><br><span class="line">    gx, gy = gxy.t()</span><br><span class="line">    gw, gh = gwh.t()</span><br><span class="line">    gi, gj = gxy.long().t()</span><br><span class="line">    <span class="comment"># Set masks</span></span><br><span class="line">    obj_mask[b, best_n, gj, gi] = <span class="number">1</span></span><br><span class="line">    noobj_mask[b, best_n, gj, gi] = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Set noobj mask to zero where iou exceeds ignore threshold</span></span><br><span class="line">    <span class="keyword">for</span> i, anchor_ious <span class="keyword">in</span> enumerate(ious.t()):</span><br><span class="line">        noobj_mask[b[i], anchor_ious &gt; ignore_thres, gj[i], gi[i]] = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Coordinates</span></span><br><span class="line">    tx[b, best_n, gj, gi] = gx - gx.floor()</span><br><span class="line">    ty[b, best_n, gj, gi] = gy - gy.floor()</span><br><span class="line">    <span class="comment"># Width and height</span></span><br><span class="line">    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, <span class="number">0</span>] + <span class="number">1e-16</span>)</span><br><span class="line">    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, <span class="number">1</span>] + <span class="number">1e-16</span>)</span><br><span class="line">    <span class="comment"># One-hot encoding of label</span></span><br><span class="line">    tcls[b, best_n, gj, gi, target_labels] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># Compute label correctness and iou at best anchor</span></span><br><span class="line">    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(<span class="number">-1</span>) == target_labels).float()</span><br><span class="line">    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">    tconf = obj_mask.float()</span><br><span class="line">    <span class="keyword">return</span> iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf</span><br></pre></td></tr></table></figure><p>根据下图，不难理解：<br><strong>nB</strong>：Batch是多大。<br><strong>nA</strong>：多少个Anchor 。<br><strong>nC</strong>：训练多少个class，在这里我之训练一个类，所以是1。<br><strong>nG</strong>：grid大小，每一行分（列）成多少个cell。<img src="https://img-blog.csdnimg.cn/2020092321093911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">)<img src="https://img-blog.csdnimg.cn/20200923211239660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>同时提取<strong>targets</strong>中的坐标信息，分别给<strong>gxy</strong>和<strong>gwh</strong>张量，乘以<strong>nG</strong>是因为坐标信息是归一化到0~1之间，需要进行放大。<br><img src="https://img-blog.csdnimg.cn/20200923211703697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>下一步便是用anchor进行计算iou值 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get anchors with best iou</span></span><br><span class="line">    ious = torch.stack([bbox_wh_iou(anchor, gwh) <span class="keyword">for</span> anchor <span class="keyword">in</span> anchors])</span><br><span class="line">    best_ious, best_n = ious.max(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>实现的函数为 ==bbox_wh_iou==，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_wh_iou</span><span class="params">(wh1, wh2)</span>:</span></span><br><span class="line">    wh2 = wh2.t()</span><br><span class="line">    w1, h1 = wh1[<span class="number">0</span>], wh1[<span class="number">1</span>]</span><br><span class="line">    w2, h2 = wh2[<span class="number">0</span>], wh2[<span class="number">1</span>]</span><br><span class="line">    inter_area = torch.min(w1, w2) * torch.min(h1, h2)</span><br><span class="line">    union_area = (w1 * h1 + <span class="number">1e-16</span>) + w2 * h2 - inter_area</span><br><span class="line">    <span class="keyword">return</span> inter_area / union_area</span><br></pre></td></tr></table></figure><p>计算结果如下。仍然把<strong>batch</strong>设为8。<strong>ious.shape</strong>为【3， 8】这是因为有三个<strong>anchor</strong>，每一个anchor都会和标记的label进行计算<strong>iou</strong>值，即看哪一个<strong>anchor</strong>和<strong>ground truth</strong>（真实的、标注的边界框）最接近。<strong>注意：【3，8】的8不是batch是8，而是有8个target，恰好每一张图都有一个target，所以是8，但往往一张图可能存在多个taget</strong>。<br><img src="https://img-blog.csdnimg.cn/20200923215913232.png#pic_center" alt="在这里插入图片描述"><br>gxy.t()是为了把shape从n x 2 变成 2 x n。 <code>gi, gj = gxy.long().t()</code>，是通过.long的方式去除小数点，保留整数。如此便可以设置masks。<strong>b</strong>是指第几个<strong>target</strong>。<strong>gi</strong>, <strong>gj</strong> 便是特征图中对应的左上角的坐标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set masks</span></span><br><span class="line">   obj_mask[b, best_n, gj, gi] = <span class="number">1</span></span><br><span class="line">   noobj_mask[b, best_n, gj, gi] = <span class="number">0</span></span><br></pre></td></tr></table></figure><h6 id="坐标预测"><a href="#坐标预测" class="headerlink" title="坐标预测"></a>坐标预测</h6><p>接下来是坐标预测，我们先来看YOLOv3坐标预测图。<br><img src="https://img-blog.csdnimg.cn/20200923223018564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其中，Cx,Cy是feature map中grid cell的左上角坐标，在yolov3中每个grid cell在feature map中的宽和高均为1。如下图1的情形时，这个bbox边界框的中心属于第二行第二列的grid cell，它的左上角坐标为(1,1)，故Cx=1,Cy=1.公式中的Pw、Ph是预设的anchor box映射到feature map中的宽和高(<strong>anchor box原本设定是相对于416*416坐标系下的坐标，在yolov3.cfg文件中写明了，代码中是把cfg中读取的坐标除以stride如32映射到feature map坐标系中</strong>)。</p><p>最终得到的边框坐标值是bx,by,bw,bh，即边界框bbox相对于feature map的位置和大小，是我们需要的预测输出坐标。<strong>但我们网络实际上的学习目标是tx,ty,tw,th这４个offsets</strong>，其中tx,ty是预测的坐标偏移值，tw,th是尺度缩放，有了这４个offsets，自然可以根据之前的公式去求得真正需要的bx,by,bw,bh４个坐标。</p><p><strong>那么我们的网络为何不直接学习bx,by,bw,bh呢</strong>？因为YOLO 的输出是一个卷积特征图，包含沿特征图深度的边界框属性。边界框属性由彼此堆叠的单元格预测得出。因此，如果你需要在 (5,6) 处访问该单元格的第二个边框bbox，那么你需要通过 map[5,6, (5+C): 2<em>(5+C)] 将其编入索引。这种格式对于输出处理过程（例如通过目标置信度进行阈值处理、添加对中心的网格偏移、应用锚点等）很不方便，因此我们求偏移量即可。那么这样就只需要求偏移量，也就可以用上面的公式求出bx,by,bw,bh，反正是等价的。另外，*</em>通过学习偏移量，就可以通过网络原始给定的anchor box坐标经过线性回归微调（平移加尺度缩放）去逐渐靠近groundtruth**。为何微调可看做线性回归往下看。</p><p>这里需要注意的是，虽然输入尺寸是416 * 416,但原图是按照纵横比例缩放至416 * 416的， <strong>取 min(w/img_w, h/img_h)这个比例来缩放，保证长的边缩放为需要的输入尺寸416，而短边按比例缩放不会扭曲</strong>，img_w,img_h是原图尺寸768,576, 缩放后的尺寸为new_w, new_h=416,312，需要的输入尺寸是w,h=416 * 416.如下图所示：<br><img src="https://img-blog.csdnimg.cn/2020092322310363.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200923231534284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>剩下的灰色区域用(128,128,128)填充即可构造为416 * 416。不管训练还是测试时都需要这样操作原图。pytorch代码中比较好理解这一点。下面这个函数实现了对原图的变换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterbox_image</span><span class="params">(img, inp_dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    lteerbox_image()将图片按照纵横比进行缩放，将空白部分用(128,128,128)填充,调整图像尺寸</span></span><br><span class="line"><span class="string">    具体而言,此时某个边正好可以等于目标长度,另一边小于等于目标长度</span></span><br><span class="line"><span class="string">    将缩放后的数据拷贝到画布中心,返回完成缩放</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    img_w, img_h = img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]</span><br><span class="line">    w, h = inp_dim<span class="comment">#inp_dim是需要resize的尺寸（如416*416）</span></span><br><span class="line">    <span class="comment"># 取min(w/img_w, h/img_h)这个比例来缩放，缩放后的尺寸为new_w, new_h,即保证较长的边缩放后正好等于目标长度(需要的尺寸)，另一边的尺寸缩放后还没有填充满.</span></span><br><span class="line">    new_w = int(img_w * min(w/img_w, h/img_h))</span><br><span class="line">    new_h = int(img_h * min(w/img_w, h/img_h))</span><br><span class="line">    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC) <span class="comment">#将图片按照纵横比不变来缩放为new_w x new_h，768 x 576的图片缩放成416x312.,用了双三次插值</span></span><br><span class="line">    <span class="comment"># 创建一个画布, 将resized_image数据拷贝到画布中心。</span></span><br><span class="line">    canvas = np.full((inp_dim[<span class="number">1</span>], inp_dim[<span class="number">0</span>], <span class="number">3</span>), <span class="number">128</span>)<span class="comment">#生成一个我们最终需要的图片尺寸hxwx3的array,这里生成416x416x3的array,每个元素值为128</span></span><br><span class="line">    <span class="comment"># 将wxhx3的array中对应new_wxnew_hx3的部分(这两个部分的中心应该对齐)赋值为刚刚由原图缩放得到的数组,得到最终缩放后图片</span></span><br><span class="line">    canvas[(h-new_h)//<span class="number">2</span>:(h-new_h)//<span class="number">2</span> + new_h,(w-new_w)//<span class="number">2</span>:(w-new_w)//<span class="number">2</span> + new_w,  :] = resized_image</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> canvas</span><br></pre></td></tr></table></figure><p>   而且我们注意yolov3需要的训练数据的label是根据原图尺寸归一化了的，这样做是因为怕<strong>大的边框的影响比小的边框影响大，因此做了归一化的操作，这样大的和小的边框都会被同等看待了，而且训练也容易收敛(类比于refinedbox)</strong>。既然label是根据原图的尺寸归一化了的，自己制作数据集时也需要归一化才行，如何转为yolov3需要的label网上有一大堆教程，也放一篇链接<a href="https://blog.csdn.net/qq_34199326/article/details/83819140" target="_blank" rel="noopener">https://blog.csdn.net/qq_34199326/article/details/83819140</a>。</p><p>  这里解释一下anchor box，YOLO3为每种FPN预测特征图（13 * 13,26 * 26,52 * 52）设定3种anchor box，总共聚类出9种尺寸的anchor box。在COCO数据集这9个anchor box是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。分配上，在最小的13 * 13特征图上由于其感受野最大故应用最大的anchor box (116x90)，(156x198)，(373x326)，（这几个坐标是针对416 * 416下的，当然要除以32把尺度缩放到13*13下），适合检测较大的目标。中等的26 * 26特征图上由于其具有中等感受野故应用中等的anchor box (30x61)，(62x45)，(59x119)，适合检测中等大小的目标。较大的52 * 52特征图上由于其具有较小的感受野故应用最小的anchor box(10x13)，(16x30)，(33x23)，适合检测较小的目标。同Faster-Rcnn一样，特征图的每个像素（即每个grid）都会有对应的三个anchor box，如13 * 13特征图的每个grid都有三个anchor box (116x90)，(156x198)，(373x326)（这几个坐标需除以32缩放尺寸）。</p><p><strong>那么4个坐标tx,ty,tw,th是怎么求出来的呢</strong>？对于训练样本，在大多数文章里需要用到ground truth的真实框来求这4个坐标：<br><img src="https://img-blog.csdnimg.cn/20200923233652727.png#pic_center" alt="在这里插入图片描述"><br>上面这个公式是<strong>faster-rcnn</strong>系列文章用到的公式，Px,Py在faster-rcnn系列文章是预设的anchor box在feature map上的中心点坐标。 Pw、Ph是预设的anchor box的在feature map上的宽和高。至于Gx、Gy、Gw、Gh自然就是ground truth在这个feature map的4个坐标了(其实上面已经描述了这个过程，要根据原图坐标系先根据原图纵横比不变映射为416 * 416坐标下的一个子区域如416 * 312，取 min(w/img_w, h/img_h)这个比例来缩放成416 * 312，再填充为416 * 416，坐标变换上只需要让ground truth在416 * 312下的y1,y2（即左上角和右下角纵坐标）加上图2灰色部分的一半<br>y1=y1+(416-416/768 * 576)/2=y1+(416-312)/2，<br>y2同样的操作，把x1,x2,y1,y2的坐标系的换算从针对实际红框的坐标系(416 * 312)变为416 * 416下了，这样保证bbox不会扭曲，然后除以stride得到相对于feature map的坐标)。</p><p><strong>用x,y坐标减去anchor box的x,y坐标得到偏移量好理解，为何要除以feature map上anchor box的宽和高呢</strong>？我认为可能是为了把绝对尺度变为相对尺度，毕竟作为偏移量，不能太大了对吧。而且不同尺度的anchor box如果都用Gx-Px来衡量显然不对，有的anchor box大有的却很小，都用Gx-Px会导致不同尺度的anchor box权重相同，而大的anchor box肯定更能容忍大点的偏移量，小的anchor box对小偏移都很敏感，故除以宽和高可以权衡不同尺度下的预测坐标偏移量。</p><p>但是在yolov3中与faster-rcnn系列文章用到的公式在前两行是不同的，yolov3里Px和Py就换为了feature map上的grid cell左上角坐标Cx,Cy了，即在yolov3里是Gx,Gy减去grid cell左上角坐标Cx,Cy。x,y坐标并没有针对anchon box求偏移量，所以并不需要除以Pw,Ph。</p><p>也就是说是tx = Gx - Cx ，ty = Gy - Cy<br>这样就可以直接求bbox中心距离grid cell左上角的坐标的偏移量。</p><p>tw和th的公式yolov3和faster-rcnn系列是一样的，是物体所在边框的长宽和anchor box长宽之间的比率，不管Faster-RCNN还是YOLO，都不是直接回归bounding box的长宽而是尺度缩放到对数空间，是怕训练会带来不稳定的梯度。因为如果不做变换，直接预测相对形变tw，那么要求tw&gt;0，因为你的框的宽高不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。</p><p>这里就有个重要的疑问了，<strong>一个尺度的feature map有三个anchors，那么对于某个ground truth框，究竟是哪个anchor负责匹配它呢</strong>？前面已经说过，和YOLOv1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的3个anchor box负责预测它，具体是哪个anchor box预测它，需要在训练中确定，即由那个与ground truth的IOU最大的anchor box预测它，而剩余的2个anchor box不与该ground truth匹配。YOLOv3需要假定每个cell至多含有一个grounth truth，而在实际上基本不会出现多于1个的情况。与ground truth匹配的anchor box计算坐标误差、置信度误差（此时target为1）以及分类误差，而其它的anchor box只计算置信度误差（此时target为0）。</p><p>有了平移（tx,ty）和尺度缩放（tw,th）才能让anchor box经过微调与grand truth重合。如图3，红色框为anchor box，绿色框为Ground Truth，平移+尺度缩放可实线红色框先平移到虚线红色框，然后再缩放到绿色框。边框回归最简单的想法就是通过平移加尺度缩放进行微调。</p><p><img src="https://img-blog.csdnimg.cn/20200923234645255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="加粗样式"><br><strong>边框回归为何只能微调</strong>？当输入的 Proposal 与 Ground Truth 相差较小时，即IOU很大时(RCNN 设置的是 IoU&gt;0.6)， 可以认为这种变换是一种线性变换， 那么我们就可以用线性回归（线性回归就是给定输入的特征向量 X, 学习一组参数 W, 使得经过线性回归后的值跟真实值 Y(Ground Truth)非常接近. 即Y≈WX ）来建模对窗口进行微调， 否则会导致训练的回归模型不work（当 Proposal跟 GT 离得较远，就是复杂的非线性问题了，此时用线性回归建模显然就不合理了）<br><img src="https://img-blog.csdnimg.cn/20200923235052608.png#pic_center" alt="在这里插入图片描述"><br>那么训练时用的groundtruth的4个坐标去做差值和比值得到tx,ty,tw,th，测试时就用预测的bbox就好了，公式修改就简单了，把Gx和Gy改为预测的x,y，Gw、Gh改为预测的w,h即可。</p><p>所以从前面的分析我们可以看出网络可以不断学习<strong>tx,ty,tw,th偏移量和尺度缩放</strong>，<strong>预测时</strong>使用这4个offsets求得bx,by,bw,bh即可，那么问题是：<br><img src="https://img-blog.csdnimg.cn/20200923235420229.png#pic_center" alt="在这里插入图片描述"><br><strong>这个公式tx,ty为何要sigmoid一下呢</strong>？前面讲到了在yolov3中没有让Gx - Cx后除以Pw得到tx，而是直接Gx - Cx得到tx，这样会有问题是导致tx比较大且很可能&gt;1.(因为没有除以Pw归一化尺度)。用sigmoid将tx,ty压缩到[0,1]区间內，可以有效的确保目标中心处于执行预测的网格单元中，防止偏移过多。举个例子，我们刚刚都知道了网络不会预测边界框中心的确切坐标而是预测与预测目标的grid cell左上角相关的偏移tx,ty。如13*13的feature map中，某个目标的中心点预测为(0.4,0.7)，它的cx,cy即中心落入的grid cell坐标是(6,6)，则该物体的在feature map中的中心实际坐标显然是(6.4,6.7).这种情况没毛病，但若tx,ty大于1，比如(1.2,0.7)则该物体在feature map的的中心实际坐标是(7.2,6.7)，注意这时候该物体中心在这个物体所属grid cell外面了，但(6,6)这个grid cell却检测出我们这个单元格内含有目标的中心（yolo是采取物体中心归哪个grid cell整个物体就归哪个grid celll了），这样就矛盾了，因为左上角为(6,6)的grid cell负责预测这个物体，这个物体中心必须出现在这个grid cell中而不能出现在它旁边网格中，一旦tx,ty算出来大于1就会引起矛盾，因而必须归一化。</p><p> <strong>看最后两行公式，tw为何要作为指数呢</strong>，这就好理解了，因为tw,th是log尺度缩放到对数空间了，当然要指数回来，而且这样可以保证大于0。 至于左边乘以Pw或者Ph是因为tw=log(Gw/Pw)当然应该乘回来得到真正的宽高。</p><p>记feature map大小为Ｗ，Ｈ（如13*13），可将bbox相对于整张图片的位置和大小计算出来（使4个值均处于[0,1]区间内）约束了bbox的位置预测值到[0,1]会使得模型更容易稳定训练（如果不是[0,1]区间，yolo的每个bbox的维度都是85，前5个属性是(Cx,Cy,w,h,confidence)，后80个是类别概率，如果坐标不归一化，和这些概率值一起训练肯定无法收敛）</p><p>只需要把之前计算的bx,bw都除以W,把by,bh都除以H。即<br><img src="https://img-blog.csdnimg.cn/20200924000346228.png#pic_center" alt="在这里插入图片描述"><br>所以回到我们的代码，gx表示x坐标的具体值，gx.floor（）则是向下取整，两者相减即可得到偏移值。<strong>所以其实总结一下在训练的时候非常巧妙，没有直接训练bw和bh，而是训练tw，th</strong>。这里注意代码是怎么写的：在==build_targets==函数中，<strong>gw</strong>和<strong>gh</strong>是标准的真实值（target）在该特征图的宽w和高h。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert to position relative to box</span></span><br><span class="line">    target_boxes = target[:, <span class="number">2</span>:<span class="number">6</span>] * nG</span><br><span class="line">    gxy = target_boxes[:, :<span class="number">2</span>]</span><br><span class="line">    gwh = target_boxes[:, <span class="number">2</span>:]</span><br></pre></td></tr></table></figure><p><strong>gw</strong>和<strong>gh</strong>则是通过尺度缩放成<strong>tw</strong>和<strong>th</strong>。注意下面代码中的参数：<strong>anchors</strong>[best_n][:, 0]和<strong>anchors</strong>[best_n][:, 1]，其实分别只指输入到该特征图大小的<strong>anchors</strong>的w和h。因为这个函数的输入<strong>anchors</strong>的值是<strong>self.scaled_anchors</strong> 。具体代码：</p><p><code>self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])</code></p><p>所以tw和th是该特征图大小下的标注的真实值（<strong>target</strong>）w和h与使用该特征图大小下进行检测的<strong>anchor</strong>的w和h的自然对数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Coordinates</span></span><br><span class="line">   tx[b, best_n, gj, gi] = gx - gx.floor()</span><br><span class="line">   ty[b, best_n, gj, gi] = gy - gy.floor()</span><br><span class="line">   <span class="comment"># Width and height</span></span><br><span class="line">   tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, <span class="number">0</span>] + <span class="number">1e-16</span>)</span><br><span class="line">   th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, <span class="number">1</span>] + <span class="number">1e-16</span>)</span><br></pre></td></tr></table></figure><p>接下来计算w和h的loss方式。计算方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask]</span><br></pre></td></tr></table></figure><p>tw和th我们知道怎么得到了，那么看下w和h是如何得到的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_boxes[..., <span class="number">2</span>] = torch.exp(w.data) * self.anchor_w</span><br><span class="line">        pred_boxes[..., <span class="number">3</span>] = torch.exp(h.data) * self.anchor_h</span><br></pre></td></tr></table></figure><p>这里的self.anchor_w和self.anchor_h就是self.scaled_anchors 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.anchor_w = self.scaled_anchors[:, <span class="number">0</span>:<span class="number">1</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.anchor_h = self.scaled_anchors[:, <span class="number">1</span>:<span class="number">2</span>].view((<span class="number">1</span>, self.num_anchors, <span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200924151223279.png#pic_center" alt="在这里插入图片描述"><br>其中可以把<img src="https://img-blog.csdnimg.cn/20200924151320586.png#pic_center" alt="在这里插入图片描述"><br>和<img src="https://img-blog.csdnimg.cn/20200924151331970.png#pic_center" alt="在这里插入图片描述"><br>当作真实值，<img src="https://img-blog.csdnimg.cn/20200924151341353.png#pic_center" alt="在这里插入图片描述"><br>和<img src="https://img-blog.csdnimg.cn/20200924151348479.png#pic_center" alt="在这里插入图片描述"><br>当作预测值，但是yolov3在训练的过程中从代码中我们也可以看到，不是直接做边界框回归，而是w和tw，h和th进行回归，做loss值。我们通过得到tw和th值就可以得到bw和bh。这是因为：<br><img src="https://img-blog.csdnimg.cn/20200924151623243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>th同理。</p><p>继续往下看==build_targets==的代码：下面这句代码，意思是第b张图片，使用第best_n个anchors来预测 哪一类（target_labels）物体。查看b和target_labels的值来方便理解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># One-hot encoding of label</span></span><br><span class="line">    tcls[b, best_n, gj, gi, target_labels] = <span class="number">1</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200924152545360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>接下来计算<strong>class_mask,iou_scores,</strong>并返回。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute label correctness and iou at best anchor</span></span><br><span class="line">    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(<span class="number">-1</span>) == target_labels).float()</span><br><span class="line">    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=<span class="literal">False</span>)</span><br><span class="line">)</span><br><span class="line">    tconf = obj_mask.float()</span><br><span class="line">    <span class="keyword">return</span> iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf</span><br></pre></td></tr></table></figure><p><strong>class_mask</strong>的计算：b表示的targets对应image的ID，这个上面有解释，这里的b的长度是20，说明有20个target。每一个target都对应一个target_labels,即类别标签，表示这个target是什么类别，这里使用的是3类，所以target_labels的取值范围是0~2。<strong>pred_cls</strong>的shape也说明了这一点。<strong>.argmax(-1)</strong>返回最后一维度最大值的索引。注意，<strong>pred_cls</strong>[b, best_n, gj, gi].shape是【20， 3】和初期的<strong>pred_cls.shape</strong>是【8， 3， 12， 12， 3】是不一样的。<strong>pred_cls</strong>[b, best_n, gj, gi]的值如下图所示，可以抽象一点理解，[b, best_n, gj, gi]是索引号，<strong>pred_cls</strong>[b, best_n, gj, gi]便是这些索引号对应的张量堆叠而成的。如果<strong>pred_cls</strong>[b, best_n, gj, gi].argmax(-1) 等于target_labels的话，就会把这里相应位置的class_mask置1，表示这个特征地图的第<strong>gj</strong>行、第<strong>gi</strong>的cell预测的类别是正确的。</p><p><strong>iou</strong>值的计算：使用<strong>iou_scores</strong>函数。这里计算iou值是需要既考虑w，h还有坐标x，y。</p><p>原因：</p><ol><li>计算w和h的loss是anchor和target形状大小的匹配程度，得到一个和真实形状（target）最接近的anchor去进行预测（检测），然后由于IOU值很高，就可以通过平移放缩的方式进行微调，边界框回归。</li><li>还需要计算IOU值的得分，所以还必须要考虑预测框和真实框的坐标。</li></ol><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_iou</span><span class="params">(box1, box2, x1y1x2y2=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns the IoU of two bounding boxes</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> x1y1x2y2:</span><br><span class="line">        <span class="comment"># Transform from center and width to exact coordinates</span></span><br><span class="line">        b1_x1, b1_x2 = box1[:, <span class="number">0</span>] - box1[:, <span class="number">2</span>] / <span class="number">2</span>, box1[:, <span class="number">0</span>] + box1[:, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        b1_y1, b1_y2 = box1[:, <span class="number">1</span>] - box1[:, <span class="number">3</span>] / <span class="number">2</span>, box1[:, <span class="number">1</span>] + box1[:, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">        b2_x1, b2_x2 = box2[:, <span class="number">0</span>] - box2[:, <span class="number">2</span>] / <span class="number">2</span>, box2[:, <span class="number">0</span>] + box2[:, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        b2_y1, b2_y2 = box2[:, <span class="number">1</span>] - box2[:, <span class="number">3</span>] / <span class="number">2</span>, box2[:, <span class="number">1</span>] + box2[:, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the coordinates of bounding boxes</span></span><br><span class="line">        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, <span class="number">0</span>], box1[:, <span class="number">1</span>], box1[:, <span class="number">2</span>], box1[:, <span class="number">3</span>]</span><br><span class="line">        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, <span class="number">0</span>], box2[:, <span class="number">1</span>], box2[:, <span class="number">2</span>], box2[:, <span class="number">3</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># get the corrdinates of the intersection rectangle</span></span><br><span class="line">    inter_rect_x1 = torch.max(b1_x1, b2_x1)</span><br><span class="line">    inter_rect_y1 = torch.max(b1_y1, b2_y1)</span><br><span class="line">    inter_rect_x2 = torch.min(b1_x2, b2_x2)</span><br><span class="line">    inter_rect_y2 = torch.min(b1_y2, b2_y2)</span><br><span class="line">    <span class="comment"># Intersection area</span></span><br><span class="line">    <span class="comment"># torch.clamp torch.clamp(input, min, max, out=None) → Tensor</span></span><br><span class="line">    <span class="comment"># 将输入input张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量。</span></span><br><span class="line">    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + <span class="number">1</span>, min=<span class="number">0</span>) * torch.clamp(</span><br><span class="line">        inter_rect_y2 - inter_rect_y1 + <span class="number">1</span>, min=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Union Area</span></span><br><span class="line">    b1_area = (b1_x2 - b1_x1 + <span class="number">1</span>) * (b1_y2 - b1_y1 + <span class="number">1</span>)</span><br><span class="line">    b2_area = (b2_x2 - b2_x1 + <span class="number">1</span>) * (b2_y2 - b2_y1 + <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    iou = inter_area / (b1_area + b2_area - inter_area + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><p>==build_targets==函数分析完了，回到==YOLOLayer==层代码中，接下来就是loss值计算，我们都知道loss需要分为三部分计算：</p><ol><li><strong>第一部分</strong>边界框损失，包含x,y,w,h。</li><li><strong>第二部分</strong>是置信度损失。</li><li><strong>第三部分</strong>是类别损失，代码如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])</span><br><span class="line">            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])</span><br><span class="line">            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])</span><br><span class="line">            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])</span><br><span class="line">            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])</span><br><span class="line">            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])</span><br><span class="line">            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj</span><br><span class="line">            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])</span><br><span class="line">            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls</span><br></pre></td></tr></table></figure><p>根据以上代码，我们写出<strong>YOLOv3的损失函数</strong><br><img src="https://img-blog.csdnimg.cn/20200924154348208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上式中<strong>batch</strong>是指批数据量的大小，<strong>anchor</strong>是指预测使用的框，每一层yolo中的<strong>anchor</strong>数为3，<strong>grid</strong>是特征图的尺寸。<img src="https://img-blog.csdnimg.cn/20200924154617876.png#pic_center" alt="在这里插入图片描述"><br>表示batch中的第<strong>i</strong>个数据，第<strong>j</strong>个anchor，在特征图中的第<strong>k</strong>个cell有预测的物体。<img src="https://img-blog.csdnimg.cn/20200924154701884.png#pic_center" alt="在这里插入图片描述"><br>和<img src="https://img-blog.csdnimg.cn/20200924154709505.png#pic_center" alt="在这里插入图片描述"><br>是惩罚项因子，在代码中是<strong>self.obj_scale</strong>和<strong>self.nobj_scale</strong>。</p><p>最后还有一小部分就是计算各种指标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Metrics</span></span><br><span class="line">           cls_acc = <span class="number">100</span> * class_mask[obj_mask].mean()</span><br><span class="line">           conf_obj = pred_conf[obj_mask].mean()</span><br><span class="line">           conf_noobj = pred_conf[noobj_mask].mean()</span><br><span class="line">           conf50 = (pred_conf &gt; <span class="number">0.5</span>).float()</span><br><span class="line">           iou50 = (iou_scores &gt; <span class="number">0.5</span>).float()</span><br><span class="line">           iou75 = (iou_scores &gt; <span class="number">0.75</span>).float()</span><br><span class="line">           detected_mask = conf50 * class_mask * tconf</span><br><span class="line">           precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + <span class="number">1e-16</span>)</span><br><span class="line">           recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br><span class="line">           recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + <span class="number">1e-16</span>)</span><br></pre></td></tr></table></figure><p>再计算出loss值之和，并进行反向传播，梯度优化。</p><h4 id="查看训练指标并评估"><a href="#查看训练指标并评估" class="headerlink" title="查看训练指标并评估"></a>查看训练指标并评估</h4><p>这段完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        <span class="comment">#print("len(dataloader):\n",len(dataloader))</span></span><br><span class="line">        <span class="keyword">for</span> batch_i, (_, imgs, targets) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">            batches_done = len(dataloader) * epoch + batch_i</span><br><span class="line">            imgs = Variable(imgs.to(device))</span><br><span class="line">            targets = Variable(targets.to(device), requires_grad=<span class="literal">False</span>)</span><br><span class="line">            print(<span class="string">"targets.shape:\n"</span>,targets.shape)</span><br><span class="line">            loss, outputs = model(imgs, targets)                      </span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">if</span> batches_done % opt.gradient_accumulations:</span><br><span class="line">                <span class="comment"># Accumulates gradient before each step</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">            <span class="comment">#   Log progress</span></span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line"> </span><br><span class="line">            log_str = <span class="string">"\n---- [Epoch %d/%d, Batch %d/%d] ----\n"</span> % (epoch, opt.epochs, batch_i, len(dataloader))</span><br><span class="line"> </span><br><span class="line">            metric_table = [[<span class="string">"Metrics"</span>, *[<span class="string">f"YOLO Layer <span class="subst">&#123;i&#125;</span>"</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model.yolo_layers))]]]</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Log metrics at each YOLO layer</span></span><br><span class="line">            <span class="keyword">for</span> i, metric <span class="keyword">in</span> enumerate(metrics):</span><br><span class="line">                formats = &#123;m: <span class="string">"%.6f"</span> <span class="keyword">for</span> m <span class="keyword">in</span> metrics&#125;</span><br><span class="line">                formats[<span class="string">"grid_size"</span>] = <span class="string">"%2d"</span></span><br><span class="line">                formats[<span class="string">"cls_acc"</span>] = <span class="string">"%.2f%%"</span></span><br><span class="line">                row_metrics = [formats[metric] % yolo.metrics.get(metric, <span class="number">0</span>) <span class="keyword">for</span> yolo <span class="keyword">in</span> model.yolo_layers]</span><br><span class="line">                metric_table += [[metric, *row_metrics]]</span><br><span class="line"> </span><br><span class="line">                <span class="comment"># Tensorboard logging</span></span><br><span class="line">                tensorboard_log = []</span><br><span class="line">                <span class="keyword">for</span> j, yolo <span class="keyword">in</span> enumerate(model.yolo_layers):</span><br><span class="line">                    <span class="keyword">for</span> name, metric <span class="keyword">in</span> yolo.metrics.items():</span><br><span class="line">                        <span class="keyword">if</span> name != <span class="string">"grid_size"</span>:</span><br><span class="line">                            tensorboard_log += [(<span class="string">f"<span class="subst">&#123;name&#125;</span>_<span class="subst">&#123;j+<span class="number">1</span>&#125;</span>"</span>, metric)]</span><br><span class="line">                tensorboard_log += [(<span class="string">"loss"</span>, loss.item())]</span><br><span class="line">                logger.list_of_scalars_summary(tensorboard_log, batches_done)</span><br><span class="line"> </span><br><span class="line">            log_str += AsciiTable(metric_table).table</span><br><span class="line">            log_str += <span class="string">f"\nTotal loss <span class="subst">&#123;loss.item()&#125;</span>"</span></span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Determine approximate time left for epoch</span></span><br><span class="line">            epoch_batches_left = len(dataloader) - (batch_i + <span class="number">1</span>)</span><br><span class="line">            time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + <span class="number">1</span>))</span><br><span class="line">            log_str += <span class="string">f"\n---- ETA <span class="subst">&#123;time_left&#125;</span>"</span></span><br><span class="line"> </span><br><span class="line">            print(log_str)</span><br><span class="line"> </span><br><span class="line">            model.seen += imgs.size(<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> epoch % opt.evaluation_interval == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\n---- Evaluating Model ----"</span>)</span><br><span class="line">            <span class="comment"># Evaluate the model on the validation set</span></span><br><span class="line">            precision, recall, AP, f1, ap_class = evaluate(</span><br><span class="line">                model,</span><br><span class="line">                path=valid_path,</span><br><span class="line">                iou_thres=<span class="number">0.5</span>,</span><br><span class="line">                conf_thres=<span class="number">0.5</span>,</span><br><span class="line">                nms_thres=<span class="number">0.5</span>,</span><br><span class="line">                img_size=opt.img_size,</span><br><span class="line">                batch_size=<span class="number">8</span>,</span><br><span class="line">            )</span><br><span class="line">            evaluation_metrics = [</span><br><span class="line">                (<span class="string">"val_precision"</span>, precision.mean()),</span><br><span class="line">                (<span class="string">"val_recall"</span>, recall.mean()),</span><br><span class="line">                (<span class="string">"val_mAP"</span>, AP.mean()),</span><br><span class="line">                (<span class="string">"val_f1"</span>, f1.mean()),</span><br><span class="line">            ]</span><br><span class="line">            logger.list_of_scalars_summary(evaluation_metrics, epoch)</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Print class APs and mAP</span></span><br><span class="line">            ap_table = [[<span class="string">"Index"</span>, <span class="string">"Class name"</span>, <span class="string">"AP"</span>]]</span><br><span class="line">            <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(ap_class):</span><br><span class="line">                ap_table += [[c, class_names[c], <span class="string">"%.5f"</span> % AP[i]]]</span><br><span class="line">            print(AsciiTable(ap_table).table)</span><br><span class="line">            print(<span class="string">f"---- mAP <span class="subst">&#123;AP.mean()&#125;</span>"</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> epoch % opt.checkpoint_interval == <span class="number">0</span>:</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">f"checkpoints/yolov3_ckpt_%d.pth"</span> % epoch)</span><br></pre></td></tr></table></figure><h5 id="展示训练进度"><a href="#展示训练进度" class="headerlink" title="展示训练进度"></a>展示训练进度</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log_str = <span class="string">"\n---- [Epoch %d/%d, Batch %d/%d] ----\n"</span> % (epoch, opt.epochs, batch_i, len(dataloader))</span><br><span class="line">            metric_table = [[<span class="string">"Metrics"</span>, *[<span class="string">f"YOLO Layer <span class="subst">&#123;i&#125;</span>"</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model.yolo_layers))]]]</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200924155836773.png#pic_center" alt="在这里插入图片描述"></p><h5 id="获取指标"><a href="#获取指标" class="headerlink" title="获取指标"></a>获取指标</h5><p>从<strong>metrics</strong>中获取指标类型，并保存到<strong>format</strong>中。<br><img src="https://img-blog.csdnimg.cn/20200924160004857.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>下一步便通过for循环获取3个yolo层的各项指标，如grid_size、loss、坐标等。并保存在metric_table列表中：<br><img src="https://img-blog.csdnimg.cn/20200924160259886.png#pic_center" alt="在这里插入图片描述"><br>并通过以下代码解析yolo层的参数，放进列表tensorboard_log中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensorboard_log = []</span><br><span class="line">                <span class="keyword">for</span> j, yolo <span class="keyword">in</span> enumerate(model.yolo_layers):</span><br><span class="line">                    <span class="keyword">for</span> name, metric <span class="keyword">in</span> yolo.metrics.items():</span><br><span class="line">                        <span class="keyword">if</span> name != <span class="string">"grid_size"</span>:</span><br><span class="line">                            tensorboard_log += [(<span class="string">f"<span class="subst">&#123;name&#125;</span>_<span class="subst">&#123;j+<span class="number">1</span>&#125;</span>"</span>, metric)]</span><br><span class="line">                tensorboard_log += [(<span class="string">"loss"</span>, loss.item())]</span><br><span class="line">                logger.list_of_scalars_summary(tensorboard_log, batches_done)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200924160407747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200924160418500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>使用log_str打印各项指标参数：<br><img src="https://img-blog.csdnimg.cn/20200924160453237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h5 id="评估训练情况"><a href="#评估训练情况" class="headerlink" title="评估训练情况"></a>评估训练情况</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">precision, recall, AP, f1, ap_class = evaluate(</span><br><span class="line">                model,</span><br><span class="line">                path=valid_path,</span><br><span class="line">                iou_thres=<span class="number">0.5</span>,</span><br><span class="line">                conf_thres=<span class="number">0.5</span>,</span><br><span class="line">                nms_thres=<span class="number">0.5</span>,</span><br><span class="line">                img_size=opt.img_size,</span><br><span class="line">                batch_size=<span class="number">8</span>,</span><br><span class="line">            )</span><br></pre></td></tr></table></figure><p>使用<strong>evaluate函数</strong>得到各项指标，evaluate函数完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, path, iou_thres, conf_thres, nms_thres, img_size, batch_size)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Get dataloader</span></span><br><span class="line">    dataset = ListDataset(path, img_size=img_size, augment=<span class="literal">False</span>, multiscale=<span class="literal">False</span>)</span><br><span class="line">    dataloader = torch.utils.data.DataLoader(</span><br><span class="line">        dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">1</span>, collate_fn=dataset.collate_fn</span><br><span class="line">    )</span><br><span class="line"> </span><br><span class="line">    Tensor = torch.cuda.FloatTensor <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line"> </span><br><span class="line">    labels = []</span><br><span class="line">    sample_metrics = []  <span class="comment"># List of tuples (TP, confs, pred)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch_i, (_, imgs, targets) <span class="keyword">in</span> enumerate(tqdm.tqdm(dataloader, desc=<span class="string">"Detecting objects"</span>)):</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Extract labels</span></span><br><span class="line">        labels += targets[:, <span class="number">1</span>].tolist()</span><br><span class="line">        <span class="comment"># Rescale target</span></span><br><span class="line">        targets[:, <span class="number">2</span>:] = xywh2xyxy(targets[:, <span class="number">2</span>:])</span><br><span class="line">        targets[:, <span class="number">2</span>:] *= img_size</span><br><span class="line"> </span><br><span class="line">        imgs = Variable(imgs.type(Tensor), requires_grad=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            outputs = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)</span><br><span class="line"> </span><br><span class="line">        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Concatenate sample statistics</span></span><br><span class="line">    true_positives, pred_scores, pred_labels = [np.concatenate(x, <span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> list(zip(*sample_metrics))]</span><br><span class="line">    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> precision, recall, AP, f1, ap_class</span><br></pre></td></tr></table></figure><p>这段代码思路很清晰，加载数据和标签，这句代码是上段的核心：<code>sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)</code>。<br>评估的时候主要需要2个值，1、样本标注值。2、模型输出值。</p><p>1、<strong>样本的标注值</strong>。为了方便理解，这里简单回顾一下：voclabel.py会生成标注文件，保存在xxxx.txt文件中，每个.txt文件中的内容为了不混淆，我们称之为boxes，其boxes=【class id, x, y, w, h】按这种形式进行保存的。在ListDataset类中的<strong>getitem</strong>函数，会读取这个boxes，并把它从x，y，w，h（已经归一化成0~1）转换成对应特征图大小下的x，y，w，h的形式，并保存为targets。（ targets = torch.zeros((len(boxes), 6))   ；targets[:, 1:] = boxes ）。不过在评估的时候，为了方便计算IOU值，把target的坐标从x，y，w，h转换到xmin，ymin，xmax，ymax。</p><p>2、<strong>模型输出值</strong>。模型的输出output的shape为【batch_size,10647,5+class】。经过非极大值抑制处理之后，outputs的变成了一个列表，根据非极大值抑制处理的说明<code>Returns detections with shape: (x1, y1, x2, y2, object_conf, class_score, class_pred)</code>，output变成了一个列表，长度为batch_size(下图设置的是8)，可以看到每一个列表元素对应的张量的shape都是不一样的，这是因为每一张图片经过非极大值抑制处理之后剩下的boxes是不一样的，即tensor.shape(0)是不一样的，但tensor.shape(1)均为7，对应的是<code>(x1, y1, x2, y2, object_conf, class_score, class_pred)</code>。<br><img src="https://img-blog.csdnimg.cn/20200924161757438.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200924161814266.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200924161920323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>同时使用了<strong>get_batch_statistics</strong>函数，获取测试样本的各项指标。结合下面代码，不难理解。其完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch_statistics</span><span class="params">(outputs, targets, iou_threshold)</span>:</span></span><br><span class="line">    <span class="string">""" Compute true positives, predicted scores and predicted labels per sample """</span></span><br><span class="line">    batch_metrics = []</span><br><span class="line">    <span class="keyword">for</span> sample_i <span class="keyword">in</span> range(len(outputs)):</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> outputs[sample_i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line">        output = outputs[sample_i]</span><br><span class="line">        pred_boxes = output[:, :<span class="number">4</span>]</span><br><span class="line">        pred_scores = output[:, <span class="number">4</span>]</span><br><span class="line">        pred_labels = output[:, <span class="number">-1</span>]</span><br><span class="line"> </span><br><span class="line">        true_positives = np.zeros(pred_boxes.shape[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#这句把对应ID下的target和图像进行匹配，使用collate_fn函数给target赋予ID。</span></span><br><span class="line">        annotations = targets[targets[:, <span class="number">0</span>] == sample_i][:, <span class="number">1</span>:]</span><br><span class="line">        target_labels = annotations[:, <span class="number">0</span>] <span class="keyword">if</span> len(annotations) <span class="keyword">else</span> []</span><br><span class="line">        <span class="keyword">if</span> len(annotations):</span><br><span class="line">            detected_boxes = []</span><br><span class="line">            target_boxes = annotations[:, <span class="number">1</span>:]</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">for</span> pred_i, (pred_box, pred_label) <span class="keyword">in</span> enumerate(zip(pred_boxes, pred_labels)):</span><br><span class="line"> </span><br><span class="line">                <span class="comment"># If targets are found break</span></span><br><span class="line">                <span class="keyword">if</span> len(detected_boxes) == len(annotations):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">                <span class="comment"># Ignore if label is not one of the target labels</span></span><br><span class="line">                <span class="keyword">if</span> pred_label <span class="keyword">not</span> <span class="keyword">in</span> target_labels:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line">                iou, box_index = bbox_iou(pred_box.unsqueeze(<span class="number">0</span>), target_boxes).max(<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">if</span> iou &gt;= iou_threshold <span class="keyword">and</span> box_index <span class="keyword">not</span> <span class="keyword">in</span> detected_boxes:</span><br><span class="line">                    true_positives[pred_i] = <span class="number">1</span></span><br><span class="line">                    detected_boxes += [box_index]</span><br><span class="line">        batch_metrics.append([true_positives, pred_scores, pred_labels])</span><br><span class="line">    <span class="keyword">return</span> batch_metrics</span><br></pre></td></tr></table></figure><p>回到evaluate函数：<br><strong>precision, recall, AP, f1, ap_class</strong>值则是使用<strong>ap_per_class</strong>函数进行计算，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ap_per_class</span><span class="params">(tp, conf, pred_cls, target_cls)</span>:</span></span><br><span class="line">    <span class="string">""" Compute the average precision, given the recall and precision curves.</span></span><br><span class="line"><span class="string">    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.</span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        tp:    True positives (list).</span></span><br><span class="line"><span class="string">        conf:  Objectness value from 0-1 (list).</span></span><br><span class="line"><span class="string">        pred_cls: Predicted object classes (list).</span></span><br><span class="line"><span class="string">        target_cls: True object classes (list).</span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        The average precision as computed in py-faster-rcnn.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Sort by objectness</span></span><br><span class="line">    i = np.argsort(-conf)</span><br><span class="line">    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Find unique classes</span></span><br><span class="line">    unique_classes = np.unique(target_cls)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Create Precision-Recall curve and compute AP for each class</span></span><br><span class="line">    ap, p, r = [], [], []</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> tqdm.tqdm(unique_classes, desc=<span class="string">"Computing AP"</span>):</span><br><span class="line">        i = pred_cls == c</span><br><span class="line">        n_gt = (target_cls == c).sum()  <span class="comment"># Number of ground truth objects</span></span><br><span class="line">        n_p = i.sum()  <span class="comment"># Number of predicted objects</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> n_p == <span class="number">0</span> <span class="keyword">and</span> n_gt == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> n_p == <span class="number">0</span> <span class="keyword">or</span> n_gt == <span class="number">0</span>:</span><br><span class="line">            ap.append(<span class="number">0</span>)</span><br><span class="line">            r.append(<span class="number">0</span>)</span><br><span class="line">            p.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Accumulate FPs and TPs</span></span><br><span class="line">            fpc = (<span class="number">1</span> - tp[i]).cumsum()</span><br><span class="line">            tpc = (tp[i]).cumsum()</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Recall</span></span><br><span class="line">            recall_curve = tpc / (n_gt + <span class="number">1e-16</span>)</span><br><span class="line">            r.append(recall_curve[<span class="number">-1</span>])</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># Precision</span></span><br><span class="line">            precision_curve = tpc / (tpc + fpc)</span><br><span class="line">            p.append(precision_curve[<span class="number">-1</span>])</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># AP from recall-precision curve</span></span><br><span class="line">            ap.append(compute_ap(recall_curve, precision_curve))</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Compute F1 score (harmonic mean of precision and recall)</span></span><br><span class="line">    p, r, ap = np.array(p), np.array(r), np.array(ap)</span><br><span class="line">    f1 = <span class="number">2</span> * p * r / (p + r + <span class="number">1e-16</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> p, r, ap, f1, unique_classes.astype(<span class="string">"int32"</span>)</span><br></pre></td></tr></table></figure><p>最后，在训练到一定程度的时候，便保存模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % opt.checkpoint_interval == <span class="number">0</span>:</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">f"checkpoints/yolov3_ckpt_%d.pth"</span> % epoch)</span><br></pre></td></tr></table></figure><p>以上，train.py基本分析完毕。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后的最后，真的是最后了！总结一下！</p><h4 id="Yolo-v3-Structure"><a href="#Yolo-v3-Structure" class="headerlink" title="Yolo_v3_Structure"></a>Yolo_v3_Structure</h4><p><img src="https://img-blog.csdnimg.cn/20200924163036860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>（图借鉴自<a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a>）</p><h4 id="检测流程（detect-py"><a href="#检测流程（detect-py" class="headerlink" title="检测流程（detect.py)"></a>检测流程（detect.py)</h4><p><img src="https://img-blog.csdnimg.cn/20200924163311998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="训练流程（train-py"><a href="#训练流程（train-py" class="headerlink" title="训练流程（train.py)"></a>训练流程（train.py)</h4><p><img src="https://img-blog.csdnimg.cn/2020092416341434.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="损失函数（train-py"><a href="#损失函数（train-py" class="headerlink" title="损失函数（train.py)"></a>损失函数（train.py)</h4><p><img src="https://img-blog.csdnimg.cn/20200924163603334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><p>搭建YOLOv3入门教程  <a href="https://link.zhihu.com/?target=https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/">https://link.zhihu.com/?target=https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/</a></p></li><li><p>How to implement a YOLO (v3) object detector from scratch in <a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/" target="_blank" rel="noopener">PyTorchhttps://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/</a><br>附中文翻译：<a href="https://link.zhihu.com/?target=https://www.jiqizhixin.com/articles/2018-04-23-3">上部分</a> <a href="https://link.zhihu.com/?target=https://www.jiqizhixin.com/articles/042602?from=synced&keyword=%25E4%25BB%258E%25E9%259B%25B6%25E5%25BC%2580%25E5%25A7%258BPyTorch%25E9%25A1%25B9%25E7%259B%25AE%25EF%25BC%259AYOLO%2520v3%25E7%259B%25AE%25E6%25A0%2587%25E6%25A3%2580%25E6%25B5%258B%25E5%25AE%259E%25E7%258E%25B0">下部分</a></p></li><li><p>Pytorch | yolov3原理及代码详解(有四个系列，文章里面有链接）<a href="https://blog.csdn.net/qq_24739717/article/details/96705055" target="_blank" rel="noopener">https://blog.csdn.net/qq_24739717/article/details/96705055</a></p></li><li><p>超详细的Pytorch版yolov3代码中文注释详解 <a href="https://zhuanlan.zhihu.com/p/49981816" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49981816</a></p></li><li><p>史上最详细的Yolov3边框预测分析 <a href="https://blog.csdn.net/qq_34199326/article/details/84109828" target="_blank" rel="noopener">https://blog.csdn.net/qq_34199326/article/details/84109828</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PyTorch-YOLOv3项目训练与代码学习&quot;&gt;&lt;a href=&quot;#PyTorch-YOLOv3项目训练与代码学习&quot; class=&quot;headerlink&quot; title=&quot;PyTorch-YOLOv3项目训练与代码学习&quot;&gt;&lt;/a&gt;PyTorch-YOLOv3项目训
      
    
    </summary>
    
      <category term="大创学习体会" scheme="http://yoursite.com/categories/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="PyTorch-YOLOv3" scheme="http://yoursite.com/tags/PyTorch-YOLOv3/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达神经网络与深度学习课程笔记Week1：Introduction</title>
    <link href="http://yoursite.com/2020/09/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0Week1%EF%BC%9AIntroduction%20/"/>
    <id>http://yoursite.com/2020/09/07/吴恩达神经网络与深度学习课程学习笔记Week1：Introduction /</id>
    <published>2020-09-07T05:48:55.000Z</published>
    <updated>2020-09-24T09:55:15.940Z</updated>
    
    <content type="html"><![CDATA[<p>课程观看链接：<a href="https://www.bilibili.com/video/av66314465?p=1&rt=V/ymTlOu4ow/y4xxNWPUZwpjr6c6HRj9SG3qbZNakJg=" target="_blank" rel="noopener">https://www.bilibili.com/video/av66314465?p=1&amp;rt=V%2FymTlOu4ow%2Fy4xxNWPUZwpjr6c6HRj9SG3qbZNakJg%3D</a><br>课程作业及更多资料的资源获取：<a href="https://blog.csdn.net/dadapongi6/article/details/105668394" target="_blank" rel="noopener">https://blog.csdn.net/dadapongi6/article/details/105668394</a></p><h2 id="What-is-a-Neural-Network"><a href="#What-is-a-Neural-Network" class="headerlink" title="What is a Neural Network?"></a>What is a Neural Network?</h2><p>以房价预测为例：<br>当给足够多的关于x,y的数据，即(x,y)训练样本，将会精确计算从x到y的精准映射函数。<br><img src="https://img-blog.csdnimg.cn/20200903163219425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning?"></a>What is Machine Learning?</h2><p>Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.<br>Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”<br>Example: playing checkers.<br>E = the experience of playing many games of checkers<br>T = the task of playing checkers.<br>P = the probability that the program will win the next game.<br>In general, any machine learning problem can be assigned to one of two broad classifications:<br>Supervised learning and Unsupervised learning.</p><p>机器学习的两种定义：</p><ul><li>在无需具体的编程规则的条件下，给予计算机以学习的能力</li><li>计算机程序从经验E中学习某些类型的任务T和性能度量P，如果它在任务T中的性能(以P度量)随着经验E的提高而提高<br><img src="https://img-blog.csdnimg.cn/20200903165003846.png#pic_center" alt="在这里插入图片描述"><h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h2>一些典型的神经网络<br><img src="https://img-blog.csdnimg.cn/20200903205222131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>被应用于结构化数据和非结构化数据<br><img src="https://img-blog.csdnimg.cn/20200903205432476.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><p><strong>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</strong><br><strong>Supervised learning problems are categorized into “regression” and “classification” problems.</strong> In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.<br><strong>Example 1:</strong><br>Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.<br>We could turn this example into a classification problem by instead making our output about whether the house “sells for more or less than the asking price.” Here we are classifying the houses based on price into two discrete categories.<br><strong>Example 2:</strong><br>(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture<br>(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.</p><p><img src="https://img-blog.csdnimg.cn/20200903210239344.jpg#pic_center" alt="在这里插入图片描述"></p><h2 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h2><p><strong>Unsupervised learning allows us to approach problems with little or no idea what our results should look like.</strong><br>We can <strong>derive structure from data</strong> where we don’t necessarily know the effect of the variables.<br>We can derive this structure by clustering the data based on relationships among the variables in the data.<br>With unsupervised learning there is no feedback based on the prediction results.<br><strong>Example:</strong><br>Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.<br>Non-clustering: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).</p><p><img src="https://img-blog.csdnimg.cn/202009032106526.jpg#pic_center" alt="在这里插入图片描述"><br>印象非常深的话：<br>Thanks to deep learning, thanks to neural networks, computers are now much better at interpeting unstructured data as well compared to just a few years ago. And this creates opportunities for many new exciting applications that use speech recognition, image recognition, natural language processing on text, much more than was possible even just two or three years ago.</p><h2 id="What-drives-deep-learning-progress"><a href="#What-drives-deep-learning-progress" class="headerlink" title="What drives deep learning progress"></a>What drives deep learning progress</h2><p><strong>Scale drives deep learning progress.</strong><br>规模包括神经网络（一个有许多隐藏单元，许多参数，许多连接的神经网络）的规模，数据的规模。</p><p><img src="https://img-blog.csdnimg.cn/20200903212228304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>:</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;课程观看链接：&lt;a href=&quot;https://www.bilibili.com/video/av66314465?p=1&amp;rt=V/ymTlOu4ow/y4xxNWPUZwpjr6c6HRj9SG3qbZNakJg=&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
      <category term="吴恩达神经网络与深度学习课程笔记" scheme="http://yoursite.com/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文准备之研读论文</title>
    <link href="http://yoursite.com/2020/07/05/%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87%E4%B9%8B%E7%A0%94%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    <id>http://yoursite.com/2020/07/05/论文准备之研读论文/</id>
    <published>2020-07-05T11:12:55.000Z</published>
    <updated>2020-07-05T11:12:43.937Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Refinedbox-Refining-for-fewer-and-high-quality-object-proposals"><a href="#Refinedbox-Refining-for-fewer-and-high-quality-object-proposals" class="headerlink" title="Refinedbox: Refining for fewer and high-quality object proposals"></a>Refinedbox: Refining for fewer and high-quality object proposals</h2><p>原论文链接：<a href="https://www.sciencedirect.com/science/article/pii/S0925231220305816?dgcid=coauthor" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S0925231220305816?dgcid=coauthor</a></p><p>从以下几个方面对论文进行研读：</p><ol><li>论文解决了什么任务</li><li>用了什么模型</li><li>主要创新点是什么</li><li>在什么数据集上实验</li><li>跟谁对比</li><li>结论<h2 id="论文解决了什么问题"><a href="#论文解决了什么问题" class="headerlink" title="论文解决了什么问题"></a>论文解决了什么问题</h2></li></ol><blockquote><p>We are motivated by the fact that many traditional proposal methods generate dense proposals to cover as many objects as possible but that<br> i) they usually fail to rank theseproposals properly and<br>  ii) the number of proposals is very large.  </p></blockquote><p><strong>object proposal方法的缺陷</strong></p><ol><li>不能将候选框正确的排序</li><li>产生的候选数量庞大才能保证检测的精度<blockquote><p>Hence the sub-optimal proposal sampling strategies make<br>them difficult to fully leverage the powerful capability of CNNs. As<br>a result, the number of true objects (e.g. usually less than 10) in<br>an image is still much smaller than the number of proposals generated by these deep-based methods (e.g. usually a few hundred)</p></blockquote></li></ol><p><strong>deep-based methods存在的缺陷</strong><br>次优的候选抽样策略使它们难以充分利用CNN的强大功能，生成的候选数量远远大于实际的对象的数量。</p><h2 id="用了什么模型"><a href="#用了什么模型" class="headerlink" title="用了什么模型"></a>用了什么模型</h2><p><strong><em>related work</em></strong></p><blockquote><p>We broadly divide the related research into four parts: segmentation-based proposal generation methods, edgebased methods, CNN-based methods, and proposal post-processing methods.</p></blockquote><p><strong><em>segmentation-based proposal generation methods</em></strong><br>使用图像分割作为输入，并尝试找到这些图像分割的正确组合以覆盖所有完整的候选框。<br>这些方法通常结合了一些底层功能（例如 显着性，颜色，SIFT等）对边界框进行评分，然后选择分数较高的候选框。<br><strong>选择性搜索</strong>，最流行的产生候选框方法是利用穷举搜索和分割的优势，通过对超像素进行分层合并来获得高质量的候选框。<br><strong>MCG</strong>，介绍一种有效利用多尺度信息的高性能图像分割算法。通过探索组合空间，将生成的区域多尺度层次结构组合为候选框。<br><strong><em>edgebased methods</em></strong><br>利用观察自然图像中的完整物体通常具有明确的定义封闭边界。 近年来，几种有效的算法已经提出使用边缘特征。<br><strong>级联排序SVM（CSVM）方法</strong>，使用梯度特征获得候选框。<br><strong>BING算法</strong>，该算法通过量化CSVM以300fps的速度运行进行一些二进制运算。<br>基于闭合路径积分的闭合轮廓测量，边缘框根据数量计算客观分数全部包含在每个边界框中的轮廓。<br><strong><em>CNN-based methods</em></strong><br>利用CNN直接地产生目标候选，例如RPN，DeepMask和SharpMask。<br> <strong>RPN</strong>，同时预测对象边界和对象得分在每个位置全图像卷积特征。<br> <strong>DeepMask</strong>，经过联合培训有两个目标：给定图像补丁，系统首先输出与类无关的分割模板，然后输出补丁集中在整个对象上的可能性。<br><strong>SharpMask</strong>，候选中增加前馈网络用于对象分割，具有新颖的自上而下的细化方法。 由此产生的自下而上/自上而下的体系结构能够有效地生成highly object mask。<br><strong><em>proposal post-processing methods</em></strong><br>目标在于准确定位图像中的对象。 基于已经提出的一个名为DeepBox的小型神经网络，并对它优化改进。</p><p>对以上的OP方法进行拓展学习并总结：<br><img src="https://img-blog.csdnimg.cn/20200704015911515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>马克斯普朗克研究所（<a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/object-recognition-and-scene-understanding/" target="_blank" rel="noopener">max planck institute</a>，有一项叫<a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/object-recognition-and-scene-understanding/how-good-are-detection-proposals-really/" target="_blank" rel="noopener">What makes for effective detection proposals?</a>的研究，综合分析了各种OP的性能表现！<br><strong>在复现能力上</strong>，作者认为Bing和EdgeBoxes俩算法更好。原因可能是这俩算法都使用了SVM。另外，作者还认为超像素（superpixels）的灵敏度对图像的扰动是一些OP算法复现能力下降的主要原因！<br><strong>在召回能力上</strong>，（什么是召回？召回率Recall：正确的结果有多少被识别出来了，可阅读文章<a href="https://blog.csdn.net/plsong_csdn/article/details/89476696" target="_blank" rel="noopener">目标检测问题中的“召回率Recall”、“精确率Precision”</a>）<br>还是直接上结论：<br>MCG， EdgeBox，SelectiveSearch, Rigor和Geodesic在不同proposal数目下表现都不错<br>如果只限制小于1000的proposal，MCG,endres和CPMC效果最好<br>如果一开始没有较好地定位好候选框的位置，随着IoU标准严格，recall会下降比较快的包括了Bing, Rahtu, Objectness和Edgeboxes。其中Bing下降尤为明显。<br>在AR这个标准下，MCG表现稳定；Endres和Edgeboxes在较少proposal时候表现比较好，当允许有较多的proposal时候，Rigor和SelectiveSearch的表现会比其他要好。<br>PASCAL和ImageNet上，各个OP方法都是比较相似的，这说明了这些OP方法的泛化性能都不错。<br>参考自博客<a href="https://blog.csdn.net/guoyunfei20/article/details/78739168" target="_blank" rel="noopener">Object Proposal（OP）综述</a></p><p>而在本文中，作者建立了一个细化的网络架构来细化现有的边界框，RefinedBox具有最先进的性能来用于对象候选生成评估和对象检测评价。<br><strong><em>RefinedBox</em></strong><br><img src="https://img-blog.csdnimg.cn/20200705153302573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><blockquote><p>Our method takes the object proposals produced by other proposal generation methods as input and then tries to refine them.The refinement is twofold:<br>re-ranking and box regression.<br>To rerank the existing boxes, we recompute the objectness score for each box using the semantic information in the deep neural network.<br>To obtain the box regression, the network is designed to learn the regressions of the center coordinates, width, and height for each box</p></blockquote><p><strong>基于VGG16 (含13个卷积层和3个全连接层) 构建神经网络，以自然图像和由其他对象候选框生成方法产生的initial boxes作为输入。</strong><br>主要采用的两个改进方法：</p><ul><li>re-ranking 重排列 </li><li>box regression 边框回归 ：重新调整候选框的形状和位置，以便更加紧密的覆盖真实物体</li></ul><p>逐层分析</p><ul><li>Convolutional layer 卷积层：卷积层与内核连接，第13个卷积层后的大小 3x3，通道数减少512-&gt;128 </li><li>ROI Pooling 池化层 ：将每个初始框区域采样为固定的特征图大小，7×7。ROI将输入的要素图划分为网格具有相同的宽度和高度，并在每个格中进行最大池化</li><li>FC全连接层： 512个输出神经元 卷积层、全连接层后加上ReLU层（ReLU激活函数）<br><em>最后使用ranking和box regression两个分支来进行对象得分 (objectness score) 的重新计算以及对任意initial boxes位置偏移量的获取</em></li><li>ranking 分支： 是输出为两个神经元完全连接层，表示是否是目标对象的概率</li><li>box reg ：与坐标偏移相关</li></ul><p>具体实现的损失函数：</p><blockquote><p>In the training of RefinedBox, each initial box is assigned a binary class label of being an object or not. The loss function can be written as<br><img src="https://img-blog.csdnimg.cn/20200705175245888.png" alt="在这里插入图片描述"><br>where p is computed by a softmax over the two outputs of a fully connected layer and u is the label of this box (1 or 0). The box regression layer is a fully connected layer which is designed to learn the coordinate offsets. We perform the parameterizations of four coordinates as following:<br><img src="https://img-blog.csdnimg.cn/20200705175354228.png" alt="在这里插入图片描述"><br>where x, y, w, and h represent the coordinates of the box center, width, andheight,respectively.Variables x, xin, and x∗ are for the predicted box, input box, and ground truth box, respectively; similar definitions hold for y, w, and h. Hence variables v is the regression target and t is the predicted tuple.<br>The box regression loss is defined as<br><img src="https://img-blog.csdnimg.cn/20200705174934267.png" alt="在这里插入图片描述"><br>Thus the joint loss function can be written as<br><img src="https://img-blog.csdnimg.cn/20200705175025386.png" alt="在这里插入图片描述"><br>in which the parameter λ is a balance parameter, and we set it as 1 in this paper.</p></blockquote><p><strong>joint training</strong></p><blockquote><p>The branch of RefinedBox is designed to refine the initial boxes, then the refined boxes are inputted into the branch of Fast R-CNN for classification.<br>…<br>we connect the well-known detection framework, Fast R-CNN, after the convolutional layers as a parallel branch to RefinedBox. The refined proposals produced by the RefinedBox branch are inputted into Fast R-CNN.<br>…<br>Fast R-CNN, to evaluate the quality of proposals in object detection. Our experiments demonstrate that our method can generate high-quality proposals for object detection with good efficiency.</p></blockquote><p>将Fast R-CNN作为基础卷积层后与RefinedBox平行的分支，经过RefinedBox精炼后的候选们将会作为Fast R-CNN的输入，以此评估对象检测中候选的质量。</p><h2 id="主要创新点是什么"><a href="#主要创新点是什么" class="headerlink" title="主要创新点是什么"></a>主要创新点是什么</h2><blockquote><p>To significantly reduce the number of proposals, we design a computationally lightweight neural network to refine the initial object proposals. <strong>The refinement consists of two parallel processes, re-ranking and box regression.</strong><br>…<br>To combine the superiority of traditional proposal methods and the powerful representation capability of CNNs, we <strong>propose a novel method to re-rank and align existing proposal boxes in a single inference of a neural network.</strong><br>…<br>The proposed network can share convolutional features with other high-level tasks by joint training, so the proposal refinement can be very fast.</p></blockquote><p><strong>Goal</strong>:we focus on mining the number of proposals while obtaining high detection recall—-在保持高检测召回率的前提下减少候选数量</p><p><strong>Solvement</strong>:轻量级的神经网络来精选最初的目标候选集，且提炼过程很迅速</p><ul><li>候选重排 - 根据覆盖完整对象的紧密程度对候选进行排序</li><li>边框回归 - 微调候选位置与形状，使其能更紧密地覆盖真实物体</li></ul><p>该神经网络能够通过联合训练与其他高层任务共享卷积特征，以提高候选的优化速度。<br><img src="https://img-blog.csdnimg.cn/20200705185805306.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="在什么数据集上实验"><a href="#在什么数据集上实验" class="headerlink" title="在什么数据集上实验"></a>在什么数据集上实验</h2><blockquote><p>We evaluate the proposed method on two widely used object detection datasets, including PASCAL VOC2007 and MS COCO.</p></blockquote><ol><li>PASCAL VOC2007<br>由训练集（2501幅训练图像及2510幅验证图像）和测试集（4952幅测试图像）组成，共包含20个种类</li></ol><blockquote><p>`aeroplane<br>bicycle<br>bird<br>boat<br>bottle<br>bus<br>car<br>cat<br>chair<br>cow<br>diningtable<br>dog<br>horse<br>motorbike<br>person<br>pottedplant<br>sheep<br>sofa<br>train<br>tvmonitor</p></blockquote><p>   在VOC2007训练验证集上训练RefinedBox，在VOC2007测试集上进行测试</p><ol start="2"><li>MS COCO<br>包含82,783幅训练图像和40,504幅验证图像，91种类别<br>采用其训练集训练RefinedBox，采用其验证集进行对候选(proposals)进行评估</li></ol><p><strong>两种数据集的对比</strong><br><img src="https://img-blog.csdnimg.cn/20200705183449521.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="跟谁对比"><a href="#跟谁对比" class="headerlink" title="跟谁对比"></a>跟谁对比</h2><p>与现有的主流proposal method进行对比。</p><ul><li>非基于深度学习的方法<br>BING, CSVM, Edge Boxes, Endres, GoP, LPO, MCG, Objectness, Rahtu, RandomPrim, Rantalankila, Selective Search</li><li>基于深度学习的方法<br>RPN, DeepBox, DeepMaskZoom, SharpMaskZoom</li></ul><p><strong>对比度量：</strong></p><blockquote><p>To evaluate the proposals, we adopt the metrics of object detection recall (DR), mean average best overlap (MABO), and average recall (AR)</p></blockquote><p>DR (对象检测召回率), MABO (所有类别的平均最高重叠率), AR(平均召回率)</p><p><strong>对比结果：</strong></p><ol><li><p>Evaluation of different refinement algorithms.<br>(proposals generated by Edge Boxes as input)<br><img src="https://img-blog.csdnimg.cn/20200705184348716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li><li><p>Evaluation results on the PASCAL VOC2007 test set.<br> (proposals generated by Edge Boxes as input)<br> <img src="https://img-blog.csdnimg.cn/20200705184512688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li><li><p>Evaluation results on the MS COCO validation set.<br> (RefinedBox uses RPN as inputs)<br> <img src="https://img-blog.csdnimg.cn/20200705184652786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li><li><p>Qualitative comparison for object detection using only top 10 proposals.<br>(proposals generated by Edge Boxes as input)</p></li></ol><blockquote><p>Compared with other proposal generation methods, RefinedBox can also achieve much higher detection performance. These evaluation results demonstrate that RefinedBox can generate a small amount of proposals with significantly high quality.</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20200705184827894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul><li>RefinedBox可以有效地精炼给定原始候选集，且因为用于精炼的网络易于被优化的特点，可以与后续的应用进行联合训练</li><li><strong>limitation:</strong><br>当初始的候选数目过多时会降低其效率(RefinedBox的效率与初始的候选数目呈负相关)；识别图片具有太多小的物体时会影响RefinedBox的性能</li><li><strong>future work</strong>:<br>在未标注的数据集上最小化候选个数，在更多高级的应用上使用RefinedBox</li></ul><blockquote><p>In the future, we plan to apply our refinement method to other high-level applications, e.g. mining knowledge from huge amounts of unlabeled data.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Refinedbox-Refining-for-fewer-and-high-quality-object-proposals&quot;&gt;&lt;a href=&quot;#Refinedbox-Refining-for-fewer-and-high-quality-object-pro
      
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="refined box" scheme="http://yoursite.com/tags/refined-box/"/>
    
  </entry>
  
  <entry>
    <title>大二下学期末总结</title>
    <link href="http://yoursite.com/2020/06/29/%E5%A4%A7%E4%BA%8C%E4%B8%8B%E5%AD%A6%E6%9C%9F%E6%9C%AB%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/06/29/大二下学期末总结/</id>
    <published>2020-06-29T09:17:55.000Z</published>
    <updated>2020-06-29T09:18:39.826Z</updated>
    
    <content type="html"><![CDATA[<h2 id="今日鸭汤"><a href="#今日鸭汤" class="headerlink" title="今日鸭汤"></a>今日鸭汤</h2><p>  这篇文章其实早就想写的，但是因为一些突如其来的事情有些耽误了，大二下的所有考试已经结束了，论文、大作业、答辩全部都结束了，回想一下疫情期间这在家度过的半学期，还是过得充实而忙碌的，好像因为有了想要到达但却不算太容易到达的目的地，而有了一直向前奔跑的动力，当然想要站的更高一定会更耗费力气，好像每个自己不足的地方都在去尝试，竞赛、科研，不管结果怎么样，我自己的专业知识上，团队协作上，沟通协调上都感觉自己得到了提高，收获了老师的肯定，收获了志同道合的小伙伴，收获了找到以后想要做什么的幸福感，收获了小成就带来的内心的丰盈感，这对我来说便是枉少年吧。<br>  “你有想过放弃吗”<br>  “有啊，一直坚持多累啊”<br>  “那怎么还坚持呢”<br>  “没办法，因为喜欢吧”<br>接下来进入正题吧，这学期的技术性很强，学到了很多开发工具的使用方法，在小组作业的时候努力去和组员们利用有限的时间进行更多有效沟通，共同学习共同进步，共同发现问题并且去解决，是非常有挑战性但也很快乐的事情。</p><h2 id="安卓开发"><a href="#安卓开发" class="headerlink" title="安卓开发"></a>安卓开发</h2><p>做了一个疫情期间比较方便查看新闻、收集信息的app for android，大致的界面如下：</p><p><img src="https://img-blog.csdnimg.cn/20200629114948123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">)<img src="https://img-blog.csdnimg.cn/20200629115040191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020062911520034.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200629115227764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200629115320156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在做这个app的时候，最大的感受就是细节决定成败，反复尝试一定会有一次会到达，永远相信“下一次”的幸运。</p><h2 id="虚拟现实VR"><a href="#虚拟现实VR" class="headerlink" title="虚拟现实VR"></a>虚拟现实VR</h2><p>用unity做了一个跳舞的线新关卡的新版本，主要参考的一些教程素材放在这里：<br><a href="https://www.bilibili.com/video/BV1Rs41157ta?from=search&seid=6307777384455021856" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Rs41157ta?from=search&amp;seid=6307777384455021856</a><br><a href="https://www.bilibili.com/video/BV1Fb411G77f?p=3" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Fb411G77f?p=3</a><br>没错，我认为B站就是一个用来学习的网站！<br>因为会涉及到我们的名字等信息，所以就不把视频放上来了，放一些场景的图：<br><img src="https://img-blog.csdnimg.cn/20200629121241223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200629121322854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200629121340346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200629121410618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200629121431771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>做这个游戏对我来说最大的收获大概就是完成了当初高考结束选择这个专业的初心吧，就是想要自己能做出来那些小时候爱玩的游戏，也希望自己做出的游戏能成为别人的童年回忆，前面一个完成了一点点，后面的可能还需要以后去做更多尝试吧。</p><h2 id="基于UML的软件设计"><a href="#基于UML的软件设计" class="headerlink" title="基于UML的软件设计"></a>基于UML的软件设计</h2><p>问题定义与可行性分析—-需求报告—-概要设计—-详细设计—-软件测试报告—-项目规划设计书<br>推荐processOn来画UML图，数据流图等等，有很多模板，可以很多人一起协作。<br>写了很多文档，但是u1s1，就是画图和写文档差点劝退我。</p><h2 id="Springboot-SpringCloud-Vue商城开发"><a href="#Springboot-SpringCloud-Vue商城开发" class="headerlink" title="Springboot+SpringCloud+Vue商城开发"></a>Springboot+SpringCloud+Vue商城开发</h2><p>这个因为时间问题开发的不算特别完备，但是学到了很多东西，觉得最难的是这种前后端分离的系统开发对接的时候，跨域等等很多问题，这段开发经历为自己以后要是接触这方面的前端后端系统都能够奠定了一点基础。不过在开发过程中确实也有一点小插曲，最后也总算是解决了，觉得是一段很独特的经历，也算是给自己买了个教训。</p><p>暑假开始了，但好像还是有很多事情要忙，给自己短暂的放个假，就好好充实自己的暑假吧。已经看到学校开学的日期了，很开心，真没想到回到学校我们都已经大三了，珍惜最后在学校待的两年吧，今年下半学期没回成学校觉得有点遗憾的是没能送别大四的学长学姐们毕业，祝她们前程似锦，一切都好吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;今日鸭汤&quot;&gt;&lt;a href=&quot;#今日鸭汤&quot; class=&quot;headerlink&quot; title=&quot;今日鸭汤&quot;&gt;&lt;/a&gt;今日鸭汤&lt;/h2&gt;&lt;p&gt;  这篇文章其实早就想写的，但是因为一些突如其来的事情有些耽误了，大二下的所有考试已经结束了，论文、大作业、答辩全部都结束了
      
    
    </summary>
    
      <category term="本科学期总结" scheme="http://yoursite.com/categories/%E6%9C%AC%E7%A7%91%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="大二下" scheme="http://yoursite.com/tags/%E5%A4%A7%E4%BA%8C%E4%B8%8B/"/>
    
  </entry>
  
  <entry>
    <title>build your blog</title>
    <link href="http://yoursite.com/2020/06/28/how%20to%20build%20your%20blog_/"/>
    <id>http://yoursite.com/2020/06/28/how to build your blog_/</id>
    <published>2020-06-28T15:35:55.000Z</published>
    <updated>2020-06-28T15:36:25.790Z</updated>
    
    <content type="html"><![CDATA[<h2 id="用hexo搭建你的blog"><a href="#用hexo搭建你的blog" class="headerlink" title="用hexo搭建你的blog"></a>用hexo搭建你的blog</h2><p>refenrence：<br>build bog:<a href="https://www.jianshu.com/p/ef3f221a0374" target="_blank" rel="noopener">https://www.jianshu.com/p/ef3f221a0374</a><br>change theme:<a href="https://www.jianshu.com/p/869c6bcd0647" target="_blank" rel="noopener">https://www.jianshu.com/p/869c6bcd0647</a><br>关联域名：<a href="https://zhuanlan.zhihu.com/p/22183337" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22183337</a></p><h2 id="我的blog"><a href="#我的blog" class="headerlink" title="我的blog"></a>我的blog</h2><p>之前一直忘记放自己的blog地址了，其实会跟csdn同步更新:<br><a href="https://phoebeyu731.github.io/" target="_blank" rel="noopener">Phoebe’s blog</a><br>我用的是archer主题，如果想看看这个主题的搭建效果的，欢迎来我的blog参观！</p><h2 id="给blog加分类和标签"><a href="#给blog加分类和标签" class="headerlink" title="给blog加分类和标签"></a>给blog加分类和标签</h2><p>refenrence：<br><a href="https://blog.csdn.net/smileyan9/article/details/86666602" target="_blank" rel="noopener">https://blog.csdn.net/smileyan9/article/details/86666602</a><br>我的总结如下：<br>  其实本来我用的archer主题就是带有tag和category的页面的，如下图：<br>  <img src="https://img-blog.csdnimg.cn/20200628231229402.png" alt="在这里插入图片描述"><br>  所以就很好办了，如果在自己的主题里没有看到有自带这个，可以检查自己的_config.yml文件<br>中相关项的值有没有设为true，如下图：<br><img src="https://img-blog.csdnimg.cn/20200628231600155.png" alt="在这里插入图片描述"><br>检查好了之后，自己的每篇要push上去的文章在开头都加上你想要的加的tags和categories，如下图：<br><img src="https://img-blog.csdnimg.cn/20200628232004619.png" alt="在这里插入图片描述"><br>再直接按照命令更新上去就可以啦。</p><h2 id="常用的command命令"><a href="#常用的command命令" class="headerlink" title="常用的command命令"></a>常用的command命令</h2><p>push：hexo d -g<br>run at localhost：hexo s（有时候远程不能很快就更新，可以先在本地运行，就能看见远程的更新了）<br>create new blog：hexo new “file name”<br>refenrence：<a href="https://blog.csdn.net/weixin_34239592/article/details/91448749" target="_blank" rel="noopener">https://blog.csdn.net/weixin_34239592/article/details/91448749</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;用hexo搭建你的blog&quot;&gt;&lt;a href=&quot;#用hexo搭建你的blog&quot; class=&quot;headerlink&quot; title=&quot;用hexo搭建你的blog&quot;&gt;&lt;/a&gt;用hexo搭建你的blog&lt;/h2&gt;&lt;p&gt;refenrence：&lt;br&gt;build bog:&lt;
      
    
    </summary>
    
      <category term="小白的技术成长之路" scheme="http://yoursite.com/categories/%E5%B0%8F%E7%99%BD%E7%9A%84%E6%8A%80%E6%9C%AF%E6%88%90%E9%95%BF%E4%B9%8B%E8%B7%AF/"/>
    
    
      <category term="hexo blog" scheme="http://yoursite.com/tags/hexo-blog/"/>
    
  </entry>
  
  <entry>
    <title>大创学习记录（二）</title>
    <link href="http://yoursite.com/2020/05/06/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2020/05/06/大创学习记录（二）/</id>
    <published>2020-05-06T13:34:55.000Z</published>
    <updated>2020-06-28T14:53:43.756Z</updated>
    
    <content type="html"><![CDATA[<p>  五一假期忙忙碌碌就快过去了，虽然也不知道具体自己都在忙些什么，但是还是每天都有每天的收获，很充实，我常常在想人为什么需要假期，忙着奔跑之后的停顿可能也会带来一些小惊喜，对我来说，可能是去野餐瞥见阳光下的夏天的味道让人沉溺的只想睡去，可能是去串门发现高二的妹妹满墙的公式单词和“鸡汤”拼命地把我往回忆里拽，原来那时看起来那么幼稚的举动竟然真的能给现在的我这么大的力量……哈哈哈还是改不了一开始在键盘上敲字就要写小作文的习惯，不多说啦，假期的最后一天，记录一下大创学习的内容。‘<br>  老师在五一放假前请研究生学姐来给我们简要的介绍了一下深度学习LeNet-5的一些架构，当时来说还不是很明白，所以假期期间去看了老师推荐的一些教学视频和博客，再来理解学姐给我们的动手去跑一跑的代码，并且记录如下：<br>  对于卷积和池化的操作都比较简单了，卷积主要是做特征提取后向量的一些內积，计算方式可以用下图来表示<br> <img src="https://img-blog.csdnimg.cn/20200505172536282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>有几个符号及计算关系需要记住：<br>H：图片高度；<br>W：图片宽度；<br>D：原始图片通道数，也是卷积核个数；<br>F：卷积核高宽大小；<br>P：图像边扩充大小；<br>S：滑动步长。<br>在上图中，H=5,W=5,D=3，P=1，经过卷积操作得到输出为3×3×2（用Ho×Wo×K表示）大小的特征图，即3×3大小的2通道特征图，其中Ho计算公式为：Ho=(H−F+2×P)/S+1，Wo同理。<br>池化则是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。通常在卷积层的后面会加上一个池化层。池化包括最大池化、平均池化等。其中最大池化是用不重叠的矩形框将输入层分成不同的区域，对于每个矩形框的数取最大值作为输出层，是比较常采用的形式。<br>然后有一个LeNet5网络图比较有助于理解，一层一层的去剖析：<br><img src="https://img-blog.csdnimg.cn/20200505175038614.png" alt="在这里插入图片描述"><br><strong>1、INPUT层-输入层</strong><br>首先是数据 INPUT 层，输入图像的尺寸统一归一化为32×32。<br><strong>本层是不算LeNet-5的网络结构，传统上，不将输入层视为网络层次结构之一。</strong><br><strong>2、**</strong>C1层-卷积层<em>*<br>输入图片：32×32<br>卷积核大小：5×5<br>卷积核种类：6<br>输出featuremap大小：28×28 （32-5+1）=28<br>神经元数量：28×28×6<br>可训练参数：（5×5+1) ×6（每个滤波器5×5=25个unit参数和一个bias参数，一共6个滤波器）<br>连接数：（5×5+1）×6×28×28=122304<br><strong>3、S2层-池化层（下采样层）</strong><br>输入：28×28<br>采样区域：2×2<br>采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid<br>采样种类：6<br>输出featureMap大小：14×14（28/2）<br>神经元数量：14×14×6<br>连接数：（2×2+1）<em>6×14×14，S2中每个特征图的大小是C1中特征图大小的1/4。<br>*</em>4、C3层-卷积层**<br>输入：S2中所有6个或者几个特征map组合卷积核大小：5×5<br>卷积核种类：16<br>输出featureMap大小：10×10 (14-5+1)=10<br>C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合<br>存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。<br>则：可训练参数：6×(3×5×5+1)+6×(4×5×5+1)+3×(4×5×5+1)+1×(6×5×5+1)=1516<br>连接数：10×10×1516=151600<br><strong>详细说明</strong>：第一次池化之后是第二次卷积，第二次卷积的输出是C3，16个10x10的特征图，卷积核大小是 5×5. 我们知道S2 有6个 14×14 的特征图，怎么从6 个特征图得到 16个特征图了？ 这里是通过对S2 的特征图特殊组合计算得到的16个特征图。具体如下：<br><img src="https://img-blog.csdnimg.cn/20200505181950117.png" alt="在这里插入图片描述"><br><strong>之前这里一直不是特别懂，现在理解了！</strong>C3的前6个feature map（对应上图第一个红框的6列）与S2层相连的任选3个feature map相连接（上图第一个红框），后面6个feature map与S2层相连的任选4个feature map相连接（上图第二个红框），后面3个feature map与S2层部分不相连的任选4个feature map相连接，最后一个与S2层的所有feature map相连。<br><img src="https://img-blog.csdnimg.cn/20200505182235180.png" alt="在这里插入图片描述"><br><strong>5、S4层-池化层（下采样层）</strong><br>输入：10×10<br>采样区域：2×2<br>采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid<br>采样种类：16<br>输出featureMap大小：5×5（10/2）<br>神经元数量：5×5×16=400<br>连接数：16×（2×2+1）×5×5=2000<br>S4中每个特征图的大小是C3中特征图大小的1/4<br><strong>6、C5层-卷积层</strong><br>输入：S4层的全部16个单元特征map（与s4全相连）<br>卷积核大小：5×5<br>卷积核种类：120<br>输出featureMap大小：1×1（5-5+1）<br>可训练参数/连接：120×（16×5×5+1）=48120<br>C5层的网络结构如下：<br><img src="https://img-blog.csdnimg.cn/20200505190115290.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>7、F6层-全连接层</strong><br>输入：c5 120维向量<br>计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过sigmoid函数输出。<br>可训练参数:84</em>(120+1)=10164<br><strong>详细说明</strong>：6层是全连接层。F6层有84个节点，对应于一个7x12的比特图，-1表示白色，1表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84=10164。ASCII编码图如下：<br><img src="https://img-blog.csdnimg.cn/20200505190255889.png" alt="在这里插入图片描述"><br>F6层的连接方式如下：<br><img src="https://img-blog.csdnimg.cn/2020050519032624.png" alt="在这里插入图片描述"><br><strong>8、Output层-全连接层</strong><br>Output层也是全连接层，共有10个节点，分别代表数字0到9，且如果节点i的值为0，则网络识别的结果是数字i。采用的是径向基函数（RBF）的网络连接方式。假设x是上一层的输入，y是RBF的输出，则RBF输出的计算方式是：<br><img src="https://img-blog.csdnimg.cn/20200505190421564.png" alt="在这里插入图片描述"></p><p>上式w_ij 的值由i的比特图编码确定，i从0到9，j取值从0到7*12-1。RBF输出的值越接近于0，则越接近于i，即越接近于i的ASCII编码图，表示当前网络输入的识别结果是字符i。该层有84x10=840个参数和连接。<br><img src="https://img-blog.csdnimg.cn/20200505190438571.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上图是LeNet-5识别数字3的过程。<br>以上参考自这篇博客。 <a href="https://cuijiahua.com/blog/2018/01/dl_3.html" target="_blank" rel="noopener">LeNet5详解</a></p><p>把上述过程的一些参数的计算弄清楚之后，再去看代码。就会更好理解一些，但是在开始准备运行代码的时候，又遇到了一个障碍，学姐给的代码是pytorch环境的，而我之前搭的是tensorflow的，基于初学者，对于两者之间的转换可能不是很熟悉，想着以后反正都要用，就再去下载了一下pytorch。<br>过程如下：<br><strong><em>使用conda来安装pytorch</em></strong><br>在这里推荐一篇博客，安装过程写的非常详细。  <a href="https://blog.csdn.net/pw1623/article/details/90257347" target="_blank" rel="noopener">pytorch环境搭建</a></p><p>安装的命令同样可以登录pytorch的官网，然后选择和自己电脑配置以下的信息，会自动生成的对应conda命令</p><p><a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">https://pytorch.org/get-started/locally/</a><br><img src="https://img-blog.csdnimg.cn/20200505221837632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>如果不知道自己的各种安装的版本号，可以在anaconda navigator中查看，在Anaconda prompt中运行这句话之后，（我的是10.0）就会出现以下画面：<br><img src="https://img-blog.csdnimg.cn/20200505222300919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>但是我在运行了之后怎么都下载不下来，百度了一下很多人都有这个问题，有人说退出再运行一次就可以，但我的问题显然不是网络问题，所以无奈之下，只好换了一个命令，又去pytorch官网搜了更以前的版本来运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch&#x3D;&#x3D;1.2.0 torchvision&#x3D;&#x3D;0.4.0 cudatoolkit&#x3D;10.0 -c pytorch</span><br></pre></td></tr></table></figure><p>终于可以正常下载了，但是下载的过程还是花了很久，有人说把后面的-c pytorch去掉可以下载的快一点，不建议用pip安装，还是参照官网更好一点。<br>不管怎么样，最终还是下载完了，用下面这个命令查看已经搭建了哪些环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --env</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200506110733374.png" alt="在这里插入图片描述"><br>可以看到pytorch已经存在了（tf2.0b是之前搭的tensorflow的环境）<br>接下来激活环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch</span><br></pre></td></tr></table></figure><p>之后进入python</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python</span><br></pre></td></tr></table></figure><p>进行测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></table></figure><p>都是可以正常运行的<br>于是开始运行项目以及代码，进入到保存代码的目录下，以后都可以直接用vscode或者什么吧代码编辑好之后，改成.py文件，放在目录下，我的这个目录下已经包括了下载好的MNIST手写数据集，所以直接用python运行就可以了<br><img src="https://img-blog.csdnimg.cn/20200506172937999.png" alt="在这里插入图片描述"><br>结果截图：<br><img src="https://img-blog.csdnimg.cn/20200506173250687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到准确度非常高，但是我仍然疑惑loss应该是一直下降的，我的结果不是一直下降的，而且震荡是非常大的，这个问题之前学姐演示的时候也被老师指出过，于是去问了老师，老师说是learning_rate太大的问题，于是后来又改小了那个指数，这个问题就解决的比较好了，结果如下：<br><img src="https://img-blog.csdnimg.cn/20200506211036738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在这里再放上篇博客，对于LeNet5每一层的讲解以及代码的呈现都是经典的LeNet5结构，非常值得学习的代码思路，所以我又在tensorflow环境里去跑了一下这篇博客里的代码，得到的结果也和博客里的结果是一样的。<a href="https://www.cnblogs.com/ai-learning-blogs/p/11094039.html" target="_blank" rel="noopener">CNN-1 LeNet5卷积神经网络模型</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;  五一假期忙忙碌碌就快过去了，虽然也不知道具体自己都在忙些什么，但是还是每天都有每天的收获，很充实，我常常在想人为什么需要假期，忙着奔跑之后的停顿可能也会带来一些小惊喜，对我来说，可能是去野餐瞥见阳光下的夏天的味道让人沉溺的只想睡去，可能是去串门发现高二的妹妹满墙的公式单
      
    
    </summary>
    
      <category term="大创学习体会" scheme="http://yoursite.com/categories/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="LeNet-5" scheme="http://yoursite.com/tags/LeNet-5/"/>
    
  </entry>
  
  <entry>
    <title>大创学习记录（一）</title>
    <link href="http://yoursite.com/2020/05/02/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2020/05/02/大创学习记录（一）/</id>
    <published>2020-05-02T02:48:55.000Z</published>
    <updated>2020-06-28T14:46:52.883Z</updated>
    
    <content type="html"><![CDATA[<p>  把之前在csdn上写的两篇文章直接搬过来啦<br>  不久前看到了知乎上的一篇文章，《疫情期间大学生应该如何提升自己》，看完所有的文字，就觉得自己参加这样一个大创项目是多么的必要，目前大二的我已经参与了这是第二个大创项目，很幸运身边可以有大佬朋友愿意带我，但是说实话，第一个大创项目，我没有非常认真的去参与，这真是我非常后悔的一点，到现在也还没有结题，所以可能后期希望自己能为团队贡献自己的一点力量。但是！在这里我准备记录我参与的第二个大创项目，从组队到联系老师，再到写申请立项书，我都参与其中，希望能够真正的主动地去学习到有用的东西。<br>   好了，话不多说，开始今天的记录。<br>   首先，有目标才会有动力，有目标才会有计划。<br>   我们的项目是基于深度学习中的卷积神经网络（Faster R-CNN)以及YOLOv3预测模型，并不断用机器学习来优化算法，按照项目负责人的要求，近两周的学习计划如下，主要是先打个基础：</p><ol><li>学习深度学习相关知识。上<a href="http://eds.tju.edu.cn/ermsClient/browse.do查阅相关文献。top等级的论文几乎都在SCI上，登录资源平台外文数据库第一个web" target="_blank" rel="noopener">http://eds.tju.edu.cn/ermsClient/browse.do查阅相关文献。top等级的论文几乎都在SCI上，登录资源平台外文数据库第一个web</a> of science。</li><li>熟悉tensorflow框架。安装Anaconda，它是一个容器，然后在它里面可以搭建深度学习环境，安装Python, Numpy， Matplotlib，Tensorflow或者pytorch包。代码编辑器可以用VS code。</li><li>学习相关编程语言的语法。我们的算法用python实现，web用html+javascript。</li></ol><p>今天准备先搭建好tensorflow框架。<br>在网上看到，2019年春季，google发布了Tensorflow2.0测试版本。</p><p>指导老师温馨提醒：要注意自己的机器的配置，尤其是显卡。一般nVidia显卡可以装tensorflow的GPU版本，不是的话就装CPU版本。<br>笔者在我的电脑中的管理查了一下自己的显卡，发现自己的是Nvida GTX1050 Ti,运算能力4以上，符合安装GPU版本的要求。</p><p><strong>Anaconda + TensorFlow 2.0 GPU安装</strong></p><p><strong><em>Anoconda3下载</em></strong><br>下载官网<a href="https://www.anaconda.com/distribution/#download-section" target="_blank" rel="noopener">Anoconda3下载地址</a><br>建议安装Python 3.7 version,之后的操作比较方便<br>下载下来后一直next<br><img src="https://img-blog.csdnimg.cn/20200413103927732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>安装好后在开始菜单中找到Anaconda Prompt（开始菜单），如下图：<br><img src="https://img-blog.csdnimg.cn/20200413155540510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在命令行中输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n your_env_name python&#x3D;X.X</span><br></pre></td></tr></table></figure><p>your env name可以自己改，最好和用这个环境要安装的主力包有关，比如我用这个环境装了Tensorflow2.0.0beta1-gpu，我就把环境简单命名成tf2.0b。后面跟的是python版本号，因为我下的是3.7所以就改成python=3.7。之后让你回答Y/N，输入y就好。<br>安装成功后，命令行中输入<br><img src="https://img-blog.csdnimg.cn/20200413155957659.png" alt="在这里插入图片描述"><br>将环境激活，结果如图：<br><img src="https://img-blog.csdnimg.cn/20200413160203396.png" alt="在这里插入图片描述"><br>这个时候就可以在自己的虚拟环境下安装Tensorflow或者Pytorch, Numpy， Matplotlib等了。</p><p><strong><em>Tensorflow2.0 GPU安装</em></strong></p><p>Tensorflow2.0还没被收到anaconda库，所以需要用pip安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow-gpu&#x3D;&#x3D;2.0.0b1</span><br></pre></td></tr></table></figure><p>之后就一直安装就可以了，这个过程我花了几乎一天的时间下载，安装好了命令行输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><p>可以看到<br><img src="https://img-blog.csdnimg.cn/20200413235433349.png" alt="在这里插入图片描述"><br>有以上图中显示的条目，包括上面还有keras的，因为tf2.0把keras高度集成了。<br>接下来如果是安装的CPU，就不会有以下的步骤了。<br><strong><em>安装cuda和cudnn</em></strong><br>这个我本来以为只用用命令行就可以安装，于是之前试了用conda直接下载。tf2.0对于cuda的支持暂时仅限于10.0，cudnn是7.6.0。所以安装的时候要指定版本号。<br>在命令行输入如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install cudnn&#x3D;7.6.0</span><br><span class="line">conda install cudatoolkit&#x3D;10.0.130</span><br></pre></td></tr></table></figure><p>但是后来过了一天打开tensorflow，准备运行一下简单的“Hello, tensorflow!”的程序，结果第一步import tensorflow就一直在报错，如下图：<br><img src="https://img-blog.csdnimg.cn/20200418213610871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">是cuda和cudnn的问题，查了很多资料也没解决，于是去问了老师，老师说不能直接用虚拟环境下的命令行来安装cuda和cudnn，还是之前踩了坑，所以还是应该在网上下载好安装包，于是在网上按照教程开始下载安装。<br><strong><em>安装CUDA Toolkit + cuDNN</em></strong><br>1.查看需要安装的CUDA+cuDNN版本<br>注意，tensorflow是在持续更新的，具体安装的CUDA和cuDNN版本需要去官网查看，要与最新版本的tensorflow匹配。<br>2.下载CUDA + cuDNN<br>在这个网址查找CUDA已发布版本：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a><br>下载好CUDA Toolkit 10.0 后，我们开始下载cuDnn，需要注意的是，下载cuDNN需要在nvidia上注册账号，使用邮箱注册就可以，免费的。登陆账号后才能下载。</p><p>cuDNN历史版本在该网址下载：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a><br><strong>重要的一步：卸载显卡驱动</strong><br>由于CUDA Toolkit需要在指定版本显卡驱动环境下才能正常使用的，所以如果我们已经安装了nvidia显卡驱动（很显然，大部分人都安装了），再安装CUDA Toolkit时，会因二者版本不兼容而导致CUDA无法正常使用，这也就是很多人安装失败的原因。而CUDA Toolkit安装包中自带与之匹配的显卡驱动，所以务必要删除电脑先前的显卡驱动。<br>（搜索栏—-控制面板—程序—-卸载或更改程序—-选择NVIDIA的显卡驱动）<br><strong>安装CUDA</strong><br>我在安装的时候遇到了一个问题，显示Chrome_elf.dll不能创建，查了一下资料，把电脑360安全防护什么的关掉就行了，但是并不知道是什么原理。<br>点击下载好的安装包，选择安装的文件夹，同意并继续，然后下面这里选择自定义：<br><img src="https://img-blog.csdnimg.cn/2020041822462037.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>后面一系列通过以后，又遇到了这个问题：<br><img src="https://img-blog.csdnimg.cn/20200419002649219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>百度了一下，才发现是VS的问题<br>于是我找了所有有关的百度，找到了DDU，它可以帮助我们删除显卡驱动，于是就去下载了（网址：<a href="https://www.wagnardsoft.com/forums/viewtopic.php?f=5&t=1490" target="_blank" rel="noopener">https://www.wagnardsoft.com/forums/viewtopic.php?f=5&amp;t=1490</a> ）<br>运行成功后重启电脑，然后重新安装cuda，这时候就要比较小心，点击自定义，转到另一个可以自定义功能的界面，把vs相关的那一项的勾选去掉，然后就ok了，最后显示安装成功！亲测有效啊！<br>接下来，解压cudnn下载下来的压缩文件的三个文件夹，拷贝到CUDA安装的根目录下。<br>下面要进行环境变量的配置。</p><p>将下面四个路径加入到环境变量中，注意要换成自己的安装路径。</p><p>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0</p><p>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin</p><p>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64</p><p>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp</p><p>到此，全部的安装步骤都已经完成，我们来测试一下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">tf.test.gpu_device_name()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200419110549853.png" alt="在这里插入图片描述"><br>2.查看在使用哪个GPU</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.python.client import device_lib</span><br><span class="line">device_lib.list_local_devices()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200419110826466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZmZl9wcmFncmFtbWVy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>好了，到这里终于大功告成，下载安装这个从昨天到今天又弄了大半天，于是打算今天稍晚点再开始真正的跑跑代码，学习一下tensorflow的一些基本用法和算法实例。</p><p><strong><em>安装matplotlib</em></strong><br>命令行输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install matplotlib</span><br></pre></td></tr></table></figure><p>显示结果如下，则安装成功。<img src="https://img-blog.csdnimg.cn/20200414004928846.png" alt="在这里插入图片描述"></p><p>至此，环境基本上都搭建好了，用来写代码的vs code 之前也已经安装过了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;  把之前在csdn上写的两篇文章直接搬过来啦&lt;br&gt;  不久前看到了知乎上的一篇文章，《疫情期间大学生应该如何提升自己》，看完所有的文字，就觉得自己参加这样一个大创项目是多么的必要，目前大二的我已经参与了这是第二个大创项目，很幸运身边可以有大佬朋友愿意带我，但是说实话，第
      
    
    </summary>
    
      <category term="大创学习体会" scheme="http://yoursite.com/categories/%E5%A4%A7%E5%88%9B%E5%AD%A6%E4%B9%A0%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
</feed>
